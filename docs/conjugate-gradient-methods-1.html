<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Conjugate Gradient Methods | Introduction to Mathematical Optimization</title>
  <meta name="description" content="A book for teaching introductory numerical optimization algorithms with Python" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Conjugate Gradient Methods | Introduction to Mathematical Optimization" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://indrag49.github.io/Numerical-Optimization/" />
  
  <meta property="og:description" content="A book for teaching introductory numerical optimization algorithms with Python" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Conjugate Gradient Methods | Introduction to Mathematical Optimization" />
  
  <meta name="twitter:description" content="A book for teaching introductory numerical optimization algorithms with Python" />
  

<meta name="author" content="Indranil Ghosh" />


<meta name="date" content="2021-07-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="line-search-descent-methods.html"/>
<link rel="next" href="quasi-newton-methods.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Mathematical Optimization</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> What is Numerical Optimization?</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction-to-optimization"><i class="fa fa-check"></i><b>1.1</b> Introduction to Optimization</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#a-solution"><i class="fa fa-check"></i><b>1.2</b> A Solution</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#maximization"><i class="fa fa-check"></i><b>1.3</b> Maximization</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#feasible-region"><i class="fa fa-check"></i><b>1.4</b> Feasible Region</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#discrete-optimization-problems"><i class="fa fa-check"></i><b>1.5</b> Discrete Optimization Problems</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#linear-programming-problems"><i class="fa fa-check"></i><b>1.6</b> Linear Programming Problems</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#stochastic-optimization-problems"><i class="fa fa-check"></i><b>1.7</b> Stochastic Optimization Problems</a></li>
<li class="chapter" data-level="1.8" data-path="index.html"><a href="index.html#scaling-of-decision-variables"><i class="fa fa-check"></i><b>1.8</b> Scaling of Decision Variables</a></li>
<li class="chapter" data-level="1.9" data-path="index.html"><a href="index.html#gradient-vector-and-hessian-matrix-of-the-objective-function"><i class="fa fa-check"></i><b>1.9</b> Gradient Vector and Hessian Matrix of the Objective Function</a></li>
<li class="chapter" data-level="1.10" data-path="index.html"><a href="index.html#directional-derivative-of-the-objective-function"><i class="fa fa-check"></i><b>1.10</b> Directional Derivative of the Objective Function</a></li>
<li class="chapter" data-level="1.11" data-path="index.html"><a href="index.html#positive-definite-and-positive-semi-definite-matrices"><i class="fa fa-check"></i><b>1.11</b> Positive Definite and Positive Semi-definite Matrices</a></li>
<li class="chapter" data-level="1.12" data-path="index.html"><a href="index.html#what-is-convexity"><i class="fa fa-check"></i><b>1.12</b> What is Convexity?</a></li>
<li class="chapter" data-level="1.13" data-path="index.html"><a href="index.html#numerical-optimization-algorithms"><i class="fa fa-check"></i><b>1.13</b> Numerical Optimization Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html"><i class="fa fa-check"></i><b>2</b> Introduction to Unconstrained Optimization</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#the-unconstrained-optimization-problem"><i class="fa fa-check"></i><b>2.1</b> The Unconstrained Optimization Problem</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#smooth-functions"><i class="fa fa-check"></i><b>2.2</b> Smooth Functions</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#taylors-theorem"><i class="fa fa-check"></i><b>2.3</b> Taylorâ€™s Theorem</a></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#necessary-and-sufficient-conditions-for-local-minimizer-in-unconstrained-optimization"><i class="fa fa-check"></i><b>2.4</b> Necessary and Sufficient Conditions for Local Minimizer in Unconstrained Optimization</a><ul>
<li class="chapter" data-level="2.4.1" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#first-order-necessary-condition"><i class="fa fa-check"></i><b>2.4.1</b> First-Order Necessary Condition</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#second-order-necessary-conditions"><i class="fa fa-check"></i><b>2.4.2</b> Second-Order Necessary Conditions</a></li>
<li class="chapter" data-level="2.4.3" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#second-order-sufficient-conditions"><i class="fa fa-check"></i><b>2.4.3</b> Second-Order Sufficient Conditions</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#algorithms-for-solving-unconstrained-minimization-tasks"><i class="fa fa-check"></i><b>2.5</b> Algorithms for Solving Unconstrained Minimization Tasks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html"><i class="fa fa-check"></i><b>3</b> Solving One Dimensional Optimization Problems</a><ul>
<li class="chapter" data-level="3.1" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#one-dimensional-optimization-problems"><i class="fa fa-check"></i><b>3.1</b> One Dimensional Optimization Problems</a></li>
<li class="chapter" data-level="3.2" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#what-is-a-unimodal-function"><i class="fa fa-check"></i><b>3.2</b> What is a Unimodal Function?</a></li>
<li class="chapter" data-level="3.3" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#fibonacci-search-method"><i class="fa fa-check"></i><b>3.3</b> Fibonacci Search Method</a></li>
<li class="chapter" data-level="3.4" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#golden-section-search-method"><i class="fa fa-check"></i><b>3.4</b> Golden Section Search Method</a></li>
<li class="chapter" data-level="3.5" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#powells-quadratic-interpolation-method"><i class="fa fa-check"></i><b>3.5</b> Powellâ€™s Quadratic Interpolation Method</a></li>
<li class="chapter" data-level="3.6" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#inverse-quadratic-interpolation-method"><i class="fa fa-check"></i><b>3.6</b> Inverse Quadratic Interpolation Method</a></li>
<li class="chapter" data-level="3.7" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#newtons-method"><i class="fa fa-check"></i><b>3.7</b> Newtonâ€™s Method</a></li>
<li class="chapter" data-level="3.8" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#halleys-method"><i class="fa fa-check"></i><b>3.8</b> Halleyâ€™s Method</a></li>
<li class="chapter" data-level="3.9" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#secant-method"><i class="fa fa-check"></i><b>3.9</b> Secant Method</a></li>
<li class="chapter" data-level="3.10" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#bisection-method"><i class="fa fa-check"></i><b>3.10</b> Bisection Method</a></li>
<li class="chapter" data-level="3.11" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#brents-method"><i class="fa fa-check"></i><b>3.11</b> Brentâ€™s Method</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html"><i class="fa fa-check"></i><b>4</b> Line Search Descent Methods</a><ul>
<li class="chapter" data-level="4.1" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#introduction-to-line-search-descent-methods-for-unconstrained-minimization"><i class="fa fa-check"></i><b>4.1</b> Introduction to Line Search Descent Methods for Unconstrained Minimization</a></li>
<li class="chapter" data-level="4.2" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#selection-of-step-length"><i class="fa fa-check"></i><b>4.2</b> Selection of Step Length</a><ul>
<li class="chapter" data-level="4.2.1" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#the-wolfe-conditions"><i class="fa fa-check"></i><b>4.2.1</b> The Wolfe Conditions</a></li>
<li class="chapter" data-level="4.2.2" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#an-algorithm-for-the-strong-wolfe-conditions"><i class="fa fa-check"></i><b>4.2.2</b> An Algorithm for the Strong Wolfe Conditions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#first-order-line-search-gradient-descent-method-the-steepest-descent-algorithm"><i class="fa fa-check"></i><b>4.3</b> First Order Line Search Gradient Descent Method: The Steepest Descent Algorithm</a></li>
<li class="chapter" data-level="4.4" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#conjugate-gradient-methods"><i class="fa fa-check"></i><b>4.4</b> Conjugate Gradient Methods</a></li>
<li class="chapter" data-level="4.5" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#second-order-line-search-gradient-descent-method"><i class="fa fa-check"></i><b>4.5</b> Second Order Line Search Gradient Descent Method</a></li>
<li class="chapter" data-level="4.6" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#marquardt-method"><i class="fa fa-check"></i><b>4.6</b> Marquardt Method</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html"><i class="fa fa-check"></i><b>5</b> Conjugate Gradient Methods</a><ul>
<li class="chapter" data-level="5.1" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#introduction-to-conjugate-gradient-methods"><i class="fa fa-check"></i><b>5.1</b> Introduction to Conjugate Gradient Methods</a></li>
<li class="chapter" data-level="5.2" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#linear-conjugate-gradient-algorithm"><i class="fa fa-check"></i><b>5.2</b> Linear Conjugate Gradient Algorithm</a><ul>
<li class="chapter" data-level="5.2.1" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#mutual-conjugacy"><i class="fa fa-check"></i><b>5.2.1</b> Mutual Conjugacy</a></li>
<li class="chapter" data-level="5.2.2" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#conjugate-direction-algorithm"><i class="fa fa-check"></i><b>5.2.2</b> Conjugate Direction Algorithm</a></li>
<li class="chapter" data-level="5.2.3" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#preliminary-algorithm"><i class="fa fa-check"></i><b>5.2.3</b> Preliminary Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#nonlinear-conjugate-gradient-algorithm"><i class="fa fa-check"></i><b>5.3</b> Nonlinear Conjugate Gradient Algorithm</a><ul>
<li class="chapter" data-level="5.3.1" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#feltcher-reeves-algorithm"><i class="fa fa-check"></i><b>5.3.1</b> Feltcher-Reeves Algorithm</a></li>
<li class="chapter" data-level="5.3.2" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#polak-ribiere-algorithm"><i class="fa fa-check"></i><b>5.3.2</b> Polak-Ribiere Algorithm</a></li>
<li class="chapter" data-level="5.3.3" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#hestenes-stiefel-algorithm"><i class="fa fa-check"></i><b>5.3.3</b> Hestenes-Stiefel Algorithm</a></li>
<li class="chapter" data-level="5.3.4" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#dai-yuan-algorithm"><i class="fa fa-check"></i><b>5.3.4</b> Dai-Yuan Algorithm</a></li>
<li class="chapter" data-level="5.3.5" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#hager-zhang-algorithm"><i class="fa fa-check"></i><b>5.3.5</b> Hager-Zhang Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#the-scipy.optimize.minimize-function"><i class="fa fa-check"></i><b>5.4</b> The <code>scipy.optimize.minimize()</code> Function</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="quasi-newton-methods.html"><a href="quasi-newton-methods.html"><i class="fa fa-check"></i><b>6</b> Quasi-Newton Methods</a><ul>
<li class="chapter" data-level="6.1" data-path="quasi-newton-methods.html"><a href="quasi-newton-methods.html#introduction-to-quasi-newton-methods"><i class="fa fa-check"></i><b>6.1</b> Introduction to Quasi-Newton Methods</a></li>
<li class="chapter" data-level="6.2" data-path="quasi-newton-methods.html"><a href="quasi-newton-methods.html#the-approximate-inverse-matrix"><i class="fa fa-check"></i><b>6.2</b> The Approximate Inverse Matrix</a></li>
<li class="chapter" data-level="6.3" data-path="quasi-newton-methods.html"><a href="quasi-newton-methods.html#rank-1-update-algorithm"><i class="fa fa-check"></i><b>6.3</b> Rank 1 Update Algorithm</a></li>
<li class="chapter" data-level="6.4" data-path="quasi-newton-methods.html"><a href="quasi-newton-methods.html#rank-2-update-algorithms"><i class="fa fa-check"></i><b>6.4</b> Rank 2 Update Algorithms</a><ul>
<li class="chapter" data-level="6.4.1" data-path="quasi-newton-methods.html"><a href="quasi-newton-methods.html#davidon-fletcher-powell-algorithm"><i class="fa fa-check"></i><b>6.4.1</b> Davidon-Fletcher-Powell Algorithm</a></li>
<li class="chapter" data-level="6.4.2" data-path="quasi-newton-methods.html"><a href="quasi-newton-methods.html#broyden-fletcher-goldfarb-shanno-bfgs-algorithm"><i class="fa fa-check"></i><b>6.4.2</b> Broyden-Fletcher-Goldfarb-Shanno (BFGS) Algorithm</a></li>
<li class="chapter" data-level="6.4.3" data-path="quasi-newton-methods.html"><a href="quasi-newton-methods.html#huangs-family-of-rank-2-update-formulae"><i class="fa fa-check"></i><b>6.4.3</b> Huangâ€™s Family of Rank 2 Update Formulae</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Mathematical Optimization</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="conjugate-gradient-methods-1" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Conjugate Gradient Methods</h1>
<p>This chapter is dedicated to studying the <em>Conjugate Gradient Methods</em> in detail. The Linear and Non-linear versions of the CG methods have been discussed with five sub classes falling under the nonlinear CG method class. The five nonlinear CG methods that have been discussed are: <em>Flethcher-Reeves method</em>, <em>Polak-Ribiere method</em>, <em>Hestenes-Stiefel method</em>, <em>Dai-Yuan method</em> and <em>Hager-Zhang method</em>. Mathematical proofs have been provided wherever necessary. Python implementations of the algorithms have been included along with optimization examples. The chapter ends with introducing a specific Python function called the <code>scipy.optimize.minimize()</code> function that can be used to work with the <em>Polak-Ribiere</em> CG method.</p>
<hr />
<div id="introduction-to-conjugate-gradient-methods" class="section level2">
<h2><span class="header-section-number">5.1</span> Introduction to Conjugate Gradient Methods</h2>
<p>The <em>conjugate gradient methods</em> are frequently used for solving large linear systems of equations and also for solving nonlinear optimization problems. This let us characterize the <em>conjugate gradient methods</em> into two classes:</p>
<ul>
<li><strong>Linear Conjugate Gradient Method</strong>: This is an iterative method to solve large linear systems where the coefficient matrices are positive definite. This can be treated as a replacement of the <em>Gaussian elimination method</em> in numerical analysis.</li>
<li><strong>nonlinear Conjugate Gradient method</strong>: This is used for solving nonlinear optimization problems. We will study five methods under this class:
<ul>
<li><em>Fletcher-Reeves algorithm</em>,</li>
<li><em>Polak-Ribiere algorithm</em>,</li>
<li><em>Hestenes-Stiefel algorithm</em>,</li>
<li><em>Dai-Yuan algorithm</em>, and</li>
<li><em>Hager-Zhang algorithm</em>.</li>
</ul></li>
</ul>
</div>
<div id="linear-conjugate-gradient-algorithm" class="section level2">
<h2><span class="header-section-number">5.2</span> Linear Conjugate Gradient Algorithm</h2>
<p>Suppose we want to find the minimizer of an objective function, having the quadratic form:
<span class="math display" id="eq:1">\[\begin{equation}
    f(\mathbb{x}) = \frac{1}{2}\mathbb{x}^T\mathbb{A}\mathbb{x} - \mathbb{b}^T\mathbb{x} \tag{5.1}
\end{equation}\]</span>
where, <span class="math inline">\(\mathbb{A}\)</span> is a <span class="math inline">\(n \times n\)</span> symmetric positive definite matrix.The problem can be formulated as:
<span class="math display" id="eq:2">\[\begin{equation}
    \underset{\mathbb{x}\in \mathbb{R}^n}{\min} f(\mathbb{x}) = \frac{1}{2}\mathbb{x}^T\mathbb{A}\mathbb{x} - \mathbb{b}^T\mathbb{x} \tag{5.2}
\end{equation}\]</span></p>
<p>Eq. <a href="conjugate-gradient-methods-1.html#eq:2">(5.2)</a> can be equivalently stated as the problem of solving the linear system of equations given by:
<span class="math display" id="eq:3">\[\begin{equation}
    \mathbb{A}\mathbb{x} = \mathbb{b} \tag{5.3}
\end{equation}\]</span></p>
<p>We use the <em>linear conjugate gradient method</em> to solve Eq. <a href="conjugate-gradient-methods-1.html#eq:3">(5.3)</a>.</p>
<p>The <em>residual</em> of a linear system of equations, given by Eq. <a href="conjugate-gradient-methods-1.html#eq:3">(5.3)</a> is defined as:
<span class="math display" id="eq:4">\[\begin{equation}
    r(\mathbb{x}) = \mathbb{A}\mathbb{x} - \mathbb{b} \tag{5.4}
\end{equation}\]</span></p>

<div class="theorem">
<span id="thm:unnamed-chunk-1" class="theorem"><strong>Theorem 5.1  </strong></span>The gradient of the objective function given by Eq. <a href="conjugate-gradient-methods-1.html#eq:1">(5.1)</a> is equal to the residual of the linear system given by Eq. <a href="conjugate-gradient-methods-1.html#eq:4">(5.4)</a>.
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> From Eq. <a href="conjugate-gradient-methods-1.html#eq:1">(5.1)</a>, we see that the gradient of the objective function is:
<span class="math display" id="eq:5">\[\begin{equation}
    \nabla f(\mathbb{x}) = \mathbb{A}\mathbb{x} - \mathbb{b} = r(\mathbb{x}) \tag{5.5}
\end{equation}\]</span>
This proves the theorem.
</div>

<div id="mutual-conjugacy" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Mutual Conjugacy</h3>
<p>For a given symmetric positive definite matrix <span class="math inline">\(\mathbb{A}\)</span>, two vectors <span class="math inline">\(\mathbb{v}, \mathbb{w} \neq \mathbb{0} \in \mathbb{R}^n\)</span> are defined to be <em>mutually conjugate</em> if the following condition is satisfied:
<span class="math display" id="eq:6">\[\begin{equation}
    \mathbb{v}^T\mathbb{A}\mathbb{w} = 0 \tag{5.6}
\end{equation}\]</span></p>

<div class="theorem">
<span id="thm:unnamed-chunk-3" class="theorem"><strong>Theorem 5.2  </strong></span>A set of <em>mutually conjugate</em> vectors <span class="math inline">\(\mathbb{v}_j, j=1, 2, \ldots\)</span> with respect to a positive definite symmetric matrix <span class="math inline">\(\mathbb{A}\)</span>, forms a basis in <span class="math inline">\(\mathbb{R}^n\)</span>, i.e., the set is linearly independent.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Above theorem equivalently states that, for <span class="math inline">\(\mathbb{x} \in \mathbb{R}^n\)</span>, the following condition is satisfied:
<span class="math display" id="eq:7">\[\begin{equation}
    \mathbb{x} = \sum_{j=1}^n\lambda_j\mathbb{v}_j \tag{5.7}
\end{equation}\]</span>
where,
<span class="math display" id="eq:8">\[\begin{equation}
    \lambda_j = \frac{\mathbb{v}_j^T\mathbb{A}\mathbb{x}}{\mathbb{v}_j^T\mathbb{A}\mathbb{v}_j} \tag{5.8}
\end{equation}\]</span>
Let us consider the linear combination,
<span class="math display" id="eq:9">\[\begin{equation}
    \sum_{j=1}^n c_j\mathbb{v}_j = \mathbb{0} \tag{5.9}
\end{equation}\]</span>
Multiplying the above equation with the matrix <span class="math inline">\(\mathbb{A}\)</span>, we have,
<span class="math display" id="eq:10">\[\begin{equation}
    \sum_{j=1}^n c_j \mathbb{A}\mathbb{v}_j=\mathbb{0} \tag{5.10}
\end{equation}\]</span>
Since the vectors <span class="math inline">\(\mathbb{v}_j\)</span> are mutually conjugate with respect to the matrix <span class="math inline">\(\mathbb{A}\)</span>, from Eq. <a href="conjugate-gradient-methods-1.html#eq:6">(5.6)</a>, we can write that,
<span class="math display" id="eq:11">\[\begin{equation}
    c_j\mathbb{v}_j^T\mathbb{A}\mathbb{v}_j = 0 \tag{5.11}
\end{equation}\]</span></p>
From the facts that <span class="math inline">\(\mathbb{A}\)</span> is positive definite and that <span class="math inline">\(\mathbb{v}_j\)</span> never equals <span class="math inline">\(\mathbb{0}\)</span>, from Eq. <a href="conjugate-gradient-methods-1.html#eq:11">(5.11)</a> we can state that, <span class="math inline">\(c_j=0\)</span> for <span class="math inline">\(j=1, 2, \ldots, n\)</span>. This proves the fact that the set of vectors <span class="math inline">\(\mathbb{v}_j\)</span> is linearly independent and may be used as a basis. Therefore, there exists a unique set <span class="math inline">\(\lambda_j, j=1, 2, \ldots, n\)</span> for any <span class="math inline">\(\mathbb{x} \in \mathbb{R}^n\)</span>, such that Eq. <a href="conjugate-gradient-methods-1.html#eq:7">(5.7)</a> is satisfied. The positive definiteness of <span class="math inline">\(\mathbb{A}\)</span> leads to the fact that,
<span class="math display" id="eq:12">\[\begin{equation}
    \mathbb{v}_j^T\mathbb{A}\mathbb{x} = \lambda_j\mathbb{v}_j^T\mathbb{A}\mathbb{x} \tag{5.12}
\end{equation}\]</span>
Finally, from Eq. <a href="conjugate-gradient-methods-1.html#eq:12">(5.12)</a> we can write that,
<span class="math display" id="eq:13">\[\begin{equation}
    \lambda_j = \frac{\mathbb{v}_j^T\mathbb{A}\mathbb{x}}{\mathbb{v}_j^T\mathbb{A}\mathbb{v}_j} \tag{5.13}
\end{equation}\]</span>
The proves the theorem.
</div>

</div>
<div id="conjugate-direction-algorithm" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Conjugate Direction Algorithm</h3>
<p>For our optimization task, where we aim to minimize the objective function <span class="math inline">\(f(\mathbb{x})\)</span>, where <span class="math inline">\(\mathbb{x} \in \mathbb{R}^n\)</span>, let <span class="math inline">\(\mathbb{x}_0\)</span> be the starting iterate and the conjugate directions be set as <span class="math inline">\({\mathbb{\delta}_j}, j=1, 2, \ldots, n-1\)</span>. The successive iterates are generated by following:</p>
<p><span class="math display" id="eq:14">\[\begin{equation}
    \mathbb{x}_j = \mathbb{x}_{j-1}+\beta_j\mathbb{\delta}_j \tag{5.14}
\end{equation}\]</span></p>
<p>This <span class="math inline">\(\beta_j\)</span> is the minimizer of the function <span class="math inline">\(f(\mathbb{x}_{j-1}+\beta\delta_j)\)</span>. We will now find the explicit form of <span class="math inline">\(\beta_j\)</span>. From Eq. <a href="conjugate-gradient-methods-1.html#eq:1">(5.1)</a> we can write that,</p>
<p><span class="math display" id="eq:15">\[\begin{align}
f(x_{j-1}+\beta \mathbb{\delta}_j) &amp;= \frac{1}{2}[(\mathbb{x}_{j-1}+\beta\mathbb{\delta}_j)^T\mathbb{A}(\mathbb{x}_{j-1}+\beta\mathbb{\delta}_j)] - \mathbb{b}^T(\mathbb{x}_{j-1}+\beta\mathbb{\delta}_j) \nonumber \\
&amp;= \frac{1}{2}[\mathbb{x}_{j-1}^T\mathbb{A}\mathbb{x}_{j-1}+2\beta\mathbb{x}_{j-1}^T\mathbb{A}\mathbb{\delta}_j + \beta^2\mathbb{\delta}_j^T\mathbb{A}\mathbb{\delta}_j] - \mathbb{b}^T(\mathbb{x}_{j-1}+\beta\mathbb{\delta}_j) \tag{5.15}
\end{align}\]</span></p>
<p>Now, differentiating Eq. <a href="conjugate-gradient-methods-1.html#eq:15">(5.15)</a> with respect to <span class="math inline">\(\beta\)</span> and setting it to <span class="math inline">\(0\)</span>, we get,
<span class="math display" id="eq:17" id="eq:16">\[\begin{align}
&amp; \frac{\partial f(\mathbb{x}_{j-1}+\beta\mathbb{\delta}_j)}{\partial \beta} = 0 \tag{5.16} \\
&amp;\implies \mathbb{x}_{j-1}^T\mathbb{A}\mathbb{x}_{j-1} + \beta\mathbb{\delta}_j\mathbb{A}\mathbb{\delta}_j - \mathbb{b}^T\mathbb{\delta}_j = 0 \nonumber \\
&amp;\implies (\mathbb{x}_{j-1}^T\mathbb{A} - \mathbb{b}^T)\mathbb{\delta}_j + \beta\mathbb{\delta}_j^T\mathbb{A}\mathbb{\delta}_j = 0 \tag{5.17}
\end{align}\]</span></p>
<p>Now, from Eq. <a href="conjugate-gradient-methods-1.html#eq:4">(5.4)</a> we can write,
<span class="math display" id="eq:18">\[\begin{equation}
    \mathbb{r}_j^T=(\mathbb{A}\mathbb{x}_{j-1}-\mathbb{b})^T = \mathbb{x}_{j-1}^T\mathbb{A} - \mathbb{b}^T \tag{5.18}
\end{equation}\]</span>
where, we have used the fact that <span class="math inline">\(\mathbb{A}^T=\mathbb{A}\)</span>. So, from Eq. <a href="conjugate-gradient-methods-1.html#eq:17">(5.17)</a> we can write,
<span class="math display" id="eq:19">\[\begin{equation}
    \mathbb{r}_j^T\mathbb{\delta}_j+\beta\mathbb{\delta}_j^T\mathbb{A}\mathbb{\delta}_j = 0 \tag{5.19}
\end{equation}\]</span></p>
<p>This finally fetches us,
<span class="math display" id="eq:20">\[\begin{equation}
    \beta_j = -\frac{\mathbb{r}_j^T\mathbb{\delta}_j}{\mathbb{\delta}_j^T\mathbb{A}\mathbb{\delta}_j} \tag{5.20}
\end{equation}\]</span>
Eq. <a href="conjugate-gradient-methods-1.html#eq:20">(5.20)</a> is equivalent to the step-length formulation given by Eq. <a href="conjugate-gradient-methods-1.html#eq:26">(5.26)</a>.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-5" class="theorem"><strong>Theorem 5.3  </strong></span>The convergence of the conjugate direction algorithm, given by Eq. <a href="conjugate-gradient-methods-1.html#eq:14">(5.14)</a> and Eq. <a href="conjugate-gradient-methods-1.html#eq:20">(5.20)</a>, to its solution, takes place in at most <span class="math inline">\(n\)</span> steps, where <span class="math inline">\(\mathbb{x}_0\in \mathbb{R}^n\)</span> is the given initial iterate.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> The conjugate directions <span class="math inline">\(\mathbb{\delta}\)</span> are linearly independent, and thus for any scalar values <span class="math inline">\(\lambda_i\)</span>, we can write,
<span class="math display" id="eq:21">\[\begin{align}
    \mathbb{x}^* &amp;= \mathbb{x}_0 + \lambda_1\mathbb{\delta}_1 + \ldots + \lambda_{n-1}\mathbb{\delta}_{n-1} \nonumber \\
    \mathbb{x}^* - \mathbb{x}_0 &amp;=  \lambda_1\mathbb{\delta}_1 + \ldots + \lambda_{n-1}\mathbb{\delta}_{n-1} \tag{5.21}
\end{align}\]</span></p>
<p>Now, multiplying Eq. <a href="conjugate-gradient-methods-1.html#eq:21">(5.21)</a> non-commutatively by the preceding factor <span class="math inline">\(\mathbb{\delta}_j^T\mathbb{A}\)</span>, and using the mutual conjugacy from Eq. <a href="conjugate-gradient-methods-1.html#eq:6">(5.6)</a>, we will have,</p>
<p><span class="math display" id="eq:22">\[\begin{equation}
    \mathbb{\delta}_j^T\mathbb{A}(\mathbb{x}^*-\mathbb{x}_0) = \lambda_j\mathbb{\delta}_j^T\mathbb{A}\mathbb{\delta}_j^T \tag{5.22}
\end{equation}\]</span>
which ultimately gives us,
<span class="math display" id="eq:23">\[\begin{equation}
    \lambda_j = \frac{\mathbb{\delta}_j^T\mathbb{A}(\mathbb{x}^*-\mathbb{x}_0)}{\mathbb{\delta}_j^T\mathbb{A}\mathbb{\delta}_j^T} \tag{5.23}
\end{equation}\]</span></p>
<p>Now again, using Eq. <a href="conjugate-gradient-methods-1.html#eq:14">(5.14)</a> we can generate the <span class="math inline">\(j^{th}\)</span> iterate, given by,
<span class="math display" id="eq:24">\[\begin{equation}
    \mathbb{x}_j = x_0 + \beta_1\mathbb{\delta}_1 + \beta_2\mathbb{\delta}_2 + \ldots + \beta_{j-1}\mathbb{\delta}_{j-1} \tag{5.24}
\end{equation}\]</span></p>
<p>Now subtracting Eq. <a href="conjugate-gradient-methods-1.html#eq:24">(5.24)</a> from the solution <span class="math inline">\(\mathbb{x}^*\)</span>, we get,
<span class="math display" id="eq:25">\[\begin{equation}
    \mathbb{x}^* - \mathbb{x}_j = \mathbb{x}^* - \mathbb{x}_0 - \beta_1\mathbb{\delta}_1 - \ldots - \beta_{j-1}\mathbb{\delta}_{j-1} \tag{5.25}
\end{equation}\]</span>
fetching us,
<span class="math display" id="eq:26">\[\begin{equation}
    \mathbb{x}^* - \mathbb{x}_0 = \mathbb{x}^* - \mathbb{x}_j + \beta_1\mathbb{\delta}_1 - \ldots + \beta_{j-1}\mathbb{\delta}_{j-1} \tag{5.26}
\end{equation}\]</span>
Now, again multiplying Eq. <a href="conjugate-gradient-methods-1.html#eq:26">(5.26)</a> non-commutatively by the preceding factor <span class="math inline">\(\mathbb{\delta}_j^T\mathbb{A}\)</span>, and using the mutual conjugacy from Eq. <a href="conjugate-gradient-methods-1.html#eq:6">(5.6)</a>, we will have,
<span class="math display" id="eq:27">\[\begin{equation}
    \mathbb{\delta}_j^T\mathbb{A}(\mathbb{x}^* - \mathbb{x}_0) = \mathbb{\delta}_j^T\mathbb{A}(\mathbb{x}^* - \mathbb{x}_j) \tag{5.27}
\end{equation}\]</span></p>
<p>Using the fact that <span class="math inline">\(\mathbb{A}\mathbb{x}^*=\mathbb{b}\)</span> and also Eq. <a href="conjugate-gradient-methods-1.html#eq:18">(5.18)</a>, we can modify Eq. <a href="conjugate-gradient-methods-1.html#eq:27">(5.27)</a> in the following way:
<span class="math display" id="eq:28">\[\begin{align}
    \mathbb{\delta}_j^T\mathbb{A}(\mathbb{x}^* - \mathbb{x}_0) &amp;= \mathbb{\delta}_j^T(\mathbb{b} - \mathbb{x}_j) \nonumber \\ &amp;= -\mathbb{\delta}_j^T\mathbb{r}_j \nonumber \\ &amp;= -\mathbb{r}_j^T\mathbb{\delta}_j \tag{5.28}
\end{align}\]</span></p>
<p>So, Eq. <a href="conjugate-gradient-methods-1.html#eq:23">(5.23)</a> becomes,
<span class="math display" id="eq:29">\[\begin{equation}
    \lambda_j = -\frac{\mathbb{r}_j^T\mathbb{\delta}_j}{\mathbb{\delta}_j^T\mathbb{A}\mathbb{\delta}_j} \tag{5.29}
\end{equation}\]</span></p>
which is similar to Eq. <a href="conjugate-gradient-methods-1.html#eq:20">(5.20)</a>. So it can be concluded that,
<span class="math display" id="eq:30">\[\begin{equation}
    \lambda_j = \beta_j \tag{5.30}
\end{equation}\]</span>
This completes the proof of the theorem.
</div>


<div class="theorem">
<span id="thm:unnamed-chunk-7" class="theorem"><strong>Theorem 5.4  </strong></span>The residual at the <span class="math inline">\(j^{th}\)</span> iterate can be generated from the residual at the preceding iterate by the following iteration formula:
<span class="math display" id="eq:31">\[\begin{equation}
    \mathbb{r}_{j} = \mathbb{r}_{j-1} + \beta_j\mathbb{A}\mathbb{\delta}_j \tag{5.31}
\end{equation}\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> Substituting Eq. <a href="conjugate-gradient-methods-1.html#eq:14">(5.14)</a> in Eq. <a href="conjugate-gradient-methods-1.html#eq:4">(5.4)</a>, we get,
<span class="math display" id="eq:32">\[\begin{align}
    \mathbb{r}_j &amp;= \mathbb{A}(\mathbb(x)_{j-1} + \beta_j\mathbb{\delta}_j) - \mathbb{b} \nonumber \\
    &amp;= \mathbb{A}\mathbb{x}_{j-1} + \beta_j\mathbb{A}\mathbb{\delta}_j - b \nonumber \\
    &amp;= (\mathbb{A}\mathbb{x}_{j-1} - b) + \beta_j\mathbb{A}\mathbb{\delta}_j \nonumber \\
    &amp;= \mathbb{r}_{j-1} + \beta_j\mathbb{A}\mathbb{\delta}_j \tag{5.32}
\end{align}\]</span>
This completes the proof.
</div>

</div>
<div id="preliminary-algorithm" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Preliminary Algorithm</h3>
<p>In the linear conjugate gradient method, the direction <span class="math inline">\(\mathbb{\delta}_j\)</span> ix a linear combination of the preceding direction <span class="math inline">\(\mathbb{\delta}_{j-1}\)</span> and the negative of the residual <span class="math inline">\(-\mathbb{r}_j\)</span>. So we can write,
<span class="math display" id="eq:33">\[\begin{equation}
    \mathbb{\delta}_j = \chi_j \mathbb{\delta}_{j-1} - \mathbb{r}_j \tag{5.33}
\end{equation}\]</span></p>
<p>Now, to evaluate <span class="math inline">\(\chi_j\)</span>, we multiply Eq. <a href="conjugate-gradient-methods-1.html#eq:33">(5.33)</a> non-commutatively with the preceding factor <span class="math inline">\(\mathbb{\delta}_{j-1}^T\mathbb{A}\)</span> and use the mutual conjugacy condition that <span class="math inline">\(\mathbb{\delta}_{j-1}^T\mathbb{A}\mathbb{\delta}_j=0\)</span>. <span class="math display" id="eq:34">\[\begin{equation}
    \mathbb{\delta}_{j-1}^T\mathbb{A}\mathbb{\delta}_j = \chi_j\mathbb{\delta}_{j-1}^T\mathbb{A}\mathbb{\delta}_{j-1} - \mathbb{\delta}_{j-1}^T\mathbb{A}\mathbb{r}_j = 0 \tag{5.34}
\end{equation}\]</span></p>
<p>So, we see that,
<span class="math display" id="eq:35">\[\begin{align}
    \chi_j &amp;= \frac{\mathbb{\delta}_{j-1}^T\mathbb{A}\mathbb{r}_j}{\mathbb{\delta}_{j-1}^T\mathbb{A}\mathbb{\delta}_{j-1}} \nonumber \\
    &amp;= \frac{(\mathbb{A}\mathbb{r}_j)^T\mathbb{\delta}_{j-1}}{\mathbb{\delta}_{j-1}^T\mathbb{A}\mathbb{\delta}_{j-1}} \nonumber \\
    &amp;= \frac{\mathbb{r}_j^T\mathbb{A}^T\mathbb{\delta}_{j-1}}{\mathbb{\delta}_{j-1}^T\mathbb{A}\mathbb{\delta}_{j-1}} \nonumber \\
    &amp;= \frac{\mathbb{r}_j^T\mathbb{A}\mathbb{\delta}_{j-1}}{\mathbb{\delta}_{j-1}^T\mathbb{A}\mathbb{\delta}_{j-1}} \tag{5.35}
\end{align}\]</span></p>
<p>The <em>linear conjugate gradient algorithm</em> is given below:</p>
<p><img src="img%2018.png" /></p>

<div class="example">
<p><span id="exm:unnamed-chunk-9" class="example"><strong>Example 5.1  </strong></span>Let us consider an objective function given by:
<span class="math display" id="eq:36">\[\begin{equation}
    f(x_1, x_2) = \frac{x_1^2}{2} + x_1x_2 + x_2^2-2x_2 \tag{5.36}
\end{equation}\]</span></p>
<p>Finding the minimizer of this objective function is equivalent to finding the solution to the equation given by <span class="math inline">\(\mathbb{A}\mathbb{x} = \mathbb{b}\)</span>, where <span class="math inline">\(\mathbb{A} = \begin{bmatrix} \frac{1}{2} &amp; \frac{1}{2} \\ \frac{1}{2} &amp; 1\end{bmatrix}\)</span>, <span class="math inline">\(\mathbb{x} = \begin{bmatrix}x_1 \\ x_2\end{bmatrix}\)</span> and <span class="math inline">\(\mathbb{b} = \begin{bmatrix}0 \\2\end{bmatrix}\)</span>. So, we use the <em>linear conjugate gradient algorithm</em> to solve
<span class="math display" id="eq:37">\[\begin{equation}
    \begin{bmatrix} \frac{1}{2} &amp; \frac{1}{2} \\ \frac{1}{2} &amp; 1\end{bmatrix} \begin{bmatrix}x_1 \\ x_2\end{bmatrix} = \begin{bmatrix}0 \\2\end{bmatrix} \tag{5.37}
\end{equation}\]</span></p>
where, we will consider the starting iterate to be <span class="math inline">\(\begin{bmatrix}-2.3 \\ 2.2 \end{bmatrix}\)</span> tolerance <span class="math inline">\(\epsilon =10^{-5}\)</span>. As usual, let us first define the objective function in Python.
</div>

<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="conjugate-gradient-methods-1.html#cb1-1"></a><span class="kw">def</span> f(x): <span class="co"># Define the objective function</span></span>
<span id="cb1-2"><a href="conjugate-gradient-methods-1.html#cb1-2"></a>    <span class="cf">return</span> x[<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span> <span class="op">+</span> x[<span class="dv">0</span>]<span class="op">*</span>x[<span class="dv">1</span>] <span class="op">+</span> x[<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>x[<span class="dv">1</span>]</span></code></pre></div>
<p>Next we define the matrix <span class="math inline">\(\mathbb{A}\)</span> and the vector <span class="math inline">\(\mathbb{b}\)</span> in Python.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="conjugate-gradient-methods-1.html#cb2-1"></a>A <span class="op">=</span> np.array(([<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>], [<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">1</span>]), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb2-2"><a href="conjugate-gradient-methods-1.html#cb2-2"></a>b <span class="op">=</span> np.array([<span class="fl">0.</span>, <span class="fl">2.</span>])</span></code></pre></div>
<p>We can make it sure that <span class="math inline">\(\mathbb{A}\)</span> is actually a symmetric positive definite matrix.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="conjugate-gradient-methods-1.html#cb3-1"></a>eigs <span class="op">=</span> np.linalg.eigvals(A)</span>
<span id="cb3-2"><a href="conjugate-gradient-methods-1.html#cb3-2"></a><span class="bu">print</span>(<span class="st">&quot;The eigenvalues of A:&quot;</span>, eigs)</span></code></pre></div>
<pre><code>## The eigenvalues of A: [0.19098301 1.30901699]</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="conjugate-gradient-methods-1.html#cb5-1"></a><span class="cf">if</span> (np.<span class="bu">all</span>(eigs<span class="op">&gt;</span><span class="dv">0</span>)):</span>
<span id="cb5-2"><a href="conjugate-gradient-methods-1.html#cb5-2"></a>    <span class="bu">print</span>(<span class="st">&quot;A is positive definite&quot;</span>)</span>
<span id="cb5-3"><a href="conjugate-gradient-methods-1.html#cb5-3"></a><span class="cf">elif</span> (np.<span class="bu">all</span>(eigs<span class="op">&gt;=</span><span class="dv">0</span>)):</span>
<span id="cb5-4"><a href="conjugate-gradient-methods-1.html#cb5-4"></a>    <span class="bu">print</span>(<span class="st">&quot;A is positive semi-definite&quot;</span>)</span>
<span id="cb5-5"><a href="conjugate-gradient-methods-1.html#cb5-5"></a><span class="cf">else</span>:</span>
<span id="cb5-6"><a href="conjugate-gradient-methods-1.html#cb5-6"></a>    <span class="bu">print</span>(<span class="st">&quot;A is negative definite&quot;</span>)</span></code></pre></div>
<pre><code>## A is positive definite</code></pre>
<p>We see that <span class="math inline">\(\mathbb{A}\)</span> is indeed positive definite. To check whether it is symmetric, we can check whether <span class="math inline">\(\mathbb{A}^T\)</span> equals <span class="math inline">\(\mathbb{A}\)</span>.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="conjugate-gradient-methods-1.html#cb7-1"></a><span class="cf">if</span> (A.T<span class="op">==</span>A).<span class="bu">all</span>()<span class="op">==</span><span class="va">True</span>: <span class="bu">print</span>(<span class="st">&quot;A is symmetric&quot;</span>)</span></code></pre></div>
<pre><code>## A is symmetric</code></pre>
<p>So <span class="math inline">\(\mathbb{A}\)</span> is symmetric too. Now we write the Python function <code>linear_CG()</code> that implements the <em>linear conjugate gradient algorithm</em></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="conjugate-gradient-methods-1.html#cb9-1"></a><span class="kw">def</span> linear_CG(x, A, b, epsilon):</span>
<span id="cb9-2"><a href="conjugate-gradient-methods-1.html#cb9-2"></a>    res <span class="op">=</span> A.dot(x) <span class="op">-</span> b <span class="co"># Initialize the residual</span></span>
<span id="cb9-3"><a href="conjugate-gradient-methods-1.html#cb9-3"></a>    delta <span class="op">=</span> <span class="op">-</span>res <span class="co"># Initialize the descent direction</span></span>
<span id="cb9-4"><a href="conjugate-gradient-methods-1.html#cb9-4"></a>    </span>
<span id="cb9-5"><a href="conjugate-gradient-methods-1.html#cb9-5"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb9-6"><a href="conjugate-gradient-methods-1.html#cb9-6"></a>        </span>
<span id="cb9-7"><a href="conjugate-gradient-methods-1.html#cb9-7"></a>        <span class="cf">if</span> np.linalg.norm(res) <span class="op">&lt;=</span> epsilon:</span>
<span id="cb9-8"><a href="conjugate-gradient-methods-1.html#cb9-8"></a>            <span class="cf">return</span> x, f(x) <span class="co"># Return the minimizer x* and the function value f(x*)</span></span>
<span id="cb9-9"><a href="conjugate-gradient-methods-1.html#cb9-9"></a>        </span>
<span id="cb9-10"><a href="conjugate-gradient-methods-1.html#cb9-10"></a>        D <span class="op">=</span> A.dot(delta)</span>
<span id="cb9-11"><a href="conjugate-gradient-methods-1.html#cb9-11"></a>        beta <span class="op">=</span> <span class="op">-</span>(res.dot(delta))<span class="op">/</span>(delta.dot(D)) <span class="co"># Line (11) in the algorithm</span></span>
<span id="cb9-12"><a href="conjugate-gradient-methods-1.html#cb9-12"></a>        x <span class="op">=</span> x <span class="op">+</span> beta<span class="op">*</span>delta <span class="co"># Generate the new iterate</span></span>
<span id="cb9-13"><a href="conjugate-gradient-methods-1.html#cb9-13"></a></span>
<span id="cb9-14"><a href="conjugate-gradient-methods-1.html#cb9-14"></a>        res <span class="op">=</span> A.dot(x) <span class="op">-</span> b <span class="co"># generate the new residual</span></span>
<span id="cb9-15"><a href="conjugate-gradient-methods-1.html#cb9-15"></a>        chi <span class="op">=</span> res.dot(D)<span class="op">/</span>(delta.dot(D)) <span class="co"># Line (14) in the algorithm </span></span>
<span id="cb9-16"><a href="conjugate-gradient-methods-1.html#cb9-16"></a>        delta <span class="op">=</span> chi<span class="op">*</span>delta <span class="op">-</span>  res <span class="co"># Generate the new descent direction</span></span></code></pre></div>
<p>Finally, we pass the parameters that we consider for this example to the function <code>linear_CG()</code>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="conjugate-gradient-methods-1.html#cb10-1"></a>linear_CG(np.array([<span class="fl">2.3</span>, <span class="fl">-2.2</span>]), A, b, <span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>)</span></code></pre></div>
<pre><code>## (array([-4.,  4.]), 0.0)</code></pre>
<p>We see that the solution is <span class="math inline">\(\mathbb{x^*} \sim \begin{bmatrix}-4 \\ 4 \end{bmatrix}\)</span> and the function value at this point is <span class="math inline">\(0\)</span>. We can verify the result is correct by following the trivial solution of Eq. <a href="conjugate-gradient-methods-1.html#eq:37">(5.37)</a>:</p>
<p><span class="math display" id="eq:38">\[\begin{align}
    \begin{bmatrix}x_1 \\ x_2\end{bmatrix} &amp;= \begin{bmatrix} \frac{1}{2} &amp; \frac{1}{2} \\ \frac{1}{2} &amp; 1\end{bmatrix}^{-1} \begin{bmatrix}0 \\2\end{bmatrix} \nonumber \\
    &amp;= \begin{bmatrix}-4 \\ 4\end{bmatrix} \tag{5.38}
\end{align}\]</span></p>
<p>We can even write a Python code to check the above case:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="conjugate-gradient-methods-1.html#cb12-1"></a>np.linalg.inv(A).dot(b)</span></code></pre></div>
<pre><code>## array([-4.,  4.])</code></pre>
<p>We see that our Python implementation of the <em>linear conjugate gradient algorithm</em> works perfectly. We will now discuss <em>nonlinear conjugate gradient algorithms</em> in the next section</p>
</div>
</div>
<div id="nonlinear-conjugate-gradient-algorithm" class="section level2">
<h2><span class="header-section-number">5.3</span> Nonlinear Conjugate Gradient Algorithm</h2>
<p>We can modify our <em>conjugate gradient method</em> to optimize convex nonlinear objective functions. The first method that we study under this class is the <em>Fletcher-Reeves</em> method.</p>
<div id="feltcher-reeves-algorithm" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Feltcher-Reeves Algorithm</h3>
<p>The first application of the <em>Conjugate Gradient Method</em> on nonlinear objective functions was introduced by Fletcher and Reeves. The directions <span class="math inline">\(\mathbb{\delta}_j\)</span> given by Fletcher and Reeves are mutually conjugate with respect to the symmetric positive definite matrix <span class="math inline">\(\mathbb{A}\)</span> in Eq. <a href="conjugate-gradient-methods-1.html#eq:1">(5.1)</a>, where the residual is given by Eq. <a href="conjugate-gradient-methods-1.html#eq:5">(5.5)</a>. The descent direction is given by,</p>
<p><span class="math display" id="eq:39">\[\begin{equation}
    \mathbb{\delta}_{j+1} =
    \begin{cases}
    -\nabla f(\mathbb{x}_j),\ \ j=0 \\
    -\nabla f(\mathbb{x}_j) + \chi_j\mathbb{\delta}_j,\ \ j=1, 2, \ldots, n-1 \tag{5.39}
    \end{cases}
\end{equation}\]</span></p>
<p>In the above equation,
<span class="math display" id="eq:40">\[\begin{equation}
    \mathbb{x}_j = \mathbb{x}_{j-1}+\beta_j\mathbb{\delta}_j \tag{5.40}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\beta_j\)</span> is the <span class="math inline">\(j^{th}\)</span> step length. <span class="math inline">\(\chi_j\)</span> in Eq. <a href="conjugate-gradient-methods-1.html#eq:39">(5.39)</a> is given by,
<span class="math display" id="eq:41">\[\begin{equation}
    \chi_j = \frac{\|\nabla f(\mathbb{x}_j)\|^2}{\|\nabla f(\mathbb{x}_{j-1})\|^2} \tag{5.41}
\end{equation}\]</span></p>
<p>The <em>Fletcher-Reeves Algorithm</em> is given below:</p>
<p><img src="img%2019.png" /></p>

<div class="example">
<span id="exm:unnamed-chunk-18" class="example"><strong>Example 5.2  </strong></span>Let us consider an objective function having the form,
<span class="math display" id="eq:42">\[\begin{equation}
    f(x_1, x_2) = x_1^4 - 2x_1^2x_2+x_1^2 + x_2^2-2x_1+1 \tag{5.42}
\end{equation}\]</span>
The function has a local minimizer at <span class="math inline">\(f(1, 1) = 0\)</span>. We will implement the <em>Fletcher-Reeves algorithm</em> in Python to figure out the minimizer. Let the starting iterate be given by <span class="math inline">\(\mathbb{x}_j = \begin{bmatrix}2 \\ -1.8 \end{bmatrix}\)</span>, the tolerance be <span class="math inline">\(\epsilon = 10^{-5}\)</span> and the constants to be used for determining the step length using the <em>strong Wolfe conditions</em> be <span class="math inline">\(\alpha_1=10^{-4}\)</span> and <span class="math inline">\(\alpha_2=0.38\)</span>. Let us first define the objective function and its gradient in Python.
</div>

<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="conjugate-gradient-methods-1.html#cb14-1"></a><span class="kw">def</span> func(x): <span class="co"># Objective function</span></span>
<span id="cb14-2"><a href="conjugate-gradient-methods-1.html#cb14-2"></a>    <span class="cf">return</span> x[<span class="dv">0</span>]<span class="op">**</span><span class="dv">4</span> <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>x[<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span><span class="op">*</span>x[<span class="dv">1</span>] <span class="op">+</span> x[<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> x[<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>x[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb14-3"><a href="conjugate-gradient-methods-1.html#cb14-3"></a></span>
<span id="cb14-4"><a href="conjugate-gradient-methods-1.html#cb14-4"></a>Df <span class="op">=</span> grad(func) <span class="co"># Gradient of the objective function</span></span></code></pre></div>
<p>Next we define the function <code>Fletcher_Reeves()</code> in Python:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="conjugate-gradient-methods-1.html#cb15-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> line_search</span>
<span id="cb15-2"><a href="conjugate-gradient-methods-1.html#cb15-2"></a>NORM <span class="op">=</span> np.linalg.norm</span>
<span id="cb15-3"><a href="conjugate-gradient-methods-1.html#cb15-3"></a></span>
<span id="cb15-4"><a href="conjugate-gradient-methods-1.html#cb15-4"></a><span class="kw">def</span> Fletcher_Reeves(Xj, tol, alpha_1, alpha_2):</span>
<span id="cb15-5"><a href="conjugate-gradient-methods-1.html#cb15-5"></a>    x1 <span class="op">=</span> [Xj[<span class="dv">0</span>]]</span>
<span id="cb15-6"><a href="conjugate-gradient-methods-1.html#cb15-6"></a>    x2 <span class="op">=</span> [Xj[<span class="dv">1</span>]]</span>
<span id="cb15-7"><a href="conjugate-gradient-methods-1.html#cb15-7"></a>    D <span class="op">=</span> Df(Xj)</span>
<span id="cb15-8"><a href="conjugate-gradient-methods-1.html#cb15-8"></a>    delta <span class="op">=</span> <span class="op">-</span>D <span class="co"># Initialize the descent direction</span></span>
<span id="cb15-9"><a href="conjugate-gradient-methods-1.html#cb15-9"></a>    </span>
<span id="cb15-10"><a href="conjugate-gradient-methods-1.html#cb15-10"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb15-11"><a href="conjugate-gradient-methods-1.html#cb15-11"></a>        start_point <span class="op">=</span> Xj <span class="co"># Start point for step length selection </span></span>
<span id="cb15-12"><a href="conjugate-gradient-methods-1.html#cb15-12"></a>        beta <span class="op">=</span> line_search(f<span class="op">=</span>func, myfprime<span class="op">=</span>Df, xk<span class="op">=</span>start_point, pk<span class="op">=</span>delta, c1<span class="op">=</span>alpha_1, c2<span class="op">=</span>alpha_2)[<span class="dv">0</span>] <span class="co"># Selecting the step length</span></span>
<span id="cb15-13"><a href="conjugate-gradient-methods-1.html#cb15-13"></a>        <span class="cf">if</span> beta<span class="op">!=</span><span class="va">None</span>:</span>
<span id="cb15-14"><a href="conjugate-gradient-methods-1.html#cb15-14"></a>            X <span class="op">=</span> Xj<span class="op">+</span> beta<span class="op">*</span>delta <span class="co">#Newly updated experimental point</span></span>
<span id="cb15-15"><a href="conjugate-gradient-methods-1.html#cb15-15"></a>        </span>
<span id="cb15-16"><a href="conjugate-gradient-methods-1.html#cb15-16"></a>        <span class="cf">if</span> NORM(Df(X)) <span class="op">&lt;</span> tol:</span>
<span id="cb15-17"><a href="conjugate-gradient-methods-1.html#cb15-17"></a>            x1 <span class="op">+=</span> [X[<span class="dv">0</span>], ]</span>
<span id="cb15-18"><a href="conjugate-gradient-methods-1.html#cb15-18"></a>            x2 <span class="op">+=</span> [X[<span class="dv">1</span>], ]</span>
<span id="cb15-19"><a href="conjugate-gradient-methods-1.html#cb15-19"></a>            <span class="cf">return</span> X, func(X) <span class="co"># Return the results</span></span>
<span id="cb15-20"><a href="conjugate-gradient-methods-1.html#cb15-20"></a>        <span class="cf">else</span>:</span>
<span id="cb15-21"><a href="conjugate-gradient-methods-1.html#cb15-21"></a>            Xj <span class="op">=</span> X</span>
<span id="cb15-22"><a href="conjugate-gradient-methods-1.html#cb15-22"></a>            d <span class="op">=</span> D <span class="co"># Gradient at the preceding experimental point</span></span>
<span id="cb15-23"><a href="conjugate-gradient-methods-1.html#cb15-23"></a>            D <span class="op">=</span> Df(Xj) <span class="co"># Gradient at the current experimental point</span></span>
<span id="cb15-24"><a href="conjugate-gradient-methods-1.html#cb15-24"></a>            chi <span class="op">=</span> NORM(D)<span class="op">**</span><span class="dv">2</span><span class="op">/</span>NORM(d)<span class="op">**</span><span class="dv">2</span> <span class="co"># Line (16) of the Fletcher-Reeves algorithm</span></span>
<span id="cb15-25"><a href="conjugate-gradient-methods-1.html#cb15-25"></a>            delta <span class="op">=</span> <span class="op">-</span>D <span class="op">+</span> chi<span class="op">*</span>delta <span class="co"># Newly updated descent direction</span></span>
<span id="cb15-26"><a href="conjugate-gradient-methods-1.html#cb15-26"></a>            x1 <span class="op">+=</span> [Xj[<span class="dv">0</span>], ]</span>
<span id="cb15-27"><a href="conjugate-gradient-methods-1.html#cb15-27"></a>            x2 <span class="op">+=</span> [Xj[<span class="dv">1</span>], ]</span></code></pre></div>
<p>According to our example we set our parameter values and pass them to the <code>Fletcher_Reeves()</code> function:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="conjugate-gradient-methods-1.html#cb16-1"></a>Fletcher_Reeves(np.array([<span class="fl">2.</span>, <span class="fl">-1.8</span>]), <span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">4</span>, <span class="fl">0.38</span>)</span></code></pre></div>
<pre><code>## (array([0.99999267, 0.99998207]), 6.445799449750211e-11)</code></pre>
<p>We notice that, for our choice of parameters, the algorithm has converged to the minimizer <span class="math inline">\(\mathbb{x}^* \sim \begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span> with <span class="math inline">\(f(\mathbb{x}^*) \sim 0\)</span>. The figure showing the optimization trajectory is shown below:</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>The optimization data for the same is given below:</p>
<pre><code>## +----+-----------+------------+--------------+--------------+
## |    |       x_1 |        x_2 |         f(X) |     ||grad|| |
## |----+-----------+------------+--------------+--------------|
## |  0 |  2        | -1.8       | 34.64        | 49.7707      |
## |  1 | -0.98032  | -1.08571   |  8.1108      | 12.6662      |
## |  2 |  1.08966  |  0.0472277 |  1.30794     |  5.6311      |
## |  3 |  0.642619 |  0.473047  |  0.131332    |  0.877485    |
## |  4 |  0.766371 |  0.46651   |  0.0691785   |  0.260336    |
## |  5 |  0.932517 |  0.704482  |  0.0318138   |  0.583346    |
## |  6 |  1.0149   |  1.06008   |  0.00112543  |  0.110081    |
## |  7 |  1.02357  |  1.0596    |  0.000697231 |  0.0238509   |
## |  8 |  1.02489  |  1.05473   |  0.000638128 |  0.0331525   |
## |  9 |  1.00544  |  0.999549  |  0.000158528 |  0.0609372   |
## | 10 |  0.996075 |  0.987011  |  4.19723e-05 |  0.016347    |
## | 11 |  0.994792 |  0.986923  |  3.43476e-05 |  0.00538401  |
## | 12 |  0.994466 |  0.987575  |  3.25511e-05 |  0.00620548  |
## | 13 |  0.9956   |  0.992867  |  2.20695e-05 |  0.015708    |
## | 14 |  0.999909 |  1.00171   |  3.59093e-06 |  0.008628    |
## | 15 |  1.00088  |  1.00254   |  1.3779e-06  |  0.00206337  |
## | 16 |  1.00102  |  1.00249   |  1.24228e-06 |  0.000925229 |
## | 17 |  1.00106  |  1.00226   |  1.14704e-06 |  0.00161353  |
## | 18 |  1.00056  |  1.00065   |  5.3011e-07  |  0.00313135  |
## | 19 |  0.999916 |  0.99956   |  8.14653e-08 |  0.00107299  |
## | 20 |  0.999816 |  0.999511  |  4.85294e-08 |  0.000269684 |
## | 21 |  0.999798 |  0.999526  |  4.57054e-08 |  0.000185146 |
## | 22 |  0.999803 |  0.999615  |  3.90603e-08 |  0.000435884 |
## | 23 |  0.99995  |  0.999991  |  1.08357e-08 |  0.000499645 |
## | 24 |  1.00003  |  1.00009   |  2.25348e-09 |  0.000130632 |
## | 25 |  1.00004  |  1.00009   |  1.75917e-09 |  3.97529e-05 |
## | 26 |  1.00004  |  1.00009   |  1.66947e-09 |  4.22905e-05 |
## | 27 |  1.00003  |  1.00006   |  1.1931e-09  |  0.000108964 |
## | 28 |  1        |  0.999989  |  2.11734e-10 |  6.79786e-05 |
## | 29 |  0.999994 |  0.999982  |  7.24881e-11 |  1.61034e-05 |
## | 30 |  0.999993 |  0.999982  |  6.4458e-11  |  6.72611e-06 |
## +----+-----------+------------+--------------+--------------+</code></pre>
<p>The algorithm reduces to the <em>linear conjugate gradient algorithm</em> if the objective function is chosen to be strongly convex quadratic. We notice that in the algorithm, we just need to compute the objective function and its gradient at each iterate and no Hessian computation is required. Next we discuss the <em>Polak-Ribiere algorithm</em>.</p>
</div>
<div id="polak-ribiere-algorithm" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Polak-Ribiere Algorithm</h3>
<p>One of the variants of the <em>Fletcher-Reeves algorithm</em> is the <em>Polak-Ribiere algorithm</em>, where, the <span class="math inline">\(\chi_j\)</span> in Eq. <a href="conjugate-gradient-methods-1.html#eq:39">(5.39)</a> is given by:
<span class="math display" id="eq:43">\[\begin{equation}
    \chi_j = \frac{[\nabla f(\mathbb{x}_j) - \nabla f(\mathbb{x}_{j-1})]^T\nabla f(\mathbb{x}_j)}{\|\nabla f(\mathbb{x}_{j-1})\|^2} \tag{5.43}
\end{equation}\]</span></p>
<p>One important characteristic to notice here is that, the <em>strong Wolfe conditions</em> do not guarantee the direction <span class="math inline">\(\mathbb{\delta}_j\)</span> will always be a descent direction in the <em>Polak-RIbiere algorithm</em>. Then, <span class="math inline">\(\chi\)</span> needs to modified in the following way:
<span class="math display" id="eq:44">\[\begin{equation}
    \chi_j = \max\{0, \chi_j\} \tag{5.44}
\end{equation}\]</span></p>
<p>This ensures that the <em>strong wolfe conditions</em> guarantee a descent direction . The <em>Polak-Ribiere algorithm</em> reduces back to the <em>Fletcher-Reeves algorithm</em> if the objective function is strongly convex quadratic, and the line search is exact. The <em>Polak-Ribiere Algorithm</em> is given below:</p>
<p><img src="img%2020.png" /></p>
<p>The <em>polak-Ribiere algorithm</em> is most often considered as more coherent than the <em>Fletcher-Reeves algorithm</em>.</p>

<div class="example">
<span id="exm:unnamed-chunk-24" class="example"><strong>Example 5.3  </strong></span>Let us again consider an objective function given in Eq. <a href="conjugate-gradient-methods-1.html#eq:42">(5.42)</a>. We will implement the <em>Polak-Ribiere algorithm</em> in Python to figure out the minimizer. Let the starting iterate be given by <span class="math inline">\(\mathbb{x}_j = \begin{bmatrix}-1.7 \\ -3.2 \end{bmatrix}\)</span>, the tolerance be <span class="math inline">\(\epsilon = 10^{-6}\)</span> and the constants to be used for determining the step length using the <em>strong Wolfe conditions</em> be <span class="math inline">\(\alpha_1=10^{-4}\)</span> and <span class="math inline">\(\alpha_2=0.2\)</span>. We define the function <code>Polak_Ribiere()</code> in Python:
</div>

<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="conjugate-gradient-methods-1.html#cb19-1"></a><span class="kw">def</span> Polak_Ribiere(Xj, tol, alpha_1, alpha_2):</span>
<span id="cb19-2"><a href="conjugate-gradient-methods-1.html#cb19-2"></a>    x1 <span class="op">=</span> [Xj[<span class="dv">0</span>]]</span>
<span id="cb19-3"><a href="conjugate-gradient-methods-1.html#cb19-3"></a>    x2 <span class="op">=</span> [Xj[<span class="dv">1</span>]]</span>
<span id="cb19-4"><a href="conjugate-gradient-methods-1.html#cb19-4"></a>    D <span class="op">=</span> Df(Xj)</span>
<span id="cb19-5"><a href="conjugate-gradient-methods-1.html#cb19-5"></a>    delta <span class="op">=</span> <span class="op">-</span>D <span class="co"># Initialize the descent direction</span></span>
<span id="cb19-6"><a href="conjugate-gradient-methods-1.html#cb19-6"></a>    </span>
<span id="cb19-7"><a href="conjugate-gradient-methods-1.html#cb19-7"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb19-8"><a href="conjugate-gradient-methods-1.html#cb19-8"></a>        start_point <span class="op">=</span> Xj <span class="co"># Start point for step length selection </span></span>
<span id="cb19-9"><a href="conjugate-gradient-methods-1.html#cb19-9"></a>        beta <span class="op">=</span> line_search(f<span class="op">=</span>func, myfprime<span class="op">=</span>Df, xk<span class="op">=</span>start_point, pk<span class="op">=</span>delta, c1<span class="op">=</span>alpha_1, c2<span class="op">=</span>alpha_2)[<span class="dv">0</span>] <span class="co"># Selecting the step length</span></span>
<span id="cb19-10"><a href="conjugate-gradient-methods-1.html#cb19-10"></a>        <span class="cf">if</span> beta<span class="op">!=</span><span class="va">None</span>:</span>
<span id="cb19-11"><a href="conjugate-gradient-methods-1.html#cb19-11"></a>            X <span class="op">=</span> Xj<span class="op">+</span> beta<span class="op">*</span>delta <span class="co"># Newly updated experimental point </span></span>
<span id="cb19-12"><a href="conjugate-gradient-methods-1.html#cb19-12"></a>        </span>
<span id="cb19-13"><a href="conjugate-gradient-methods-1.html#cb19-13"></a>        <span class="cf">if</span> NORM(Df(X)) <span class="op">&lt;</span> tol:</span>
<span id="cb19-14"><a href="conjugate-gradient-methods-1.html#cb19-14"></a>            x1 <span class="op">+=</span> [X[<span class="dv">0</span>], ]</span>
<span id="cb19-15"><a href="conjugate-gradient-methods-1.html#cb19-15"></a>            x2 <span class="op">+=</span> [X[<span class="dv">1</span>], ]</span>
<span id="cb19-16"><a href="conjugate-gradient-methods-1.html#cb19-16"></a>            <span class="cf">return</span> X, func(X) <span class="co"># Return the results</span></span>
<span id="cb19-17"><a href="conjugate-gradient-methods-1.html#cb19-17"></a>        <span class="cf">else</span>:</span>
<span id="cb19-18"><a href="conjugate-gradient-methods-1.html#cb19-18"></a>            Xj <span class="op">=</span> X</span>
<span id="cb19-19"><a href="conjugate-gradient-methods-1.html#cb19-19"></a>            d <span class="op">=</span> D <span class="co"># Gradient of the preceding experimental point</span></span>
<span id="cb19-20"><a href="conjugate-gradient-methods-1.html#cb19-20"></a>            D <span class="op">=</span> Df(Xj) <span class="co"># Gradient of the current experimental point</span></span>
<span id="cb19-21"><a href="conjugate-gradient-methods-1.html#cb19-21"></a>            chi <span class="op">=</span> (D<span class="op">-</span>d).dot(D)<span class="op">/</span>NORM(d)<span class="op">**</span><span class="dv">2</span> </span>
<span id="cb19-22"><a href="conjugate-gradient-methods-1.html#cb19-22"></a>            chi <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, chi) <span class="co"># Line (16) of the Polak-Ribiere Algorithm</span></span>
<span id="cb19-23"><a href="conjugate-gradient-methods-1.html#cb19-23"></a>            delta <span class="op">=</span> <span class="op">-</span>D <span class="op">+</span> chi<span class="op">*</span>delta <span class="co"># Newly updated direction</span></span>
<span id="cb19-24"><a href="conjugate-gradient-methods-1.html#cb19-24"></a>            x1 <span class="op">+=</span> [Xj[<span class="dv">0</span>], ]</span>
<span id="cb19-25"><a href="conjugate-gradient-methods-1.html#cb19-25"></a>            x2 <span class="op">+=</span> [Xj[<span class="dv">1</span>], ]</span></code></pre></div>
<p>According to our example we set our parameter values and pass them to the <code>Polak_Ribiere()</code> function:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="conjugate-gradient-methods-1.html#cb20-1"></a>Polak_Ribiere(np.array([<span class="op">-</span><span class="fl">1.7</span>, <span class="fl">-3.2</span>]), <span class="dv">10</span><span class="op">**-</span><span class="dv">6</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">4</span>, <span class="fl">0.2</span>)</span></code></pre></div>
<pre><code>## (array([1., 1.]), 0.0)</code></pre>
<p>We notice that, for our choice of parameters, the algorithm has converged to the minimizer <span class="math inline">\(\mathbb{x}^* \sim \begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span> with <span class="math inline">\(f(\mathbb{x}^*) \sim 0\)</span>. The Figure showing the optimization trajectory is shown below:</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-10-3.png" width="672" /></p>
<p>The optimization data for the same is given in the table below:</p>
<pre><code>## +----+-----------+------------+--------------+--------------+
## |    |       x_1 |        x_2 |         f(X) |     ||grad|| |
## |----+-----------+------------+--------------+--------------|
## |  0 | -1.7      | -3.2       | 44.3781      | 48.3706      |
## |  1 |  0.444052 | -2.64214   |  8.37083     |  6.90669     |
## |  2 |  0.160431 |  0.0845414 |  0.708334    |  1.7209      |
## |  3 |  0.813837 |  0.626715  |  0.0359251   |  0.266095    |
## |  4 |  0.851657 |  0.637222  |  0.0297668   |  0.176227    |
## |  5 |  0.985178 |  1.02861   |  0.00358778  |  0.283221    |
## |  6 |  1.01609  |  1.06259   |  0.00116756  |  0.1086      |
## |  7 |  1.0238   |  1.05744   |  0.000652461 |  0.0208901   |
## |  8 |  0.99978  |  1.00021   |  4.68434e-07 |  0.00329728  |
## |  9 |  1.00002  |  1.00004   |  2.62911e-10 |  1.34768e-05 |
## | 10 |  1        |  1         |  2.00728e-13 |  2.16127e-06 |
## | 11 |  1        |  1         |  0           |  1.79832e-10 |
## +----+-----------+------------+--------------+--------------+</code></pre>
<p>Next we study the <em>Hestenes-Stiefel algorithm</em>.</p>
</div>
<div id="hestenes-stiefel-algorithm" class="section level3">
<h3><span class="header-section-number">5.3.3</span> Hestenes-Stiefel Algorithm</h3>
<p>In this variant, Eq. <a href="conjugate-gradient-methods-1.html#eq:39">(5.39)</a> changes to:
<span class="math display" id="eq:45">\[\begin{equation}
    \chi_j = \frac{\nabla f(\mathbb{x}_j)^T[\nabla(\mathbb{x}_j) - \nabla f(\mathbb{x}_{j-1})]}{\mathbb{\delta}_j^T[\nabla(\mathbb{x}_j) - \nabla f(\mathbb{x}_{j-1})]} \tag{5.45}
\end{equation}\]</span></p>
<p>The <em>Hestenes-Stiefel algorithm</em> is given below:</p>
<p><img src="img%2021.png" /></p>
<p>The Python implementation is given by the Python function <code>Hestenes_Stiefel()</code>.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="conjugate-gradient-methods-1.html#cb23-1"></a><span class="kw">def</span> Hestenes_Stiefel(Xj, tol, alpha_1, alpha_2):</span>
<span id="cb23-2"><a href="conjugate-gradient-methods-1.html#cb23-2"></a>    x1 <span class="op">=</span> [Xj[<span class="dv">0</span>]]</span>
<span id="cb23-3"><a href="conjugate-gradient-methods-1.html#cb23-3"></a>    x2 <span class="op">=</span> [Xj[<span class="dv">1</span>]]</span>
<span id="cb23-4"><a href="conjugate-gradient-methods-1.html#cb23-4"></a>    D <span class="op">=</span> Df(Xj)</span>
<span id="cb23-5"><a href="conjugate-gradient-methods-1.html#cb23-5"></a>    delta <span class="op">=</span> <span class="op">-</span>D</span>
<span id="cb23-6"><a href="conjugate-gradient-methods-1.html#cb23-6"></a>    </span>
<span id="cb23-7"><a href="conjugate-gradient-methods-1.html#cb23-7"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb23-8"><a href="conjugate-gradient-methods-1.html#cb23-8"></a>        start_point <span class="op">=</span> Xj <span class="co"># Start point for step length selection </span></span>
<span id="cb23-9"><a href="conjugate-gradient-methods-1.html#cb23-9"></a>        beta <span class="op">=</span> line_search(f<span class="op">=</span>func, myfprime<span class="op">=</span>Df, xk<span class="op">=</span>start_point, pk<span class="op">=</span>delta, c1<span class="op">=</span>alpha_1, c2<span class="op">=</span>alpha_2)[<span class="dv">0</span>] <span class="co"># Selecting the step length</span></span>
<span id="cb23-10"><a href="conjugate-gradient-methods-1.html#cb23-10"></a>        <span class="cf">if</span> beta<span class="op">!=</span><span class="va">None</span>:</span>
<span id="cb23-11"><a href="conjugate-gradient-methods-1.html#cb23-11"></a>            X <span class="op">=</span> Xj<span class="op">+</span> beta<span class="op">*</span>delta</span>
<span id="cb23-12"><a href="conjugate-gradient-methods-1.html#cb23-12"></a>        </span>
<span id="cb23-13"><a href="conjugate-gradient-methods-1.html#cb23-13"></a>        <span class="cf">if</span> NORM(Df(X)) <span class="op">&lt;</span> tol:</span>
<span id="cb23-14"><a href="conjugate-gradient-methods-1.html#cb23-14"></a>            x1 <span class="op">+=</span> [X[<span class="dv">0</span>], ]</span>
<span id="cb23-15"><a href="conjugate-gradient-methods-1.html#cb23-15"></a>            x2 <span class="op">+=</span> [X[<span class="dv">1</span>], ]</span>
<span id="cb23-16"><a href="conjugate-gradient-methods-1.html#cb23-16"></a>            <span class="cf">return</span> X, func(X)</span>
<span id="cb23-17"><a href="conjugate-gradient-methods-1.html#cb23-17"></a>        <span class="cf">else</span>:</span>
<span id="cb23-18"><a href="conjugate-gradient-methods-1.html#cb23-18"></a>            Xj <span class="op">=</span> X</span>
<span id="cb23-19"><a href="conjugate-gradient-methods-1.html#cb23-19"></a>            d <span class="op">=</span> D</span>
<span id="cb23-20"><a href="conjugate-gradient-methods-1.html#cb23-20"></a>            D <span class="op">=</span> Df(Xj)</span>
<span id="cb23-21"><a href="conjugate-gradient-methods-1.html#cb23-21"></a>            chi <span class="op">=</span> D.dot(D <span class="op">-</span> d)<span class="op">/</span>delta.dot(D <span class="op">-</span> d) <span class="co"># See line (16)</span></span>
<span id="cb23-22"><a href="conjugate-gradient-methods-1.html#cb23-22"></a>            delta <span class="op">=</span> <span class="op">-</span>D <span class="op">+</span> chi<span class="op">*</span>delta</span>
<span id="cb23-23"><a href="conjugate-gradient-methods-1.html#cb23-23"></a>            x1 <span class="op">+=</span> [Xj[<span class="dv">0</span>], ]</span>
<span id="cb23-24"><a href="conjugate-gradient-methods-1.html#cb23-24"></a>            x2 <span class="op">+=</span> [Xj[<span class="dv">1</span>], ]</span></code></pre></div>
<p>The Python function can be used by the user to optimize and study any objective function provided reasonable parameters are passed to it. The variant that we look into next, is called the <em>Dai-Yuan algorithm</em>.</p>
</div>
<div id="dai-yuan-algorithm" class="section level3">
<h3><span class="header-section-number">5.3.4</span> Dai-Yuan Algorithm</h3>
<p>In this variant, Eq. <a href="conjugate-gradient-methods-1.html#eq:39">(5.39)</a> changes to:
<span class="math display" id="eq:46">\[\begin{equation}
    \chi_j = \frac{\|\nabla f(\mathbb{x}_j)\|^2}{\mathbb{\delta}_j^T[\nabla f(\mathbb{x}_j) - \nabla f(\mathbb{x}_{j-1})]} \tag{5.46}
\end{equation}\]</span></p>
<p>The <em>Dai-Yuan algorithm</em> [ref, Dai-Yuan, A nonlinear conjugate gradient method with a strong global convergence property] is given below:</p>
<p><img src="img%2022.png" /></p>
<p>Python implementation is given by the Python function <code>Dai_Yuan()</code>:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="conjugate-gradient-methods-1.html#cb24-1"></a><span class="kw">def</span> Dai_Yuan(Xj, tol, alpha_1, alpha_2):</span>
<span id="cb24-2"><a href="conjugate-gradient-methods-1.html#cb24-2"></a>    x1 <span class="op">=</span> [Xj[<span class="dv">0</span>]]</span>
<span id="cb24-3"><a href="conjugate-gradient-methods-1.html#cb24-3"></a>    x2 <span class="op">=</span> [Xj[<span class="dv">1</span>]]</span>
<span id="cb24-4"><a href="conjugate-gradient-methods-1.html#cb24-4"></a>    D <span class="op">=</span> Df(Xj)</span>
<span id="cb24-5"><a href="conjugate-gradient-methods-1.html#cb24-5"></a>    delta <span class="op">=</span> <span class="op">-</span>D</span>
<span id="cb24-6"><a href="conjugate-gradient-methods-1.html#cb24-6"></a>    </span>
<span id="cb24-7"><a href="conjugate-gradient-methods-1.html#cb24-7"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb24-8"><a href="conjugate-gradient-methods-1.html#cb24-8"></a>        start_point <span class="op">=</span> Xj <span class="co"># Start point for step length selection </span></span>
<span id="cb24-9"><a href="conjugate-gradient-methods-1.html#cb24-9"></a>        beta <span class="op">=</span> line_search(f<span class="op">=</span>func, myfprime<span class="op">=</span>Df, xk<span class="op">=</span>start_point, pk<span class="op">=</span>delta, c1<span class="op">=</span>alpha_1, c2<span class="op">=</span>alpha_2)[<span class="dv">0</span>] <span class="co"># Selecting the step length</span></span>
<span id="cb24-10"><a href="conjugate-gradient-methods-1.html#cb24-10"></a>        <span class="cf">if</span> beta<span class="op">!=</span><span class="va">None</span>:</span>
<span id="cb24-11"><a href="conjugate-gradient-methods-1.html#cb24-11"></a>            X <span class="op">=</span> Xj<span class="op">+</span> beta<span class="op">*</span>delta</span>
<span id="cb24-12"><a href="conjugate-gradient-methods-1.html#cb24-12"></a>        </span>
<span id="cb24-13"><a href="conjugate-gradient-methods-1.html#cb24-13"></a>        <span class="cf">if</span> NORM(Df(X)) <span class="op">&lt;</span> tol:</span>
<span id="cb24-14"><a href="conjugate-gradient-methods-1.html#cb24-14"></a>            x1 <span class="op">+=</span> [X[<span class="dv">0</span>], ]</span>
<span id="cb24-15"><a href="conjugate-gradient-methods-1.html#cb24-15"></a>            x2 <span class="op">+=</span> [X[<span class="dv">1</span>], ]</span>
<span id="cb24-16"><a href="conjugate-gradient-methods-1.html#cb24-16"></a>            <span class="cf">return</span> X, func(X)</span>
<span id="cb24-17"><a href="conjugate-gradient-methods-1.html#cb24-17"></a>        <span class="cf">else</span>:</span>
<span id="cb24-18"><a href="conjugate-gradient-methods-1.html#cb24-18"></a>            Xj <span class="op">=</span> X</span>
<span id="cb24-19"><a href="conjugate-gradient-methods-1.html#cb24-19"></a>            d <span class="op">=</span> D</span>
<span id="cb24-20"><a href="conjugate-gradient-methods-1.html#cb24-20"></a>            D <span class="op">=</span> Df(Xj)</span>
<span id="cb24-21"><a href="conjugate-gradient-methods-1.html#cb24-21"></a>            chi <span class="op">=</span> NORM(D)<span class="op">**</span><span class="dv">2</span><span class="op">/</span>delta.dot(D <span class="op">-</span> d) <span class="co"># See line (16)</span></span>
<span id="cb24-22"><a href="conjugate-gradient-methods-1.html#cb24-22"></a>            delta <span class="op">=</span> <span class="op">-</span>D <span class="op">+</span> chi<span class="op">*</span>delta</span>
<span id="cb24-23"><a href="conjugate-gradient-methods-1.html#cb24-23"></a>            x1 <span class="op">+=</span> [Xj[<span class="dv">0</span>], ]</span>
<span id="cb24-24"><a href="conjugate-gradient-methods-1.html#cb24-24"></a>            x2 <span class="op">+=</span> [Xj[<span class="dv">1</span>], ]</span></code></pre></div>
<p>The last variant that we will discuss is the <em>Hager-Zhang algorithm</em>.</p>
</div>
<div id="hager-zhang-algorithm" class="section level3">
<h3><span class="header-section-number">5.3.5</span> Hager-Zhang Algorithm</h3>
<p>In this variant, Eq. <a href="conjugate-gradient-methods-1.html#eq:39">(5.39)</a> changes to:
<span class="math display" id="eq:47">\[\begin{equation}
    \chi_j = \mathbb{M}_j^T\mathbb{N}_j \tag{5.47}
\end{equation}\]</span></p>
<p>where,</p>
<p><span class="math display" id="eq:48">\[\begin{equation}
    \mathbb{M}_j = \mathbb{Q}_j - 2\delta_j\frac{\|\mathbb{Q}_j\|^2}{\mathbb{\delta}_j^T\mathbb{Q}_j} \tag{5.48}
\end{equation}\]</span>
and
<span class="math display" id="eq:49">\[\begin{equation}
    \mathbb{N}_j = \frac{\nabla f(\mathbb{x}_j)}{\mathbb{\delta}_j^T\mathbb{Q}_j} \tag{5.49}
\end{equation}\]</span>
In the equations above, <span class="math inline">\(\mathbb{Q}_j\)</span> is actually given by
<span class="math display" id="eq:50">\[\begin{equation}
    \mathbb{Q}_j = \nabla f(\mathbb{x}_j) - \nabla f(\mathbb{x}_{j-1}) \tag{5.50}
\end{equation}\]</span></p>
<p>The <em>Hager-Zhang algorithm</em> is given below:</p>
<p><img src="img%2023.png" /></p>
<p>The Python implementation is given by the Python function <code>Hager_Zhang()</code>:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="conjugate-gradient-methods-1.html#cb25-1"></a><span class="kw">def</span> Hager_Zhang(Xj, tol, alpha_1, alpha_2):</span>
<span id="cb25-2"><a href="conjugate-gradient-methods-1.html#cb25-2"></a>    x1 <span class="op">=</span> [Xj[<span class="dv">0</span>]]</span>
<span id="cb25-3"><a href="conjugate-gradient-methods-1.html#cb25-3"></a>    x2 <span class="op">=</span> [Xj[<span class="dv">1</span>]]</span>
<span id="cb25-4"><a href="conjugate-gradient-methods-1.html#cb25-4"></a>    D <span class="op">=</span> Df(Xj)</span>
<span id="cb25-5"><a href="conjugate-gradient-methods-1.html#cb25-5"></a>    delta <span class="op">=</span> <span class="op">-</span>D</span>
<span id="cb25-6"><a href="conjugate-gradient-methods-1.html#cb25-6"></a>    </span>
<span id="cb25-7"><a href="conjugate-gradient-methods-1.html#cb25-7"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb25-8"><a href="conjugate-gradient-methods-1.html#cb25-8"></a>        start_point <span class="op">=</span> Xj <span class="co"># Start point for step length selection </span></span>
<span id="cb25-9"><a href="conjugate-gradient-methods-1.html#cb25-9"></a>        beta <span class="op">=</span> line_search(f<span class="op">=</span>func, myfprime<span class="op">=</span>Df, xk<span class="op">=</span>start_point, pk<span class="op">=</span>delta, c1<span class="op">=</span>alpha_1, c2<span class="op">=</span>alpha_2)[<span class="dv">0</span>] <span class="co"># Selecting the step length</span></span>
<span id="cb25-10"><a href="conjugate-gradient-methods-1.html#cb25-10"></a>        <span class="cf">if</span> beta<span class="op">!=</span><span class="va">None</span>:</span>
<span id="cb25-11"><a href="conjugate-gradient-methods-1.html#cb25-11"></a>            X <span class="op">=</span> Xj<span class="op">+</span> beta<span class="op">*</span>delta</span>
<span id="cb25-12"><a href="conjugate-gradient-methods-1.html#cb25-12"></a>        </span>
<span id="cb25-13"><a href="conjugate-gradient-methods-1.html#cb25-13"></a>        <span class="cf">if</span> NORM(Df(X)) <span class="op">&lt;</span> tol:</span>
<span id="cb25-14"><a href="conjugate-gradient-methods-1.html#cb25-14"></a>            x1 <span class="op">+=</span> [X[<span class="dv">0</span>], ]</span>
<span id="cb25-15"><a href="conjugate-gradient-methods-1.html#cb25-15"></a>            x2 <span class="op">+=</span> [X[<span class="dv">1</span>], ]</span>
<span id="cb25-16"><a href="conjugate-gradient-methods-1.html#cb25-16"></a>            <span class="cf">return</span> X, func(X)</span>
<span id="cb25-17"><a href="conjugate-gradient-methods-1.html#cb25-17"></a>        <span class="cf">else</span>:</span>
<span id="cb25-18"><a href="conjugate-gradient-methods-1.html#cb25-18"></a>            Xj <span class="op">=</span> X</span>
<span id="cb25-19"><a href="conjugate-gradient-methods-1.html#cb25-19"></a>            d <span class="op">=</span> D</span>
<span id="cb25-20"><a href="conjugate-gradient-methods-1.html#cb25-20"></a>            D <span class="op">=</span> Df(Xj)</span>
<span id="cb25-21"><a href="conjugate-gradient-methods-1.html#cb25-21"></a>            Q <span class="op">=</span> D <span class="op">-</span> d</span>
<span id="cb25-22"><a href="conjugate-gradient-methods-1.html#cb25-22"></a>            M <span class="op">=</span> Q <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>delta<span class="op">*</span>NORM(Q)<span class="op">**</span><span class="dv">2</span><span class="op">/</span>(delta.dot(Q))</span>
<span id="cb25-23"><a href="conjugate-gradient-methods-1.html#cb25-23"></a>            N <span class="op">=</span> D<span class="op">/</span>(delta.dot(Q))</span>
<span id="cb25-24"><a href="conjugate-gradient-methods-1.html#cb25-24"></a>            chi <span class="op">=</span> M.dot(N) <span class="co"># See line (19)</span></span>
<span id="cb25-25"><a href="conjugate-gradient-methods-1.html#cb25-25"></a>            delta <span class="op">=</span> <span class="op">-</span>D <span class="op">+</span> chi<span class="op">*</span>delta</span>
<span id="cb25-26"><a href="conjugate-gradient-methods-1.html#cb25-26"></a>            x1 <span class="op">+=</span> [Xj[<span class="dv">0</span>], ]</span>
<span id="cb25-27"><a href="conjugate-gradient-methods-1.html#cb25-27"></a>            x2 <span class="op">+=</span> [Xj[<span class="dv">1</span>], ]</span></code></pre></div>
<p>To use the Python function for optimizing any objective function, define an objective function <code>func()</code> and its gradient <code>Df()</code> first, in the same way that we have been doing till now.</p>
<p>Although <em>Polak-Ribiere algorithm</em> most often is more efficient than <em>Fletcher-Reeves</em> algorithm, it is not always the case, because more vector storage might be required for the former. The variants by Hager and Zhang are computationally more advanced and the line search methodologies are highly precise unlike the <em>Fletcher-Reeves algorithm</em> where the line search technique is inefficient. We will now introduce the <code>minimize()</code> function of the <code>scipy.optimize</code> module which provides us with multiple optimizers belonging to the <em>Conjugate Gradient class</em>, the <em>Quasi Newton class</em>, the <em>Trust Region class</em>, the <em>Zero Order Derivative class</em> and so on.</p>
</div>
</div>
<div id="the-scipy.optimize.minimize-function" class="section level2">
<h2><span class="header-section-number">5.4</span> The <code>scipy.optimize.minimize()</code> Function</h2>
<p>The <code>minimize()</code> function in the <code>scipy.optimize</code> module is used for minimization of objective functions having one or multiple variables. The parameters to be passed to this function are listed below:</p>
<ul>
<li><code>fun</code>: This is the objective function that is of <code>callable</code> Python data type,</li>
<li><code>x0</code>: This is the starting iterate and is of <code>ndarray</code> datatype,</li>
<li><code>method</code>: Although optional this is one of the most important parameters. This specifies the optimizer that needs to be used. This is either a <code>str</code> or is <code>callable</code>. Methods provided are as follows:
<ul>
<li><code>'CG'</code>: This is the <code>Polak-Ribiere algorithm</code> under the nonlinear conjugate gradient methods,</li>
<li><code>'BFGS'</code>: This is the algorithm by Broyden,Fletcher, Goldferb and Shanno under the <em>Quasi Newton class</em> of algorithms and will be discussed in the next chapter,</li>
<li><code>'dogleg'</code>: This is the <em>dogleg method</em> under the <em>trust-region class</em> of algorithms,</li>
<li><code>'trust-ncg'</code>: This is the <em>Newton Conjugate Gradient method</em> under the <em>trust-region class</em> of algorithms,</li>
<li><code>'trust-krylov'</code>: This is the <em>Newton Genralised Lanczos method</em> under the <em>trust-region class</em> of algorithms,</li>
<li><code>'trust-exact'</code>: This is a <em>trust region method</em> to exactly compute the solution of unconstrained quadratic problems,</li>
<li><code>'Newton-CG'</code>: This is the <em>line search Newton-CG method</em> also known as the <em>truncated Newton method</em>,</li>
<li><code>'Nelder-Mead'</code>: This is the <em>downhill simplex method of Nelder and mead</em> under the <em>zero-order derivative class</em> of algorithms,</li>
<li><code>'Powell'</code>: This is <em>derivative free conjugate direction method of Powell</em> under the <em>zero-order derivative class</em> of algorithms,</li>
<li><code>'TNC'</code>: This is again the <em>truncated Newton algorithm</em> but is used to optimize objective functions with variables subject to bounds,</li>
<li><code>'L-BFGS-B'</code>: This is an extension of the <em>limited-memory BFGS algorithm</em> used for optimizing objective functions with variables subject to bounds,</li>
<li><code>'COBYLA'</code>: This is the <em>Constrained Optimization by Linear Approximation (COBYLA) method</em> falling under the <em>constratined optimization</em> category of algorithms,</li>
<li><code>'SLSQP'</code>: This is the <em>Sequential Least Squares Programming method</em> used for minimizing objective functions of multiple variables taking into consideration any combinations of bounds on the variables, any equality or inequality constrains imposed on them,</li>
<li><code>'trust-constr'</code>: This is the <em>trust-region method</em> for solving constrained optimization tasks, and</li>
<li>Any custom optimization method with <code>callable</code> datatype can be passed too.</li>
</ul></li>
<li><code>jac</code>: This is an optional parameter and denotes the gradient of the objective function, to be passed to the <code>minimize()</code> function and should either be of <code>callable</code> or <code>bool</code> datatype,</li>
<li><code>hess</code>: This is optional too and denotes the Hessian matrix of the objective function, to be passed and should either be of <code>callable</code> or <code>bool</code> datatype,</li>
<li><code>hessp</code>: This denotes the Hessian matrix of the objective function multiplied by an arbitrary vector <span class="math inline">\(\mathbb{p}\)</span> used for methods <code>'Newton-CG'</code>, <code>'trust-ncg'</code>, <code>'trust-krylov'</code>, and <code>'trust-constr'</code>. This is an optional parameter and is a <code>callable</code> datatype,</li>
<li><code>bounds</code>: This denotes the bounds that are imposed on the methods dedicated to <em>bound-constrained minimization</em> techniques like <code>L-BFGS-B</code>, <code>TNC</code>, etc. This is an optional parameter and can be an instance of <code>Bounds</code> class provided by <code>scipy.optimize</code> module or a sequence of pairs i.e, <code>(min, max)</code> for each element of the iterate <span class="math inline">\(\mathbb{x}\)</span> or can be <code>None</code> too, indicating no bounds on the variables,</li>
<li><code>constraints</code>: This is an optional parameter and represents the constraints required in solving constrained optimization tasks. It is either a Python dictionary or a list of dictionaries and can be used to denote both linear and non-linear constraints. Detailed discussion about these are out of scope of the present book,</li>
<li><code>tol</code>: This optional parameter represents the termination tolerance value and is of <code>float</code> datatype. It is better to provide a solver-specific tolerance value to the <code>options</code> filed that we will discuss next,</li>
<li><code>options</code>: This is an optional parameter and is a dictionary of options that a specific optimizer provides,</li>
<li><code>args</code>: This is an optional parameter and is a tuple of extra arguments that might be passed to the objective function, its gradient or the Hessian matrix, and</li>
<li><code>callback</code>: This is an optional parameter too and is called after each iteration in the optimization task. This is of <code>callable</code> data type.</li>
</ul>
<p>The <code>minimize()</code> function returns the optimization result as a <code>OptimizeResult</code> object similar to the <code>minimize_scalar()</code> function mentioned before.</p>

<div class="example">
<span id="exm:unnamed-chunk-32" class="example"><strong>Example 5.4  </strong></span>Till now we have been considering objective functions of two variables. For this example, let us work with an objective function of four variables, called <em>Woodâ€™s function</em>. It is defined as:
<span class="math display" id="eq:51">\[\begin{align}
    f(x_1, x_2, x_3, x_4) &amp;= 100(x_2-x_1^2)^2+(1-x_1)^2+90(x_4-x_3^2)^2 \nonumber \\
    &amp;+(1-x_3)^2+10(x_2+x_4-2)^2+0.1(x_2-x_4)^2 \tag{5.51}
\end{align}\]</span>
</div>

<p>The minimizer of the objective function is at <span class="math inline">\(\mathbb{x}^* = \begin{bmatrix} 1 \\ 1 \\ 1\\ 1 \end{bmatrix}\)</span>. We will use the <code>minimize()</code> function with the <code>method</code> parameter set to <code>'CG'</code> indicating that we will be finding the minimizer of the objective function using the <em>Polak-Ribiere conjugate gradient</em> optimization method. Let the initial point for starting the optimization be <span class="math inline">\(\mathbb{x}_0 = \begin{bmatrix}-3 \\ -1 \\ -3 \\ -1 \end{bmatrix}\)</span> and the tolerance be <span class="math inline">\(10^-6\)</span>. Before starting we list down the important solver options that are provided by the <code>'CG'</code> method:</p>
<ul>
<li><code>disp</code>: This is of <code>bool</code> datatype and should be set to <code>True</code> to print out the relevant convergence messages,</li>
<li><code>gtol</code>: This is a <code>float</code> and denotes the gradient tolerance</li>
<li><code>return_all</code>: This is optional and is a <code>bool</code>. Should be set to <code>True</code> to print out the best solution at each of the optimization steps,</li>
<li><code>maxiter</code>: This is an <code>int</code> and specifies the maximum number of iterations the optimizer needs to perform, and so on.</li>
</ul>
<p>Let us first define the objective function in Python and its gradient using the <code>autograd</code> package:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="conjugate-gradient-methods-1.html#cb26-1"></a><span class="kw">def</span> func(x): <span class="co"># Objective function</span></span>
<span id="cb26-2"><a href="conjugate-gradient-methods-1.html#cb26-2"></a>    <span class="cf">return</span> <span class="dv">100</span><span class="op">*</span>(x[<span class="dv">1</span>] <span class="op">-</span> x[<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> x[<span class="dv">0</span>])<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">90</span><span class="op">*</span>(x[<span class="dv">3</span>]<span class="op">-</span> x[<span class="dv">2</span>]<span class="op">**</span><span class="dv">2</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> x[<span class="dv">2</span>])<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">10</span><span class="op">*</span>(x[<span class="dv">1</span>] <span class="op">+</span> x[<span class="dv">3</span>] <span class="op">-</span> <span class="dv">2</span>)<span class="op">**</span><span class="dv">2</span>  <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>(x[<span class="dv">1</span>] <span class="op">-</span> x[<span class="dv">3</span>])<span class="op">**</span><span class="dv">2</span>   </span>
<span id="cb26-3"><a href="conjugate-gradient-methods-1.html#cb26-3"></a></span>
<span id="cb26-4"><a href="conjugate-gradient-methods-1.html#cb26-4"></a>Df <span class="op">=</span> grad(func) <span class="co"># Gradient of the objective function</span></span></code></pre></div>
<p>Now use the minimize function and pass the relevant parameters as have been asked in the example to run the optimization:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="conjugate-gradient-methods-1.html#cb27-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb27-2"><a href="conjugate-gradient-methods-1.html#cb27-2"></a>res<span class="op">=</span>minimize(fun<span class="op">=</span>func, x0<span class="op">=</span>np.array([<span class="op">-</span><span class="fl">3.</span>, <span class="fl">-1.</span>, <span class="fl">-3.</span>, <span class="fl">-1.</span>]), jac<span class="op">=</span>Df, method<span class="op">=</span><span class="st">&#39;CG&#39;</span>, options<span class="op">=</span>{<span class="st">&#39;gtol&#39;</span>:<span class="dv">10</span><span class="op">**-</span><span class="dv">6</span>, <span class="st">&#39;disp&#39;</span>:<span class="va">True</span>, <span class="st">&#39;return_all&#39;</span>:<span class="va">True</span>})</span></code></pre></div>
<pre><code>## Optimization terminated successfully.
##          Current function value: 0.000000
##          Iterations: 83
##          Function evaluations: 174
##          Gradient evaluations: 174</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="conjugate-gradient-methods-1.html#cb29-1"></a>res.x, res.fun, res.jac</span></code></pre></div>
<pre><code>## (array([0.99999975, 0.9999995 , 1.00000025, 1.00000051]), 2.3130964272341054e-13, array([-8.87448405e-08, -2.73118175e-07, -4.59792845e-07,  8.22518607e-07]))</code></pre>
<p>We notice that the solver was successful in computing the minimizer <span class="math inline">\(\mathbb{x}^* \sim \begin{bmatrix}1 \\ 1\\ 1\\ 1\end{bmatrix}\)</span> and <span class="math inline">\(f(\mathbb{x}^*) \sim 0\)</span>. If the user wants to print out the optimization data, just write:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="conjugate-gradient-methods-1.html#cb31-1"></a><span class="cf">for</span> i <span class="kw">in</span> res.allvecs: <span class="bu">print</span>(i)</span></code></pre></div>
<pre><code>## [-3. -1. -3. -1.]
## [ 0.69823355 -0.35939992  0.32865658 -0.42099608]
## [0.31284575 0.5271373  0.52364445 0.19201798]
## [0.44279154 0.49499859 0.51144723 0.49981977]
## [0.65900174 0.3557516  0.6538751  0.52025596]
## [0.69008685 0.37741521 0.73311253 0.53578922]
## [0.63611928 0.49146367 0.76722078 0.59795104]
## [0.78014415 0.64604868 0.83944752 0.83214895]
## [0.85811872 0.7117484  0.93740013 0.90101943]
## [0.84695401 0.74185145 0.95950132 0.9131568 ]
## [0.90658664 0.87090049 1.00839357 1.0500565 ]
## [0.95334332 0.90822136 1.04308098 1.09541832]
## [0.95319811 0.90831054 1.04566101 1.09403639]
## [0.95318265 0.90837072 1.04585393 1.0937749 ]
## [0.95506232 0.91269031 1.03960088 1.08116439]
## [0.95584603 0.91324684 1.03906205 1.07984446]
## [0.98922593 0.97470583 1.00991282 1.01906174]
## [1.00321177 1.00606161 0.99518799 0.98982839]
## [1.00303298 1.00623765 0.99496438 0.99004434]
## [1.00363962 1.00692966 0.99530288 0.99107936]
## [1.00392335 1.00786236 0.99601194 0.99204146]
## [1.00390545 1.00785937 0.99603177 0.99205253]
## [1.00358133 1.00706373 0.99675995 0.99334117]
## [1.00314081 1.00628436 0.99736284 0.99470617]
## [1.00073784 1.00158721 0.99900666 0.99811784]
## [1.00063327 1.00127025 0.99915469 0.99832087]
## [1.00061046 1.00126233 0.99923812 0.99842796]
## [1.00061242 1.00122513 0.99942148 0.99883755]
## [1.00060526 1.00121531 0.99941678 0.99884005]
## [1.00057324 1.00114729 0.99941825 0.99883969]
## [1.00057171 1.00114681 0.99942174 0.99884002]
## [1.00052303 1.00101916 0.99952956 0.9990431 ]
## [1.00045019 1.00089767 0.99962881 0.99925245]
## [1.00040309 1.00083516 0.9996023  0.99921333]
## [1.00038522 1.00078306 0.99958937 0.99918369]
## [1.00038974 1.00078076 0.99959209 0.99918382]
## [1.00038857 1.00078129 0.99960581 0.99920361]
## [1.00039    1.00077928 0.99961238 0.99922919]
## [1.00038833 1.0007753  0.99962976 0.9992647 ]
## [1.00038625 1.00077369 0.99963556 0.99926767]
## [1.00034119 1.00066861 0.9996576  0.99930601]
## [1.00028035 1.00056988 0.99967331 0.99935076]
## [1.00027922 1.00056755 0.99967387 0.9993519 ]
## [1.00028372 1.0005669  0.99967698 0.99935355]
## [1.00029274 1.00058838 0.99970046 0.99939426]
## [1.00029422 1.00058928 0.99969944 0.99939998]
## [1.00029485 1.00058966 0.99970775 0.9994181 ]
## [1.00029387 1.00058911 0.99971178 0.99942071]
## [1.00028223 1.00056393 0.99973676 0.99946219]
## [1.00026215 1.00052414 0.99976265 0.99953009]
## [1.00020838 1.00041741 0.99984171 0.99968521]
## [1.00020314 1.00040189 0.99985123 0.99969334]
## [1.0000998  1.00020943 0.99988952 0.99977851]
## [1.00009559 1.00019182 0.99989265 0.99978588]
## [1.00009527 1.00019009 0.99989869 0.99979311]
## [1.0000922  1.00018583 0.99991283 0.99982461]
## [1.00009259 1.00018492 0.99991309 0.99982588]
## [1.00007447 1.00014686 0.99992339 0.99984287]
## [1.00006482 1.00013071 0.99992603 0.999852  ]
## [1.00006395 1.0001269  0.99992741 0.99985671]
## [1.00003546 1.00006939 0.99997221 0.999943  ]
## [1.00003374 1.00006769 0.99997299 0.99994565]
## [1.00002781 1.00005447 0.99997447 0.99995153]
## [1.00002077 1.00004129 0.99997796 0.99995632]
## [1.00002061 1.00004138 0.99997818 0.99995631]
## [1.00002088 1.00004144 0.99997914 0.99995775]
## [1.00002082 1.00004162 0.99997981 0.99995945]
## [1.00002069 1.00004154 0.99997975 0.99995952]
## [1.0000187  1.00003718 0.99998018 0.99996071]
## [1.00001797 1.0000361  0.99998054 0.99996102]
## [1.00000594 1.00001185 0.99999233 0.99998467]
## [1.00000571 1.00001185 0.99999255 0.99998523]
## [1.0000056  1.00001159 0.99999438 0.99998891]
## [1.00000575 1.00001149 0.9999945  0.99998898]
## [1.00000567 1.00001139 0.99999455 0.99998897]
## [1.00000504 1.00001014 0.99999457 0.99998917]
## [1.00000501 1.00001001 0.99999462 0.99998922]
## [1.00000404 1.00000839 0.99999551 0.99999091]
## [1.00000111 1.00000202 0.99999863 0.99999742]
## [0.99999964 0.99999926 1.00000014 1.00000031]
## [0.99999963 0.99999928 1.00000016 1.00000032]
## [0.99999973 0.99999945 1.00000025 1.00000047]
## [0.99999974 0.99999947 1.00000025 1.0000005 ]
## [0.99999975 0.9999995  1.00000025 1.00000051]</code></pre>
<p>This completes our discussion of the <em>conjugate gradient methods</em> for optimization. In the next chapter we will study the <em>quasi Newton methods</em> elaborately.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="line-search-descent-methods.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="quasi-newton-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/indrag49/Numerical-Optimization/edit/master/05-Conjugate_Gradient_Methods.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/indrag49/Numerical-Optimization/blob/master/05-Conjugate_Gradient_Methods.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
