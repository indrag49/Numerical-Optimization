<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Conjugate Gradient Methods | Introduction to Mathematical Optimization</title>
  <meta name="description" content="A book for teaching introductory numerical optimization algorithms with Python" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Conjugate Gradient Methods | Introduction to Mathematical Optimization" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://indrag49.github.io/Numerical-Optimization/" />
  
  <meta property="og:description" content="A book for teaching introductory numerical optimization algorithms with Python" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Conjugate Gradient Methods | Introduction to Mathematical Optimization" />
  
  <meta name="twitter:description" content="A book for teaching introductory numerical optimization algorithms with Python" />
  

<meta name="author" content="Indranil Ghosh" />


<meta name="date" content="2021-07-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="line-search-descent-methods.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Mathematical Optimization</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> What is Numerical Optimization?</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction-to-optimization"><i class="fa fa-check"></i><b>1.1</b> Introduction to Optimization</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#a-solution"><i class="fa fa-check"></i><b>1.2</b> A Solution</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#maximization"><i class="fa fa-check"></i><b>1.3</b> Maximization</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#feasible-region"><i class="fa fa-check"></i><b>1.4</b> Feasible Region</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#discrete-optimization-problems"><i class="fa fa-check"></i><b>1.5</b> Discrete Optimization Problems</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#linear-programming-problems"><i class="fa fa-check"></i><b>1.6</b> Linear Programming Problems</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#stochastic-optimization-problems"><i class="fa fa-check"></i><b>1.7</b> Stochastic Optimization Problems</a></li>
<li class="chapter" data-level="1.8" data-path="index.html"><a href="index.html#scaling-of-decision-variables"><i class="fa fa-check"></i><b>1.8</b> Scaling of Decision Variables</a></li>
<li class="chapter" data-level="1.9" data-path="index.html"><a href="index.html#gradient-vector-and-hessian-matrix-of-the-objective-function"><i class="fa fa-check"></i><b>1.9</b> Gradient Vector and Hessian Matrix of the Objective Function</a></li>
<li class="chapter" data-level="1.10" data-path="index.html"><a href="index.html#directional-derivative-of-the-objective-function"><i class="fa fa-check"></i><b>1.10</b> Directional Derivative of the Objective Function</a></li>
<li class="chapter" data-level="1.11" data-path="index.html"><a href="index.html#positive-definite-and-positive-semi-definite-matrices"><i class="fa fa-check"></i><b>1.11</b> Positive Definite and Positive Semi-definite Matrices</a></li>
<li class="chapter" data-level="1.12" data-path="index.html"><a href="index.html#what-is-convexity"><i class="fa fa-check"></i><b>1.12</b> What is Convexity?</a></li>
<li class="chapter" data-level="1.13" data-path="index.html"><a href="index.html#numerical-optimization-algorithms"><i class="fa fa-check"></i><b>1.13</b> Numerical Optimization Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html"><i class="fa fa-check"></i><b>2</b> Introduction to Unconstrained Optimization</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#the-unconstrained-optimization-problem"><i class="fa fa-check"></i><b>2.1</b> The Unconstrained Optimization Problem</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#smooth-functions"><i class="fa fa-check"></i><b>2.2</b> Smooth Functions</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#taylors-theorem"><i class="fa fa-check"></i><b>2.3</b> Taylor’s Theorem</a></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#necessary-and-sufficient-conditions-for-local-minimizer-in-unconstrained-optimization"><i class="fa fa-check"></i><b>2.4</b> Necessary and Sufficient Conditions for Local Minimizer in Unconstrained Optimization</a><ul>
<li class="chapter" data-level="2.4.1" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#first-order-necessary-condition"><i class="fa fa-check"></i><b>2.4.1</b> First-Order Necessary Condition</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#second-order-necessary-conditions"><i class="fa fa-check"></i><b>2.4.2</b> Second-Order Necessary Conditions</a></li>
<li class="chapter" data-level="2.4.3" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#second-order-sufficient-conditions"><i class="fa fa-check"></i><b>2.4.3</b> Second-Order Sufficient Conditions</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#algorithms-for-solving-unconstrained-minimization-tasks"><i class="fa fa-check"></i><b>2.5</b> Algorithms for Solving Unconstrained Minimization Tasks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html"><i class="fa fa-check"></i><b>3</b> Solving One Dimensional Optimization Problems</a><ul>
<li class="chapter" data-level="3.1" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#one-dimensional-optimization-problems"><i class="fa fa-check"></i><b>3.1</b> One Dimensional Optimization Problems</a></li>
<li class="chapter" data-level="3.2" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#what-is-a-unimodal-function"><i class="fa fa-check"></i><b>3.2</b> What is a Unimodal Function?</a></li>
<li class="chapter" data-level="3.3" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#fibonacci-search-method"><i class="fa fa-check"></i><b>3.3</b> Fibonacci Search Method</a></li>
<li class="chapter" data-level="3.4" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#golden-section-search-method"><i class="fa fa-check"></i><b>3.4</b> Golden Section Search Method</a></li>
<li class="chapter" data-level="3.5" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#powells-quadratic-interpolation-method"><i class="fa fa-check"></i><b>3.5</b> Powell’s Quadratic Interpolation Method</a></li>
<li class="chapter" data-level="3.6" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#inverse-quadratic-interpolation-method"><i class="fa fa-check"></i><b>3.6</b> Inverse Quadratic Interpolation Method</a></li>
<li class="chapter" data-level="3.7" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#newtons-method"><i class="fa fa-check"></i><b>3.7</b> Newton’s Method</a></li>
<li class="chapter" data-level="3.8" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#halleys-method"><i class="fa fa-check"></i><b>3.8</b> Halley’s Method</a></li>
<li class="chapter" data-level="3.9" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#secant-method"><i class="fa fa-check"></i><b>3.9</b> Secant Method</a></li>
<li class="chapter" data-level="3.10" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#bisection-method"><i class="fa fa-check"></i><b>3.10</b> Bisection Method</a></li>
<li class="chapter" data-level="3.11" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#brents-method"><i class="fa fa-check"></i><b>3.11</b> Brent’s Method</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html"><i class="fa fa-check"></i><b>4</b> Line Search Descent Methods</a><ul>
<li class="chapter" data-level="4.1" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#introduction-to-line-search-descent-methods-for-unconstrained-minimization"><i class="fa fa-check"></i><b>4.1</b> Introduction to Line Search Descent Methods for Unconstrained Minimization</a></li>
<li class="chapter" data-level="4.2" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#selection-of-step-length"><i class="fa fa-check"></i><b>4.2</b> Selection of Step Length</a><ul>
<li class="chapter" data-level="4.2.1" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#the-wolfe-conditions"><i class="fa fa-check"></i><b>4.2.1</b> The Wolfe Conditions</a></li>
<li class="chapter" data-level="4.2.2" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#an-algorithm-for-the-strong-wolfe-conditions"><i class="fa fa-check"></i><b>4.2.2</b> An Algorithm for the Strong Wolfe Conditions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#first-order-line-search-gradient-descent-method-the-steepest-descent-algorithm"><i class="fa fa-check"></i><b>4.3</b> First Order Line Search Gradient Descent Method: The Steepest Descent Algorithm</a></li>
<li class="chapter" data-level="4.4" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#conjugate-gradient-methods"><i class="fa fa-check"></i><b>4.4</b> Conjugate Gradient Methods</a></li>
<li class="chapter" data-level="4.5" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#second-order-line-search-gradient-descent-method"><i class="fa fa-check"></i><b>4.5</b> Second Order Line Search Gradient Descent Method</a></li>
<li class="chapter" data-level="4.6" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#marquardt-method"><i class="fa fa-check"></i><b>4.6</b> Marquardt Method</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html"><i class="fa fa-check"></i><b>5</b> Conjugate Gradient Methods</a><ul>
<li class="chapter" data-level="5.1" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#introduction-to-conjugate-gradient-methods"><i class="fa fa-check"></i><b>5.1</b> Introduction to Conjugate Gradient Methods</a></li>
<li class="chapter" data-level="5.2" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#linear-conjugate-gradient-algorithm"><i class="fa fa-check"></i><b>5.2</b> Linear Conjugate Gradient Algorithm</a><ul>
<li class="chapter" data-level="5.2.1" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#mutual-conjugacy"><i class="fa fa-check"></i><b>5.2.1</b> Mutual Conjugacy</a></li>
<li class="chapter" data-level="5.2.2" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#conjugate-direction-algorithm"><i class="fa fa-check"></i><b>5.2.2</b> Conjugate Direction Algorithm</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Mathematical Optimization</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="conjugate-gradient-methods-1" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Conjugate Gradient Methods</h1>
<p>This chapter is dedicated to studying the <em>Conjugate Gradient Methods</em> in detail. The Linear and Non-linear versions of the CG methods have been discussed with five sub classes falling under the nonlinear CG method class. The five nonlinear CG methods that have been discussed are: <em>Flethcher-Reeves method</em>, <em>Polak-Ribiere method</em>, <em>Hestenes-Stiefel method</em>, <em>Dai-Yuan method</em> and <em>Hager-Zhang method</em>. Mathematical proofs have been provided wherever necessary. Python implementations of the algorithms have been included along with optimization examples. The chapter ends with introducing a specific Python function called the <code>scipy.optimize.minimize()</code> function that can be used to work with the <em>Polak-Ribiere</em> CG method.</p>
<hr />
<div id="introduction-to-conjugate-gradient-methods" class="section level2">
<h2><span class="header-section-number">5.1</span> Introduction to Conjugate Gradient Methods</h2>
<p>The <em>conjugate gradient methods</em> are frequently used for solving large linear systems of equations and also for solving nonlinear optimization problems. This let us characterize the <em>conjugate gradient methods</em> into two classes:</p>
<ul>
<li><strong>Linear Conjugate Gradient Method</strong>: This is an iterative method to solve large linear systems where the coefficient matrices are positive definite. This can be treated as a replacement of the <em>Gaussian elimination method</em> in numerical analysis.</li>
<li><strong>nonlinear Conjugate Gradient method</strong>: This is used for solving nonlinear optimization problems. We will study five methods under this class:
<ul>
<li><em>Fletcher-Reeves algorithm</em>,</li>
<li><em>Polak-Ribiere algorithm</em>,</li>
<li><em>Hestenes-Stiefel algorithm</em>,</li>
<li><em>Dai-Yuan algorithm</em>, and</li>
<li><em>Hager-Zhang algorithm</em>.</li>
</ul></li>
</ul>
</div>
<div id="linear-conjugate-gradient-algorithm" class="section level2">
<h2><span class="header-section-number">5.2</span> Linear Conjugate Gradient Algorithm</h2>
<p>Suppose we want to find the minimizer of an objective function, having the quadratic form:
<span class="math display" id="eq:1">\[\begin{equation}
    f(\mathbb{x}) = \frac{1}{2}\mathbb{x}^T\mathbb{A}\mathbb{x} - \mathbb{b}^T\mathbb{x} \tag{5.1}
\end{equation}\]</span>
where, <span class="math inline">\(\mathbb{A}\)</span> is a <span class="math inline">\(n \times n\)</span> symmetric positive definite matrix.The problem can be formulated as:
<span class="math display" id="eq:2">\[\begin{equation}
    \underset{\mathbb{x}\in \mathbb{R}^n}{\min} f(\mathbb{x}) = \frac{1}{2}\mathbb{x}^T\mathbb{A}\mathbb{x} - \mathbb{b}^T\mathbb{x} \tag{5.2}
\end{equation}\]</span></p>
<p>Eq. <a href="conjugate-gradient-methods-1.html#eq:2">(5.2)</a> can be equivalently stated as the problem of solving the linear system of equations given by:
<span class="math display" id="eq:3">\[\begin{equation}
    \mathbb{A}\mathbb{x} = \mathbb{b} \tag{5.3}
\end{equation}\]</span></p>
<p>We use the <em>linear conjugate gradient method</em> to solve Eq. <a href="conjugate-gradient-methods-1.html#eq:3">(5.3)</a>.</p>
<p>The <em>residual</em> of a linear system of equations, given by Eq. <a href="conjugate-gradient-methods-1.html#eq:3">(5.3)</a> is defined as:
<span class="math display" id="eq:4">\[\begin{equation}
    r(\mathbb{x}) = \mathbb{A}\mathbb{x} - \mathbb{b} \tag{5.4}
\end{equation}\]</span></p>

<div class="theorem">
<span id="thm:unnamed-chunk-1" class="theorem"><strong>Theorem 5.1  </strong></span>The gradient of the objective function given by Eq. <a href="conjugate-gradient-methods-1.html#eq:1">(5.1)</a> is equal to the residual of the linear system given by Eq. <a href="conjugate-gradient-methods-1.html#eq:4">(5.4)</a>.
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> From Eq. <a href="conjugate-gradient-methods-1.html#eq:1">(5.1)</a>, we see that the gradient of the objective function is:
<span class="math display" id="eq:5">\[\begin{equation}
    \nabla f(\mathbb{x}) = \mathbb{A}\mathbb{x} - \mathbb{b} = r(\mathbb{x}) \tag{5.5}
\end{equation}\]</span>
This proves the theorem.
</div>

<div id="mutual-conjugacy" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Mutual Conjugacy</h3>
<p>For a given symmetric positive definite matrix <span class="math inline">\(\mathbb{A}\)</span>, two vectors <span class="math inline">\(\mathbb{v}, \mathbb{w} \neq \mathbb{0} \in \mathbb{R}^n\)</span> are defined to be <em>mutually conjugate</em> if the following condition is satisfied:
<span class="math display" id="eq:6">\[\begin{equation}
    \mathbb{v}^T\mathbb{A}\mathbb{w} = 0 \tag{5.6}
\end{equation}\]</span></p>

<div class="theorem">
<span id="thm:unnamed-chunk-3" class="theorem"><strong>Theorem 5.2  </strong></span>A set of <em>mutually conjugate</em> vectors <span class="math inline">\(\mathbb{v}_j, j=1, 2, \ldots\)</span> with respect to a positive definite symmetric matrix <span class="math inline">\(\mathbb{A}\)</span>, forms a basis in <span class="math inline">\(\mathbb{R}^n\)</span>, i.e., the set is linearly independent.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Above theorem equivalently states that, for <span class="math inline">\(\mathbb{x} \in \mathbb{R}^n\)</span>, the following condition is satisfied:
<span class="math display" id="eq:7">\[\begin{equation}
    \mathbb{x} = \sum_{j=1}^n\lambda_j\mathbb{v}_j \tag{5.7}
\end{equation}\]</span>
where,
<span class="math display" id="eq:8">\[\begin{equation}
    \lambda_j = \frac{\mathbb{v}_j^T\mathbb{A}\mathbb{x}}{\mathbb{v}_j^T\mathbb{A}\mathbb{v}_j} \tag{5.8}
\end{equation}\]</span>
Let us consider the linear combination,
<span class="math display" id="eq:9">\[\begin{equation}
    \sum_{j=1}^n c_j\mathbb{v}_j = \mathbb{0} \tag{5.9}
\end{equation}\]</span>
Multiplying the above equation with the matrix <span class="math inline">\(\mathbb{A}\)</span>, we have,
<span class="math display" id="eq:10">\[\begin{equation}
    \sum_{j=1}^n c_j \mathbb{A}\mathbb{v}_j=\mathbb{0} \tag{5.10}
\end{equation}\]</span>
Since the vectors <span class="math inline">\(\mathbb{v}_j\)</span> are mutually conjugate with respect to the matrix <span class="math inline">\(\mathbb{A}\)</span>, from Eq. <a href="conjugate-gradient-methods-1.html#eq:6">(5.6)</a>, we can write that,
<span class="math display" id="eq:11">\[\begin{equation}
    c_j\mathbb{v}_j^T\mathbb{A}\mathbb{v}_j = 0 \tag{5.11}
\end{equation}\]</span></p>
From the facts that <span class="math inline">\(\mathbb{A}\)</span> is positive definite and that <span class="math inline">\(\mathbb{v}_j\)</span> never equals <span class="math inline">\(\mathbb{0}\)</span>, from Eq. <a href="conjugate-gradient-methods-1.html#eq:11">(5.11)</a> we can state that, <span class="math inline">\(c_j=0\)</span> for <span class="math inline">\(j=1, 2, \ldots, n\)</span>. This proves the fact that the set of vectors <span class="math inline">\(\mathbb{v}_j\)</span> is linearly independent and may be used as a basis. Therefore, there exists a unique set <span class="math inline">\(\lambda_j, j=1, 2, \ldots, n\)</span> for any <span class="math inline">\(\mathbb{x} \in \mathbb{R}^n\)</span>, such that Eq. <a href="conjugate-gradient-methods-1.html#eq:7">(5.7)</a> is satisfied. The positive definiteness of <span class="math inline">\(\mathbb{A}\)</span> leads to the fact that,
<span class="math display" id="eq:12">\[\begin{equation}
    \mathbb{v}_j^T\mathbb{A}\mathbb{x} = \lambda_j\mathbb{v}_j^T\mathbb{A}\mathbb{x} \tag{5.12}
\end{equation}\]</span>
Finally, from Eq. <a href="conjugate-gradient-methods-1.html#eq:12">(5.12)</a> we can write that,
<span class="math display" id="eq:13">\[\begin{equation}
    \lambda_j = \frac{\mathbb{v}_j^T\mathbb{A}\mathbb{x}}{\mathbb{v}_j^T\mathbb{A}\mathbb{v}_j} \tag{5.13}
\end{equation}\]</span>
The proves the theorem.
</div>

</div>
<div id="conjugate-direction-algorithm" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Conjugate Direction Algorithm</h3>
<p>For our optimization task, where we aim to minimize the objective function <span class="math inline">\(f(\mathbb{x})\)</span>, where <span class="math inline">\(\mathbb{x} \in \mathbb{R}^n\)</span>, let <span class="math inline">\(\mathbb{x}_0\)</span> be the starting iterate and the conjugate directions be set as <span class="math inline">\({\mathbb{\delta}_j}, j=1, 2, \ldots, n-1\)</span>. The successive iterates are generated by following:</p>
<p><span class="math display" id="eq:14">\[\begin{equation}
    \mathbb{x}_j = \mathbb{x}_{j-1}+\beta_j\mathbb{\delta}_j \tag{5.14}
\end{equation}\]</span></p>
<p>This <span class="math inline">\(\beta_j\)</span> is the minimizer of the function <span class="math inline">\(f(\mathbb{x}_{j-1}+\beta\delta_j)\)</span>. We will now find the explicit form of <span class="math inline">\(\beta_j\)</span>. From Eq. <a href="conjugate-gradient-methods-1.html#eq:1">(5.1)</a> we can write that,</p>
<p><span class="math display" id="eq:15">\[\begin{align}
f(x_{j-1}+\beta \mathbb{\delta}_j) &amp;= \frac{1}{2}[(\mathbb{x}_{j-1}+\beta\mathbb{\delta}_j)^T\mathbb{A}(\mathbb{x}_{j-1}+\beta\mathbb{\delta}_j)] - \mathbb{b}^T(\mathbb{x}_{j-1}+\beta\mathbb{\delta}_j) \nonumber \\
&amp;= \frac{1}{2}[\mathbb{x}_{j-1}^T\mathbb{A}\mathbb{x}_{j-1}+2\beta\mathbb{x}_{j-1}^T\mathbb{A}\mathbb{\delta}_j + \beta^2\mathbb{\delta}_j^T\mathbb{A}\mathbb{\delta}_j] - \mathbb{b}^T(\mathbb{x}_{j-1}+\beta\mathbb{\delta}_j) \tag{5.15}
\end{align}\]</span></p>
<p>Now, differentiating Eq. <a href="conjugate-gradient-methods-1.html#eq:15">(5.15)</a> with respect to <span class="math inline">\(\beta\)</span> and setting it to <span class="math inline">\(0\)</span>, we get,
<span class="math display" id="eq:17" id="eq:16">\[\begin{align}
&amp; \frac{\partial f(\mathbb{x}_{j-1}+\beta\mathbb{\delta}_j)}{\partial \beta} = 0 \tag{5.16} \\
&amp;\implies \mathbb{x}_{j-1}^T\mathbb{A}\mathbb{x}_{j-1} + \beta\mathbb{\delta}_j\mathbb{A}\mathbb{\delta}_j - \mathbb{b}^T\mathbb{\delta}_j = 0 \nonumber \\
&amp;\implies (\mathbb{x}_{j-1}^T\mathbb{A} - \mathbb{b}^T)\mathbb{\delta}_j + \beta\mathbb{\delta}_j^T\mathbb{A}\mathbb{\delta}_j = 0 \tag{5.17}
\end{align}\]</span></p>
<p>Now, from Eq. <a href="conjugate-gradient-methods-1.html#eq:4">(5.4)</a> we can write,
<span class="math display" id="eq:18">\[\begin{equation}
    \mathbb{r}_j^T=(\mathbb{A}\mathbb{x}_{j-1}-\mathbb{b})^T = \mathbb{x}_{j-1}^T\mathbb{A} - \mathbb{b}^T \tag{5.18}
\end{equation}\]</span>
where, we have used the fact that <span class="math inline">\(\mathbb{A}^T=\mathbb{A}\)</span>. So, from Eq. <a href="conjugate-gradient-methods-1.html#eq:17">(5.17)</a> we can write,
<span class="math display" id="eq:19">\[\begin{equation}
    \mathbb{r}_j^T\mathbb{\delta}_j+\beta\mathbb{\delta}_j^T\mathbb{A}\mathbb{\delta}_j = 0 \tag{5.19}
\end{equation}\]</span></p>
<p>This finally fetches us,
<span class="math display" id="eq:20">\[\begin{equation}
    \beta_j = -\frac{\mathbb{r}_j^T\mathbb{\delta}_j}{\mathbb{\delta}_j^T\mathbb{A}\mathbb{\delta}_j} \tag{5.20}
\end{equation}\]</span>
Eq. <a href="conjugate-gradient-methods-1.html#eq:20">(5.20)</a> is equivalent to the step-length formulation given by Eq. <a href="#eq:26">(<strong>??</strong>)</a>.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-5" class="theorem"><strong>Theorem 5.3  </strong></span>The convergence of the conjugate direction algorithm, given by Eq. <a href="conjugate-gradient-methods-1.html#eq:14">(5.14)</a> and Eq. <a href="conjugate-gradient-methods-1.html#eq:20">(5.20)</a>, to its solution, takes place in at most <span class="math inline">\(n\)</span> steps, where <span class="math inline">\(\mathbb{x}_0\in \mathbb{R}^n\)</span> is the given initial iterate.
</div>

<p><strong>This Chapter is under construction</strong></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="line-search-descent-methods.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/indrag49/Numerical-Optimization/edit/master/05-Conjugate_Gradient_Methods.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/indrag49/Numerical-Optimization/blob/master/05-Conjugate_Gradient_Methods.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
