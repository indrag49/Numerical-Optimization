[["index.html", "Introduction to Mathematical Optimization with Python Chapter 1 What is Numerical Optimization? 1.1 Introduction to Optimization 1.2 A Solution 1.3 Maximization 1.4 Feasible Region 1.5 Discrete Optimization Problems 1.6 Linear Programming Problems 1.7 Stochastic Optimization Problems 1.8 Scaling of Decision Variables 1.9 Gradient Vector and Hessian Matrix of the Objective Function", " Introduction to Mathematical Optimization with Python Indranil Ghosh 2021-05-17 Chapter 1 What is Numerical Optimization? This chapter gives an introduction to the basics of numerical optimization and will help build the tools required for our in-depth understanding in the later chapters. Some fundamental linear algebra concepts will be touched which will be required for further studies in optimization along with introduction to simple Python codes. 1.1 Introduction to Optimization Let \\(f(\\mathbf{x})\\) be a scalar function of a vector of variables \\(\\mathbf{x} = \\begin{pmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\in \\mathbb{R}^n\\). Numerical Optimization is the minimization or maximization of this function \\(f\\) subject to constraints on \\(\\mathbf{x}\\). This \\(f\\) is a scalar function of \\(\\mathbf{x}\\), also known as the objective function and the continuous components \\(x_i \\in \\mathbf{x}\\) are called the decision variables. The optimization problem is formulated in the following way: \\[\\begin{align} &amp;\\!\\min_{\\mathbf{x} \\in \\mathbb{R}^n} &amp;\\qquad&amp; f(\\mathbf{x}) \\\\ &amp;\\text{subject to} &amp; &amp; g_k(\\mathbf{x}) \\leq 0,\\ k=1,2,..., m\\\\ &amp; &amp; &amp; h_k(\\mathbf{x}) = 0,\\ k=1,2,..., r\\\\ &amp; &amp; &amp; m,r &lt; n.\\tag{1.1} \\end{align}\\] Here, \\(g_k(\\mathbf{x})\\) and \\(h_k(\\mathbf{x})\\) are scalar functions too (like \\(f(\\mathbf{x})\\)) and are called constraint functions. The constraint functions define some specific equations and/or inequalities that \\(\\mathbf{x}\\) should satisfy. 1.2 A Solution Definition 1.1 A solution of \\(f(\\mathbf{x})\\) is a point \\(\\mathbf{x^*}\\) which denotes the optimum vector that solves equation (1.1), corresponding to the optimum value \\(f(\\mathbf{x^*})\\). In case of a minimization problem, the optimum vector \\(\\mathbf{x^*}\\) is referred to as the global minimizer of \\(f\\), and \\(f\\) attains the least possible value at \\(\\mathbf{x^*}\\). To design an algorithm that finds out the global minimizer for a function is quite difficult, as in most cases we do not have the idea of the overall shape of \\(f\\). Mostly our knowledge is restricted to a local portion of \\(f\\). Definition 1.2 A point \\(\\mathbf{x^*}\\) is called a global minimizer of \\(f\\) if \\(f(\\mathbf{x^*}) \\leq f(\\mathbf{x}) \\forall\\ x\\). Definition 1.3 A point \\(\\mathbf{x^*}\\) is called a local minimizer of \\(f\\) if there is a neighborhood \\(\\mathcal{N}\\) of \\(\\mathbf{x^*}\\) such that \\(f(\\mathbf{x^*}) \\leq f(\\mathbf{x}) \\forall\\ x \\in \\mathcal{N}\\). Definition 1.4 A point \\(\\mathbf{x^*}\\) is called a strong local minimizer of \\(f\\) if there is a neighborhood \\(\\mathcal{N}\\) of \\(\\mathbf{x^*}\\) such that \\(f(\\mathbf{x^*}) &lt; f(\\mathbf{x}) \\forall\\ x \\in \\mathcal{N}\\), with \\(\\mathbf{x} \\neq \\mathbf{x}^*\\). Definition 1.5 For an objective function \\(f(\\mathbf{x})\\) where, \\(\\mathbf{x} \\in \\mathbb{R}^2\\), a point \\(\\mathbf{x}^s=\\begin{bmatrix} x_1^s \\\\ x_2^s \\end{bmatrix}\\) is called a saddle point if \\(\\forall\\ \\mathbf{x}\\), there exists an \\(\\epsilon&gt;0\\), such that the following conditions are satisfied: \\(\\frac{\\partial f}{\\partial x_1}(\\mathbf{x}) \\mid_{(x_1^s, x_2^s)} &lt; \\epsilon\\), \\(\\frac{\\partial f}{\\partial x_2}(\\mathbf{x}) \\mid_{(x_1^s, x_2^s)} &lt; \\epsilon\\), and \\([\\frac{\\partial^2 f}{\\partial x_1^2}(\\mathbf{x}) \\frac{\\partial^2 f}{\\partial x_2^2}(\\mathbf{x}) - (\\frac{\\partial^2f}{\\partial x_1 \\partial x_2}(\\mathbf{x}))^2]\\mid_{(x_1^s, x_2^s)} &lt; 0\\) generating the following chain of inequalities: \\(f(\\mathbf{x})\\mid_{(x_1, x_2^s)} \\leq f(\\mathbf{x})\\mid_{(x_1^s, x_2^s)} \\leq f(\\mathbf{x})\\mid_{(x_1^s, x_2)}\\). An example of a saddle point is shown below: 1.3 Maximization We just defined a problem as our optimization task. We could do the same with a problem with little tweaks. The problem \\(\\underset{\\mathbf{x} \\in \\mathbb{R}^n}{max} f(\\mathbf{x})\\) can be formulated as: \\[\\begin{equation} \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{max} f(\\mathbf{x}) = - \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{min}\\{- f(\\mathbf{x})\\} \\tag{1.2} \\end{equation}\\] We then apply any minimization technique after setting \\(\\hat{f}(\\mathbf{x}) = - f(\\mathbf{x})\\). Further, for the inequality constraints for the maximization problem, given by \\(g_k(\\mathbf{x}) \\geq 0\\), we set \\[\\begin{equation} \\hat{g}_k(\\mathbf{x})=-g_k(\\mathbf{x}) \\tag{1.3} \\end{equation}\\] The problem thus has become, \\[\\begin{align} &amp;\\!\\min_{\\mathbf{x} \\in \\mathbb{R}^n} &amp;\\qquad&amp; \\hat{f}(\\mathbf{x})\\\\ &amp;\\text{subject to} &amp; &amp; \\hat{g}_k(\\mathbf{x}) \\leq 0,\\ k=1,2,..., m\\\\ &amp; &amp; &amp; h_k(\\mathbf{x}) = 0,\\ k=1,2,..., r\\\\ &amp; &amp; &amp; m,r &lt; n.\\tag{1.4} \\end{align}\\] After the solution \\(\\mathbf{x^*}\\) is computed, the maximum value of the problem is given by: \\(-\\hat{f}(\\mathbf{x^*})\\). 1.4 Feasible Region Definition 1.6 A feasible region is the set of those points which satisfy all the constraints provided. 1.5 Discrete Optimization Problems Definition 1.7 The optimization problems whose variables \\(\\mathbf{x}\\) take on integer values, and the constraints have the form either \\(x_i \\in \\mathcal{Z}\\) or \\(x_i \\in \\{0, 1\\}\\) are called discrete optimization problems. The above class of problems are also sometimes called integer programming problems. The fundamental characteristic of a discrete optimization problem is that, \\(x_i\\) is drawn from a countable set. 1.6 Linear Programming Problems The class of optimization problems where both the objective function \\(f(\\mathbf{x})\\) and the constraints are linear functions of the variable vector \\(\\mathbf{x}\\), are called the linear programming problems. A linear programming problem can be formulated in the following way: \\[\\begin{align} &amp;\\!\\min_{\\mathbf{x} \\in \\mathbb{R}^n} &amp;\\qquad&amp; f(\\mathbf{x})=\\mathbf{c}^T\\mathbf{x},\\\\ &amp;\\text{subject to} &amp; &amp; \\mathbf{A}\\mathbf{x} \\leq \\mathbf{b},\\\\ &amp; &amp; &amp; \\mathbf{x} \\geq \\mathbf{0},\\\\ &amp; &amp; &amp; \\mathbf{c} \\in \\mathbb{R}^n, \\mathbf{b} \\in \\mathbb{R}^m, \\mathbf{A}\\in \\mathbb{R}^{m \\times n}. \\tag{1.5} \\end{align}\\] 1.7 Stochastic Optimization Problems Definition 1.8 The class of optimization problems, where the decision variables \\(x_i \\in \\mathbf{x}\\) depend on the outcomes of a random phenomenon besides consisting of random objective function and constraints are called stochastic optimization problems. Some examples of stochastic optimization methods are: simulated annealing, quantum annealing, genetic algorithms, etc. 1.8 Scaling of Decision Variables While formulating optimization problems, it must be guaranteed that the scale of the decision variables are approximately of the same order. If this is not taken care of, some optimization algorithms that are sensitive to scaling will perform poorly and will flounder to converge to the solution. Two of the fundamental fields that get disturbed due to poor scaling are computing the optimized step lengths and the numerical gradients. One of the widely accepted best practices is to make the decision variables dimensionless and vary them approximately between 0 and 1. One should always prefer optimization algorithms that are not sensitive to scaling. 1.9 Gradient Vector and Hessian Matrix of the Objective Function Definition 1.9 For a differentiable objective function \\(f(\\mathbf{x}): \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), its given by \\(\\nabla f(\\mathbf{x}): \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\), is defined at the point \\(\\mathbf{x}\\) in the \\(n\\)-dimensional space as the vector of first order partial derivatives: \\[\\begin{equation} \\nabla f(\\mathbf{x})= \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1}(\\mathbf{x})\\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n}(\\mathbf{x}) \\end{pmatrix}\\tag{1.6} \\end{equation}\\] Now, if \\(f(\\mathbf{x})\\) is smooth, the gradient vector \\(\\nabla f(\\mathbf{x})\\) is always perpendicular to the contours at the point \\(\\mathbf{x}\\). The gradient vector is thus in the direction of the maximum increase of \\(f(\\mathbf{x})\\). Look at the figure below. Definition 1.10 For a twice continuously differentiable function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), its given by \\(\\mathbf{H}(f(\\mathbf{x}))\\) is defined at the point \\(\\mathbf{x}\\) in the \\(n \\times n\\)-dimensional space as the matrix of second order partial derivatives: \\[\\begin{equation} \\mathbf{H} f(\\mathbf{x})=\\frac{\\partial ^2 f}{\\partial x_i \\partial x_j} = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2}(\\mathbf{x}) &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2}(\\mathbf{x}) &amp; \\ldots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n}(\\mathbf{x})\\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1}(\\mathbf{x}) &amp; \\frac{\\partial^2 f}{\\partial x_2^2}(\\mathbf{x}) &amp; \\ldots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n}(\\mathbf{x}) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1}(\\mathbf{x}) &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2}(\\mathbf{x}) &amp; \\ldots &amp; \\frac{\\partial^2 f}{\\partial x_n^2}(\\mathbf{x}) \\end{pmatrix}\\tag{1.7} \\end{equation}\\] One important relation that we will keep in mind is that the is the of the of \\(f(\\mathbf{x})\\), where the of a vector-valued function \\(\\mathbf{F}(\\mathbf{x})\\) is the matrix of all its first order partial derivatives, given by, \\(\\mathbf{JF}(\\mathbf{x})= \\begin{pmatrix} \\frac{\\partial \\mathbf{F}}{\\partial x_1} &amp; \\ldots \\frac{\\partial \\mathbf{F}}{\\partial x_n} \\end{pmatrix}\\). The relation is as followed: \\[\\begin{equation} \\mathbf{H} f(\\mathbf{x}) = \\mathbf{J}(\\nabla f(\\mathbf{x})) \\tag{1.8} \\end{equation}\\] Let us consider an example now. Example 1.1 Let an objective function be \\(f(\\mathbf{x}) = 2x_1x_2^3+3x_2^2x_3 + x_3^3x_1\\). We will find out the gradient vector \\(\\nabla f(\\mathbf{x})\\) and the Hessian matrix \\(\\mathbf{H} f(\\mathbf{x})\\) at the point \\(\\mathbf{p} = \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\end{pmatrix}\\). The gradient vector is \\(\\nabla f(\\mathbf{x}) = \\begin{pmatrix} 2x_2^3+x_3^3 \\\\ 6x_1x_2^2+6x_2x_3 \\\\ 3x_2^2+3x_3^2x_1 \\end{pmatrix}\\). So \\(\\nabla f(\\mathbf{x})| \\mathbf{p} = \\begin{pmatrix} 43 \\\\ 60 \\\\ 39 \\end{pmatrix}\\). The Hessian matrix is therefore given by, \\(\\mathbf{H}f(\\mathbf{x}) = \\begin{pmatrix} 0 &amp; 6x_2^2 &amp; 3x_3^2 \\\\ 6x_2^2 &amp; 12x_1x_2+6x_3 &amp; 6x_2 \\\\ 3x_3^2 &amp; 6x_2 &amp; 6x_3x_1 \\end{pmatrix}\\) and at point \\(\\mathbf{p}\\), \\(\\mathbf{H} f(\\mathbf{x})|\\mathbf{p} = \\begin{pmatrix} 0 &amp; 24 &amp; 27 \\\\ 24 &amp; 42 &amp; 12 \\\\ 27 &amp; 12 &amp; 18 \\end{pmatrix}\\). We will try to work out the same example with Python scripting now. For that we need an extra package called autograd [https://github.com/HIPS/autograd], besides the numpy[https://numpy.org/] package. The autograd package is used for automatically differentiating native Python and Numpy code. Fundamentally autograd is used in gradient-based optimization. First pip install the autograd package pip install autograd Now, after it is downloaded, we type the following in our notebook: import autograd.numpy as au from autograd import grad, jacobian p = np.array([1, 2, 3], dtype=float) def f(x): # Objective function return 2*x[0]*x[1]**3+3*x[1]**2*x[2]+x[2]**3*x[0] grad_f = grad(f) # gradient of the objective function hessian_f = jacobian(grad_f) # Hessian of the objective function print(&quot;gradient vector:&quot;,grad_f(p)) ## gradient vector: [43. 60. 39.] print(&quot;Hessian matrix:\\n&quot;,hessian_f(p)) ## Hessian matrix: ## [[ 0. 24. 27.] ## [24. 42. 12.] ## [27. 12. 18.]] "]]
