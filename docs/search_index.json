[["index.html", "Introduction to Mathematical Optimization with Python Book Description", " Introduction to Mathematical Optimization with Python Indranil Ghosh 2021-06-14 Book Description This is an open-source introductory book on numerical/mathematical optimization aimed at audience who want to start with the subject matter with hands on implementation of the methods with Python. I plan to make this an open source collaborative project which will grow with time with legitimate contributions from the comnmunity. This book aims to cover the following: Introduction to Numerical Optimization Introduction to Unconstrained Optimization Solving one dimensional optimization problems Line Search Descent Method Conjugate Gradient Methods Quasi-Newton Methods "],["licence.html", "Licence", " Licence This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. "],["contributor-covenant-code-of-conduct.html", "Contributor Covenant Code of Conduct 0.1 Our Pledge 0.2 Our Standards 0.3 Enforcement Responsibilities 0.4 Scope 0.5 Enforcement 0.6 Enforcement Guidelines 0.7 Attribution", " Contributor Covenant Code of Conduct 0.1 Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. 0.2 Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting 0.3 Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. 0.4 Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. 0.5 Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at Email. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. 0.6 Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 0.6.1 1. Correction Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 0.6.2 2. Warning Community Impact: A violation through a single incident or series of actions. Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 0.6.3 3. Temporary Ban Community Impact: A serious violation of community standards, including sustained inappropriate behavior. Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 0.6.4 4. Permanent Ban Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence: A permanent ban from any sort of public interaction within the community. 0.7 Attribution This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozillas code of conduct enforcement ladder. For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations. "],["what-is-numerical-optimization.html", "Chapter 1 What is Numerical Optimization? 1.1 Introduction to Optimization 1.2 A Solution 1.3 Maximization 1.4 Feasible Region 1.5 Discrete Optimization Problems 1.6 Linear Programming Problems 1.7 Stochastic Optimization Problems 1.8 Scaling of Decision Variables 1.9 Gradient Vector and Hessian Matrix of the Objective Function 1.10 Directional Derivative of the Objective Function 1.11 Positive Definite and Positive Semi-definite Matrices 1.12 What is Convexity? 1.13 Numerical Optimization Algorithms", " Chapter 1 What is Numerical Optimization? This chapter gives an introduction to the basics of numerical optimization and will help build the tools required for our in-depth understanding in the later chapters. Some fundamental linear algebra concepts will be touched which will be required for further studies in optimization along with introduction to simple Python codes. 1.1 Introduction to Optimization Let \\(f(\\mathbf{x})\\) be a scalar function of a vector of variables \\(\\mathbf{x} = \\begin{pmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\in \\mathbb{R}^n\\). Numerical Optimization is the minimization or maximization of this function \\(f\\) subject to constraints on \\(\\mathbf{x}\\). This \\(f\\) is a scalar function of \\(\\mathbf{x}\\), also known as the objective function and the continuous components \\(x_i \\in \\mathbf{x}\\) are called the decision variables. The optimization problem is formulated in the following way: \\[\\begin{align} &amp;\\!\\min_{\\mathbf{x} \\in \\mathbb{R}^n} &amp;\\qquad&amp; f(\\mathbf{x}) \\\\ &amp;\\text{subject to} &amp; &amp; g_k(\\mathbf{x}) \\leq 0,\\ k=1,2,..., m\\\\ &amp; &amp; &amp; h_k(\\mathbf{x}) = 0,\\ k=1,2,..., r\\\\ &amp; &amp; &amp; m,r &lt; n.\\tag{1.1} \\end{align}\\] Here, \\(g_k(\\mathbf{x})\\) and \\(h_k(\\mathbf{x})\\) are scalar functions too (like \\(f(\\mathbf{x})\\)) and are called constraint functions. The constraint functions define some specific equations and/or inequalities that \\(\\mathbf{x}\\) should satisfy. 1.2 A Solution Definition 1.1 A solution of \\(f(\\mathbf{x})\\) is a point \\(\\mathbf{x^*}\\) which denotes the optimum vector that solves equation (1.1), corresponding to the optimum value \\(f(\\mathbf{x^*})\\). In case of a minimization problem, the optimum vector \\(\\mathbf{x^*}\\) is referred to as the global minimizer of \\(f\\), and \\(f\\) attains the least possible value at \\(\\mathbf{x^*}\\). To design an algorithm that finds out the global minimizer for a function is quite difficult, as in most cases we do not have the idea of the overall shape of \\(f\\). Mostly our knowledge is restricted to a local portion of \\(f\\). Definition 1.2 A point \\(\\mathbf{x^*}\\) is called a global minimizer of \\(f\\) if \\(f(\\mathbf{x^*}) \\leq f(\\mathbf{x}) \\forall\\ x\\). Definition 1.3 A point \\(\\mathbf{x^*}\\) is called a local minimizer of \\(f\\) if there is a neighborhood \\(\\mathcal{N}\\) of \\(\\mathbf{x^*}\\) such that \\(f(\\mathbf{x^*}) \\leq f(\\mathbf{x}) \\forall\\ x \\in \\mathcal{N}\\). Definition 1.4 A point \\(\\mathbf{x^*}\\) is called a strong local minimizer of \\(f\\) if there is a neighborhood \\(\\mathcal{N}\\) of \\(\\mathbf{x^*}\\) such that \\(f(\\mathbf{x^*}) &lt; f(\\mathbf{x}) \\forall\\ x \\in \\mathcal{N}\\), with \\(\\mathbf{x} \\neq \\mathbf{x}^*\\). Definition 1.5 For an objective function \\(f(\\mathbf{x})\\) where, \\(\\mathbf{x} \\in \\mathbb{R}^2\\), a point \\(\\mathbf{x}^s=\\begin{bmatrix} x_1^s \\\\ x_2^s \\end{bmatrix}\\) is called a saddle point if \\(\\forall\\ \\mathbf{x}\\), there exists an \\(\\epsilon&gt;0\\), such that the following conditions are satisfied: \\(\\frac{\\partial f}{\\partial x_1}(\\mathbf{x}) \\mid_{(x_1^s, x_2^s)} &lt; \\epsilon\\), \\(\\frac{\\partial f}{\\partial x_2}(\\mathbf{x}) \\mid_{(x_1^s, x_2^s)} &lt; \\epsilon\\), and \\([\\frac{\\partial^2 f}{\\partial x_1^2}(\\mathbf{x}) \\frac{\\partial^2 f}{\\partial x_2^2}(\\mathbf{x}) - (\\frac{\\partial^2f}{\\partial x_1 \\partial x_2}(\\mathbf{x}))^2]\\mid_{(x_1^s, x_2^s)} &lt; 0\\) generating the following chain of inequalities: \\(f(\\mathbf{x})\\mid_{(x_1, x_2^s)} \\leq f(\\mathbf{x})\\mid_{(x_1^s, x_2^s)} \\leq f(\\mathbf{x})\\mid_{(x_1^s, x_2)}\\). An example of a saddle point is shown below: 1.3 Maximization We just defined a minimization problem as our optimization task. We could do the same with a maximization problem with little tweaks. The problem \\(\\underset{\\mathbf{x} \\in \\mathbb{R}^n}{max} f(\\mathbf{x})\\) can be formulated as: \\[\\begin{equation} \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{max} f(\\mathbf{x}) = - \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{min}\\{- f(\\mathbf{x})\\} \\tag{1.2} \\end{equation}\\] We then apply any minimization technique after setting \\(\\hat{f}(\\mathbf{x}) = - f(\\mathbf{x})\\). Further, for the inequality constraints for the maximization problem, given by \\(g_k(\\mathbf{x}) \\geq 0\\), we set \\[\\begin{equation} \\hat{g}_k(\\mathbf{x})=-g_k(\\mathbf{x}) \\tag{1.3} \\end{equation}\\] The problem thus has become, \\[\\begin{align} &amp;\\!\\min_{\\mathbf{x} \\in \\mathbb{R}^n} &amp;\\qquad&amp; \\hat{f}(\\mathbf{x})\\\\ &amp;\\text{subject to} &amp; &amp; \\hat{g}_k(\\mathbf{x}) \\leq 0,\\ k=1,2,..., m\\\\ &amp; &amp; &amp; h_k(\\mathbf{x}) = 0,\\ k=1,2,..., r\\\\ &amp; &amp; &amp; m,r &lt; n.\\tag{1.4} \\end{align}\\] After the solution \\(\\mathbf{x^*}\\) is computed, the maximum value of the problem is given by: \\(-\\hat{f}(\\mathbf{x^*})\\). 1.4 Feasible Region Definition 1.6 A feasible region is the set of those points which satisfy all the constraints provided. 1.5 Discrete Optimization Problems Definition 1.7 The optimization problems whose variables \\(\\mathbf{x}\\) take on integer values, and the constraints have the form either \\(x_i \\in \\mathcal{Z}\\) or \\(x_i \\in \\{0, 1\\}\\) are called discrete optimization problems. The above class of problems are also sometimes called integer programming problems. The fundamental characteristic of a discrete optimization problem is that, \\(x_i\\) is drawn from a countable set. 1.6 Linear Programming Problems The class of optimization problems where both the objective function \\(f(\\mathbf{x})\\) and the constraints are linear functions of the variable vector \\(\\mathbf{x}\\), are called the linear programming problems. A linear programming problem can be formulated in the following way: \\[\\begin{align} &amp;\\!\\min_{\\mathbf{x} \\in \\mathbb{R}^n} &amp;\\qquad&amp; f(\\mathbf{x})=\\mathbf{c}^T\\mathbf{x},\\\\ &amp;\\text{subject to} &amp; &amp; \\mathbf{A}\\mathbf{x} \\leq \\mathbf{b},\\\\ &amp; &amp; &amp; \\mathbf{x} \\geq \\mathbf{0},\\\\ &amp; &amp; &amp; \\mathbf{c} \\in \\mathbb{R}^n, \\mathbf{b} \\in \\mathbb{R}^m, \\mathbf{A}\\in \\mathbb{R}^{m \\times n}. \\tag{1.5} \\end{align}\\] 1.7 Stochastic Optimization Problems Definition 1.8 The class of optimization problems, where the decision variables \\(x_i \\in \\mathbf{x}\\) depend on the outcomes of a random phenomenon besides consisting of random objective function and constraints are called stochastic optimization problems. Some examples of stochastic optimization methods are: simulated annealing, quantum annealing, genetic algorithms, etc. 1.8 Scaling of Decision Variables While formulating optimization problems, it must be guaranteed that the scale of the decision variables are approximately of the same order. If this is not taken care of, some optimization algorithms that are sensitive to scaling will perform poorly and will flounder to converge to the solution. Two of the fundamental fields that get disturbed due to poor scaling are computing the optimized step lengths and the numerical gradients. One of the widely accepted best practices is to make the decision variables dimensionless and vary them approximately between 0 and 1. One should always prefer optimization algorithms that are not sensitive to scaling. 1.9 Gradient Vector and Hessian Matrix of the Objective Function Definition 1.9 For a differentiable objective function \\(f(\\mathbf{x}): \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), its gradient vector given by \\(\\nabla f(\\mathbf{x}): \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\), is defined at the point \\(\\mathbf{x}\\) in the \\(n\\)-dimensional space as the vector of first order partial derivatives: \\[\\begin{equation} \\nabla f(\\mathbf{x})= \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1}(\\mathbf{x})\\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n}(\\mathbf{x}) \\end{pmatrix}\\tag{1.6} \\end{equation}\\] Now, if \\(f(\\mathbf{x})\\) is smooth, the gradient vector \\(\\nabla f(\\mathbf{x})\\) is always perpendicular to the contours at the point \\(\\mathbf{x}\\). The gradient vector is thus in the direction of the maximum increase of \\(f(\\mathbf{x})\\). Look at the figure below. Definition 1.10 For a twice continuously differentiable function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), its Hessian matrix given by \\(\\mathbf{H}(f(\\mathbf{x}))\\) is defined at the point \\(\\mathbf{x}\\) in the \\(n \\times n\\)-dimensional space as the matrix of second order partial derivatives: \\[\\begin{equation} \\mathbf{H} f(\\mathbf{x})=\\frac{\\partial ^2 f}{\\partial x_i \\partial x_j} = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2}(\\mathbf{x}) &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2}(\\mathbf{x}) &amp; \\ldots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n}(\\mathbf{x})\\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1}(\\mathbf{x}) &amp; \\frac{\\partial^2 f}{\\partial x_2^2}(\\mathbf{x}) &amp; \\ldots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n}(\\mathbf{x}) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1}(\\mathbf{x}) &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2}(\\mathbf{x}) &amp; \\ldots &amp; \\frac{\\partial^2 f}{\\partial x_n^2}(\\mathbf{x}) \\end{pmatrix}\\tag{1.7} \\end{equation}\\] One important relation that we will keep in mind is that the Hessian matrix is the Jacobian of the gradient vector of \\(f(\\mathbf{x})\\), where the Jacobian matrix of a vector-valued function \\(\\mathbf{F}(\\mathbf{x})\\) is the matrix of all its first order partial derivatives, given by, \\(\\mathbf{JF}(\\mathbf{x})= \\begin{pmatrix} \\frac{\\partial \\mathbf{F}}{\\partial x_1} &amp; \\ldots \\frac{\\partial \\mathbf{F}}{\\partial x_n} \\end{pmatrix}\\). The relation is as followed: \\[\\begin{equation} \\mathbf{H} f(\\mathbf{x}) = \\mathbf{J}(\\nabla f(\\mathbf{x})) \\tag{1.8} \\end{equation}\\] Let us consider an example now. Example 1.1 Let an objective function be \\(f(\\mathbf{x}) = 2x_1x_2^3+3x_2^2x_3 + x_3^3x_1\\). We will find out the gradient vector \\(\\nabla f(\\mathbf{x})\\) and the Hessian matrix \\(\\mathbf{H} f(\\mathbf{x})\\) at the point \\(\\mathbf{p} = \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\end{pmatrix}\\). The gradient vector is \\(\\nabla f(\\mathbf{x}) = \\begin{pmatrix} 2x_2^3+x_3^3 \\\\ 6x_1x_2^2+6x_2x_3 \\\\ 3x_2^2+3x_3^2x_1 \\end{pmatrix}\\). So \\(\\nabla f(\\mathbf{x})| \\mathbf{p} = \\begin{pmatrix} 43 \\\\ 60 \\\\ 39 \\end{pmatrix}\\). The Hessian matrix is therefore given by, \\(\\mathbf{H}f(\\mathbf{x}) = \\begin{pmatrix} 0 &amp; 6x_2^2 &amp; 3x_3^2 \\\\ 6x_2^2 &amp; 12x_1x_2+6x_3 &amp; 6x_2 \\\\ 3x_3^2 &amp; 6x_2 &amp; 6x_3x_1 \\end{pmatrix}\\) and at point \\(\\mathbf{p}\\), \\(\\mathbf{H} f(\\mathbf{x})|\\mathbf{p} = \\begin{pmatrix} 0 &amp; 24 &amp; 27 \\\\ 24 &amp; 42 &amp; 12 \\\\ 27 &amp; 12 &amp; 18 \\end{pmatrix}\\). We will try to work out the same example with Python scripting now. For that we need an extra package called autograd, besides the numpy package. The autograd package is used for automatically differentiating native Python and Numpy code. Fundamentally autograd is used in gradient-based optimization. First pip install the autograd package pip install autograd Now, after it is downloaded, we type the following in our notebook: import autograd.numpy as au from autograd import grad, jacobian p = np.array([1, 2, 3], dtype=float) def f(x): # Objective function return 2*x[0]*x[1]**3+3*x[1]**2*x[2]+x[2]**3*x[0] grad_f = grad(f) # gradient of the objective function hessian_f = jacobian(grad_f) # Hessian of the objective function print(&quot;gradient vector:&quot;,grad_f(p)) ## gradient vector: [43. 60. 39.] print(&quot;Hessian matrix:\\n&quot;,hessian_f(p)) ## Hessian matrix: ## [[ 0. 24. 27.] ## [24. 42. 12.] ## [27. 12. 18.]] 1.10 Directional Derivative of the Objective Function Definition 1.11 For a real valued objective function \\(f(\\mathbf{x})\\) and a feasible direction \\(\\mathbf{\\delta}\\), the directional derivative of \\(f(\\mathbf{x})\\) in the direction \\(\\mathbf{\\delta}\\) is given by: \\[\\begin{equation} \\frac{\\partial f}{\\partial \\mathbf{\\delta}}(\\mathbf{x}) = \\lim_{\\alpha \\to 0} \\frac{f(\\mathbf{x} + \\alpha \\mathbf{\\delta}) - f(\\mathbf{x})}{\\alpha} \\tag{1.9} \\end{equation}\\] where \\(\\|\\mathbf{\\delta}\\| = 1\\). Now for \\(\\mathbf{x} \\in \\mathbb{R}^n\\), let us consider the differential equation: \\[\\begin{equation} df(\\mathbf{x}) = \\frac{\\partial f(\\mathbf{x})}{\\partial x_1}dx_1 + \\ldots + \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}dx_n = \\nabla^Tf(\\mathbf{x})d\\mathbf{x} = \\langle \\nabla f(\\mathbf{x}), d\\mathbf{x} \\rangle \\tag{1.10} \\end{equation}\\] where \\(\\langle .,. \\rangle\\) denotes the dot product between two matrices and/or vectors. Now let us consider a function \\(\\hat{f}(\\mathbf{x}) = f(\\hat{\\mathbf{x}} + \\alpha \\mathbf{\\delta})\\), such that for a point \\(\\mathbf{x}\\) passing through the point \\(\\hat{\\mathbf{x}}\\) on the line through \\(\\hat{\\mathbf{x}}\\) in the direction \\(\\mathbf{\\delta}\\) is given by \\(\\mathbf{x}(\\alpha) = \\hat{\\mathbf{x}} + \\alpha \\mathbf{\\delta}\\). now, for an infinitesimal change \\(d\\alpha\\), we have \\(d\\mathbf{x}=\\mathbf{\\delta}d\\alpha\\). Thus, the differential at the point \\(\\mathbf{x}\\) in the given direction is \\(d\\hat{f}=\\nabla^Tf(\\mathbf{x})\\delta d\\alpha\\) So, the directional derivative now can be written as: \\[\\begin{equation} \\frac{\\partial f}{\\partial \\mathbf{\\delta}}(\\mathbf{x}) = \\frac{d}{d\\alpha}f(\\mathbf{x}+\\alpha\\mathbf{\\delta})|_{\\alpha=0} = \\nabla^Tf(\\mathbf{x})\\mathbf{\\delta} \\tag{1.11} \\end{equation}\\] Now,let us look into a simple example: Example 1.2 Let an objective function be \\(f(\\mathbf{x}) = 2x_1x_2^3+3x_2^2x_3 + x_3^3x_1\\). We will find out the gradient vector \\(\\nabla f(\\mathbf{x})\\) at the point \\(\\mathbf{p} = \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\end{pmatrix}\\) and then calculate the directional derivative in the direction \\(\\mathbf{\\delta}=\\begin{pmatrix} \\frac{1}{\\sqrt{35}} &amp; \\frac{3}{\\sqrt{35}} &amp; \\frac{5}{\\sqrt{35}} \\end{pmatrix}\\). We will use the same autograd package to calculate the same using Python. p = np.array([1, 2, 3], dtype=float) delta = np.array([1, 3, 5], dtype=float)/np.sqrt(35) def f(x): return 2*x[0]*x[1]**3+3*x[1]**2*x[2]+x[2]**3*x[0] grad_f = grad(f) print(&quot;directional derivative:&quot;, grad_f(p).dot(delta)) ## directional derivative: 70.65489569530399 We will see that the directional derivative is \\(\\approx 70.655\\). 1.11 Positive Definite and Positive Semi-definite Matrices Definition 1.12 A real matrix \\(\\mathbf{M}\\in \\mathbb{R}^{N\\times N}\\) is a positive definite matrix if for any real vector \\(\\mathbf{v} \\in \\mathbb{R}^n\\) other than the null vector, the following is satisfied: \\[\\begin{equation} \\mathbf{v}^T\\mathbf{M}\\mathbf{v} &gt; 0 \\tag{1.12} \\end{equation}\\] Definition 1.13 A real matrix \\(\\mathbf{M}\\in \\mathbb{R}^{N\\times N}\\) is a positive semi-definite matrix if for any real vector \\(\\mathbf{v} \\in \\mathbb{R}^n\\), the following is satisfied: \\[\\begin{equation} \\mathbf{v}^T\\mathbf{M}\\mathbf{v} \\geq 0 \\tag{1.13} \\end{equation}\\] Theorem 1.1 All the eigenvalues of a positive definite matrix are positive. Proof. If \\(\\lambda\\) be an eigenvalue (real) of \\(\\mathbf{M}\\) and \\(\\mathbf{v}\\) be the corresponding eigenvector, then we have the following well known equation: \\[\\begin{equation} \\mathbf{Mv}=\\lambda\\mathbf{v} \\tag{1.14} \\end{equation}\\] Now multiplying the equation with \\(\\mathbf{v}^T\\) on the left, we get the following: \\[\\begin{align} \\mathbf{v}^T\\mathbf{Mv}&amp;=\\lambda\\mathbf{v}^T\\mathbf{v}\\\\ &amp;=\\lambda \\|\\mathbf{v}\\|^2 \\tag{1.15} \\end{align}\\] Now the \\(L.H.S\\) is positive as \\(\\mathbf{M}\\) is positive definite and \\(\\|bm{v}\\|^2\\) is positive too. This implies that the eigenvalue \\(\\lambda\\) is positive. The above proof can be extended to positive semi-definite matrix too in which case the eigenvalues are non-negative, i.e, either 0 or positive and we will exploit these properties in our python script to check for positive definiteness or positive semi-definiteness of a given matrix. Example 1.3 We use a Python script to compute the eigenvalues and check whether the following matrices are positive definite, positive semi-definite or negative-definite: \\(\\begin{pmatrix}2 &amp; -1 &amp; 0 \\\\ -1 &amp; 2 &amp; -1\\\\ 0 &amp; -1 &amp; 2 \\end{pmatrix}\\) \\(\\begin{pmatrix} -2 &amp; 4\\\\ 4 &amp; -8 \\end{pmatrix}\\) \\(\\begin{pmatrix} -2 &amp; 2\\\\ 2 &amp; -4 \\end{pmatrix}\\) M = np.array(([2, -1, 0], [-1, 2, -1], [0, -1, 2]), dtype=float) #M = np.array(([-2, 4], [4, -8]), dtype=float) #M = np.array(([-2, 2], [2, -4]), dtype=float) eigs = np.linalg.eigvals(M) print(&quot;The eigenvalues of M:&quot;, eigs) ## The eigenvalues of M: [3.41421356 2. 0.58578644] if (np.all(eigs&gt;0)): print(&quot;M is positive definite&quot;) elif (np.all(eigs&gt;=0)): print(&quot;M is positive semi-definite&quot;) else: print(&quot;M is negative definite&quot;) ## M is positive definite Running the script for the first matrix tells us that it is positive definite. The Reader is asked to try out the code for the other two matrices. 1.12 What is Convexity? Definition 1.14 A set \\(\\mathbf{X} \\subset \\mathbb{R}^n\\) is said to be a convex set if \\(\\forall\\ \\mathbf{x}, \\mathbf{y} \\in \\mathbf{X}\\) and \\(\\alpha \\in [0, 1]\\), the following is satisfied: \\[\\begin{equation} (1-\\alpha)\\mathbf{x} + \\alpha \\mathbf{y} \\in \\mathbf{X} \\tag{1.16} \\end{equation}\\] If the above condition is not satisfied, the set is a non-convex set. Definition 1.15 A function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is called a convex function if for every two points \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) and \\(\\alpha \\in [0,1]\\), the following condition is satisfied: \\[\\begin{equation} f(\\alpha \\mathbf{x} + (1-\\alpha)\\mathbf{y}) \\leq \\alpha f(\\mathbf{x})+(1-\\alpha)f(\\mathbf{y}) \\tag{1.17} \\end{equation}\\] Definition 1.16 The function is strictly convex if \\(\\leq\\) symbol is replaced by \\(&lt;\\). In the similar way, one can define a concave function too. Definition 1.17 A constrained optimization problem is called a convex programming problem if the following properties are satisfied: the objective function \\(f(\\mathbf{x})\\) is convex, the equality constraint functions \\(h_k(\\mathbf{x})\\) are linear, and the inequality constraint functions \\(g_k(\\mathbf{x})\\) are concave This concept of convexity is used in practically solving many optimization problems in the real world. Now to test for convexity of \\(f(\\mathbf{x})\\) we study the following two theorems: Theorem 1.2 If \\(f(\\mathbf{x})\\) is a differentiable objective function defined over the convex set \\(\\mathbf{S} \\subseteq \\mathbb{R}^n\\), then \\(\\forall\\ \\mathbf{y}, \\mathbf{z} \\in \\mathbf{S}\\), \\(f(\\mathbf{x})\\) is convex over \\(\\mathbf{S}\\) if and only if \\[\\begin{equation} f(\\mathbf{y})+\\nabla^Tf(\\mathbf{y})(\\mathbf{z}-\\mathbf{y}) \\leq f(\\mathbf{z}) \\tag{1.18} \\end{equation}\\] Proof. \\(f(\\mathbf{x})\\) is convex over \\(\\mathbf{S}\\) implies that \\(\\forall\\ \\mathbf{y}, \\mathbf{z} \\in \\mathbf{S}\\) and \\(\\forall\\ \\alpha \\in [0,1]\\) the following equation is hold: \\[\\begin{equation} f(\\alpha \\mathbf{z} + (1-\\alpha)\\mathbf{y}) \\leq \\alpha f(\\mathbf{y}) + (1-\\alpha)f(\\mathbf{y}) \\tag{1.19} \\end{equation}\\] implying, \\[\\begin{equation} \\frac{f(\\mathbf{y} + \\alpha(\\mathbf{z}-\\mathbf{y})) - f(\\mathbf{y})}{\\alpha} \\leq f(\\mathbf{z}) - f(\\mathbf{y}) \\tag{1.20} \\end{equation}\\] Now, the difference inequality can be turned into a differential inequality by taking \\(\\lim_{\\alpha \\to 0}\\): \\[\\begin{equation} \\frac{df(\\mathbf{y})}{d\\alpha}\\mid_{\\mathbf{y}-\\mathbf{z}} \\leq f(\\mathbf{z}) - f(\\mathbf{y}) \\tag{1.21} \\end{equation}\\] The \\(L.H.S\\) is the directional derivative and thus from (1.11) this can be written as: \\[\\begin{equation} \\frac{df(\\mathbf{y})}{d\\alpha}\\mid_{\\mathbf{y}-\\mathbf{z}} = \\nabla^Tf(\\mathbf{x})(\\mathbf{z}-\\mathbf{y}) \\tag{1.22} \\end{equation}\\] Now from (1.17) and (1.18), the following inequality can be written: \\(f(\\mathbf{y}) + \\nabla^Tf(\\mathbf{y})(\\mathbf{z}-\\mathbf{y}) \\leq f(\\mathbf{z})\\). Now, to work on the other way round, if (1.14) is true, then for \\(\\mathbf{x}=\\alpha\\mathbf{z}+(1-\\alpha)\\mathbf{y} \\in \\mathbf{S}\\) and \\(\\alpha \\in [0,1]\\), we will have the following inequalities: \\[\\begin{equation} f(\\mathbf{x})+\\nabla^Tf(\\mathbf{x})(\\mathbf{z}-\\mathbf{x}) \\leq f(\\mathbf{z}) \\tag{1.23} \\end{equation}\\] and \\[\\begin{equation} f(\\mathbf{x})+\\nabla^Tf(\\mathbf{x})(\\mathbf{y}-\\mathbf{x}) \\leq f(\\mathbf{y}) \\tag{1.24} \\end{equation}\\] Now ((1.23) \\(\\times \\alpha) +\\) (1.24)\\(\\times (1-\\alpha)) \\implies\\) \\[\\begin{equation} \\alpha f(\\mathbf{z}) + (1-\\alpha)f(\\mathbf{x}) \\leq \\alpha f(\\mathbf{z}) + (1-\\alpha)f(\\mathbf{y}) - f(\\mathbf{x}) \\tag{1.25} \\end{equation}\\] now \\(L.H.S = 0\\) since \\(\\alpha(\\mathbf{z}-\\mathbf{x})+(1-\\alpha)(\\mathbf{y}-\\mathbf{x})=0\\). So we get, \\[f(\\mathbf{x})=f(\\alpha\\mathbf{z}+(1-\\alpha)\\mathbf{y}) \\leq \\alpha f(\\mathbf{z}) + (1-\\alpha)f(\\mathbf{y}).\\] The converse relation is also satisfied, which completes our proof. Theorem 1.3 If \\(f(\\mathbf{x})\\) is defined over an open convex set \\(\\mathbf{S} \\subseteq \\mathbb{R}^n\\) and if the Hessian matrix \\(\\mathbf{H}f(\\mathbf{x})\\) is positive definite \\(\\forall\\ \\mathbf{x} \\in \\mathbf{S}\\), then \\(f(\\mathbf{x})\\) is strictly convex over the set \\(\\mathbf{S}\\). Proof. Let \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbf{S}\\). Using Taylor expansion, the function \\(f(\\mathbf{y})\\) can be written as, \\[\\begin{align} f(\\mathbf{y})&amp;=f(\\mathbf{x} + (\\mathbf{y} - \\mathbf{x}))\\\\ &amp;=f(\\mathbf{x}) + \\nabla^Tf(\\mathbf{x})(\\mathbf{y}-\\mathbf{x}) + \\frac{1}{2}(\\mathbf{y}-\\mathbf{x})^T\\mathbf{H}f(\\mathbf{x} + \\alpha(\\mathbf{y} - \\mathbf{x}))(\\mathbf{y} - \\mathbf{x}) \\tag{1.26} \\end{align}\\] where \\(\\alpha \\in [0,1]\\). Now positive definite Hessian matrix implies that \\(f(\\mathbf{y}) &gt; f(\\mathbf{x})+\\nabla^Tf(\\mathbf{x})(\\mathbf{y}-\\mathbf{x})\\). Now from Theorem 1.2 it can be said that \\(f(\\mathbf{x})\\) is strictly convex, thus proving the theorem. 1.13 Numerical Optimization Algorithms Optimization Algorithms are iterative techniques that follow the following fundamental steps: Initialize with a guess of the decision variables \\(\\mathbf{x}\\), Iterate through the process of generating a list of improving estimates, check whether the terminating conditions are met, and the estimates will be probably stop at the solution point \\(\\mathbf{x}^*\\). The book by Nocedal and Wright [Nocedal, Jorge, and Stephen Wright. Numerical optimization. Springer Science &amp; Business Media, 2006.] states that most of the optimization strategies make use of either the objective function \\(f(\\mathbf{x})\\), the constraint functions \\(g(\\mathbf{x})\\) and \\(h(\\mathbf{x})\\), the first or second derivatives of these said functions, information collected during previous iterations and/or local information gathered at the present point. As Nocedal and Wright mentions, a good optimization algorithm should have the following fundamental properties: Robustness: For all acceptable initial points chosen, the algorithm should operate well on a broad range of problems, in their particular class. Efficiency: The time complexity and the space complexity of the algorithm should be practicable Accuracy: The solution should be as precise as possible, with the caveat that it should not be too much delicate to errors in the data or to numerical rounding and/or truncating errors while it is being executed on a machine. There might be some trade offs allowed between speed and memory, between speed and robustness, etc. "],["introduction-to-unconstrained-optimization.html", "Chapter 2 Introduction to Unconstrained Optimization 2.1 The Unconstrained Optimization Problem 2.2 Smooth Functions 2.3 Taylors Theorem 2.4 Necessary and Sufficient Conditions for Local Minimizer in Unconstrained Optimization 2.5 Algorithms for Solving Unconstrained Minimization Tasks", " Chapter 2 Introduction to Unconstrained Optimization This chapter introduces what exactly an unconstrained optimization problem is. A detailed discussion of Taylors Theorem is provided and has been use to study the first order and second order necessary and sufficient conditions for local minimizer in an unconstrained optimization tasks. Examples have been supplied too in view of understanding the necessary and sufficient conditions better. The Python package scipy.optimize, which will form an integral part in solving many optimization problems in the later chapters of this book, is introduced too. The chapter ends with an overview of how an algorithm to solve unconstrained minimization problem works, covering briefly two procedures: line search descent method and trust region method. 2.1 The Unconstrained Optimization Problem As we have discussed in the first chapter, an unconstrained optimization problem deals with finding the local minimizer \\(\\mathbf{x}^*\\) of a real valued and smooth objective function \\(f(\\mathbf{x})\\) of \\(n\\) variables, given by \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), formulated as, \\[\\begin{equation} \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{\\min f(\\mathbf{x})} \\tag{1.1} \\end{equation}\\] with no restrictions on the decision variables \\(\\mathbf{x}\\). We work towards computing \\(\\mathbf{x}^*\\), such that \\(\\forall\\ \\mathbf{x}\\) near \\(\\mathbf{x}^*\\), the following inequality is satisfied: \\[\\begin{equation} f(\\mathbf{x}^*) \\leq f(\\mathbf{x}) \\tag{1.2} \\end{equation}\\] 2.2 Smooth Functions In terms of analysis, the measure of the number of continuous derivative a function has, characterizes the smoothness of a function. Definition 2.1 A function \\(f\\) is smooth if it can be differentiated everywhere, i.e, the function has continuous derivatives up to some desired order over particular domain [Weisstein, Eric W. Smooth Function. https://mathworld.wolfram.com/SmoothFunction.html]. Some examples of smooth functions are , \\(f(x) = x\\), \\(f(x)=e^x\\), \\(f(x)=\\sin(x)\\), etc. To study the local minima \\(\\mathbf{x}^*\\) of a smooth objective function \\(f(\\mathbf{x})\\), we emphasize on Taylors theorem for a multivariate function, thus focusing on the computations of the gradient vector \\(\\nabla f(\\mathbf{x})\\) and the Hessian matrix \\(\\mathbf{H} f(\\mathbf{x})\\). 2.3 Taylors Theorem Theorem 2.1 For a smooth function of a single variable given by \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\), \\(m(\\geq 1)\\) times differentiable at the point \\(p \\in \\mathbb{R}\\), there exists a function \\(j_m: \\mathbb{R} \\rightarrow \\mathbb{R}\\), such that the following equations are satisfied: \\[\\begin{align} f(x) &amp;= f(p) + (x - p)f^{&#39;}(p) + \\frac{(x-p)^2}{2!}f^{&#39;&#39;}(p) + \\ldots \\\\ &amp;+ \\frac{(x-p)^m}{m!}f^m(p) + (x-p)^m j_m(x) \\tag{1.3} \\end{align}\\] and \\[\\begin{equation} \\lim_{x \\to p}j_m(x)=0 \\tag{1.4} \\end{equation}\\] The \\(m-\\)th order Taylor polynomial of the function \\(f\\) around the point \\(p\\) is given by: \\[\\begin{align} P_m(x)&amp;=f(p) + (x - p)f^{&#39;}(p) + \\frac{(x-p)^2}{2!}f^{&#39;&#39;}(p) + \\ldots \\\\ &amp;+ \\frac{(x-p)^m}{m!}f^m(p) \\tag{1.5} \\end{align}\\] Now let \\(f\\) be a smooth, continuously differentiable function that takes in multiple variables, i.e, \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) and \\(\\mathbf{x}, \\mathbf{p}, \\mathbf{\\delta} \\in \\mathbb{R}^n\\), where \\(\\mathbf{\\delta}\\) is the direction in which the line \\(\\mathbf{x} = \\mathbf{p}+\\alpha \\mathbf{\\delta}\\) passes through the point \\(\\mathbf{p}\\) [Snyman, Jan A. Practical mathematical optimization. Springer Science+ Business Media, Incorporated, 2005.]. Here, \\(\\alpha \\in [0,1]\\). We have, \\[\\begin{equation} f(\\mathbf{x}) = f(\\mathbf{p} + \\alpha \\mathbf{\\delta}) \\tag{1.6} \\end{equation}\\] From the definition of the directional derivative, we get, \\[\\begin{equation} \\frac{df(\\mathbf{x})}{d\\alpha}|\\mathbf{\\delta} = \\nabla^T f(\\mathbf{x})\\mathbf{\\delta}=\\hat{f}(\\mathbf{x}) \\tag{1.7} \\end{equation}\\] Again, differentiating \\(\\hat{f}(\\mathbf{x})\\) with respect to \\(\\alpha\\). \\[\\begin{equation} \\frac{d \\hat{f}(\\mathbf{x})}{d \\alpha}|\\mathbf{\\delta} = \\frac{d^2 f(\\mathbf{x})}{d \\alpha^2}=\\nabla^T\\hat{f}(\\mathbf{x})\\mathbf{\\delta}=\\mathbf{\\delta}^T\\mathbf{H}f(\\mathbf{x})\\mathbf{\\delta} \\tag{1.8} \\end{equation}\\] So, using equations (1.5) and (1.6) we can generate the Taylor expansion for a multivariable function at a point \\(\\mathbf{p}\\). So, around \\(\\alpha = 0\\), we get, \\[\\begin{equation} f(\\mathbf{x}) = f(\\mathbf{p}+\\alpha \\mathbf{\\delta}) = f(\\mathbf{p}) + \\nabla^Tf(\\mathbf{p})\\alpha \\mathbf{\\delta} + \\frac{1}{2}\\alpha \\mathbf{\\delta}^T\\mathbf{H} f(\\mathbf{p})\\alpha \\mathbf{\\delta} + \\ldots \\tag{1.9} \\end{equation}\\] The truncated Taylor expansion of the multivariable function, where the higher order terms are ignored, can be written as, \\[\\begin{equation} f(\\mathbf{x}) = f(\\mathbf{p}+\\alpha \\mathbf{\\delta}) = f(\\mathbf{p}) + \\nabla^Tf(\\mathbf{p})\\alpha \\mathbf{\\delta} + \\frac{1}{2}\\alpha \\mathbf{\\delta}^T\\mathbf{H} f(\\mathbf{p}+\\beta\\mathbf{\\delta})\\alpha \\mathbf{\\delta} \\tag{1.10} \\end{equation}\\] where, \\(\\beta \\in [0,1]\\). 2.4 Necessary and Sufficient Conditions for Local Minimizer in Unconstrained Optimization 2.4.1 First-Order Necessary Condition If there exists a local minimizer \\(\\mathbf{x}^*\\) for a real-valued smooth function \\(f(\\mathbf{x}): \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), in an open neighborhood \\(\\subset \\mathbb{R}^n\\) of \\(\\mathbf{x}^*\\) along the direction \\(\\mathbf{\\delta}\\), then the first order necessary condition for the minimizer is given by: \\[\\begin{equation} \\nabla^Tf(\\mathbf{x}^*)\\mathbf{\\delta}=0\\ \\forall\\ \\mathbf{\\delta} \\neq 0 \\tag{1.11} \\end{equation}\\] i.e, the directional derivative is \\(0\\), which ultimately reduces to the equation: \\[\\begin{equation} \\nabla f(\\mathbf{x}^*)=0 \\tag{1.12} \\end{equation}\\] Proof. Let a real-valued smooth function \\(f(\\mathbf{x})\\) be differentiable at the point \\(\\mathbf{x}^* \\in \\mathbb{R}^n\\). Using the Taylor expansion, we can write: \\[\\begin{equation} f(\\mathbf{x})=f(\\mathbf{x}^*) + \\nabla^T f(\\mathbf{x}^*) (\\mathbf{x} - \\mathbf{x}^*)+\\sum_{|\\gamma|\\leq m}\\frac{\\mathfrak{D}^{\\gamma}f(\\mathbf{x}^*)}{\\gamma!}(\\mathbf{x}-\\mathbf{x}^*)^{\\gamma} + \\sum_{|\\gamma|=m}j_{\\gamma}(\\mathbf{x})(\\mathbf{x} - \\mathbf{x}^*)^{\\gamma} \\tag{1.13} \\end{equation}\\] where \\(\\mathfrak{D}\\) represents the differential and \\(m\\) is the smoothness of the objective function \\(f\\). Also \\(\\lim_{\\mathbf{x} \\to \\mathbf{x}^*}j_{\\gamma(\\mathbf{X})}=0\\). Clubbing together the higher order terms, we can write Eq.(1.13) as, \\[\\begin{equation} \\label{eq:2.14} f(\\mathbf{x})=f(\\mathbf{x}^*) + \\nabla^T f(\\mathbf{x}^*) (\\mathbf{x} - \\mathbf{x}^*)+ \\mathcal{O}(\\|\\mathbf{x} - \\mathbf{x}^*\\|) \\tag{1.14} \\end{equation}\\] In this case, \\[\\begin{equation} \\lim_{\\mathbf{x} \\to \\mathbf{x}^*}\\frac{\\mathcal{O}(\\|\\mathbf{x} - \\mathbf{x}^*\\|)}{\\|\\mathbf{x} - \\mathbf{x}^*\\|}=0 \\tag{1.15} \\end{equation}\\] Let us consider, \\(\\mathbf{x} = \\mathbf{x}^*-\\beta \\nabla f(\\mathbf{x}^*)\\), where \\(\\beta \\in [0,1]\\). From Eq.(1.14) we can write, \\[\\begin{equation} f(\\mathbf{x}^*-\\beta \\nabla f(\\mathbf{x}^*))=f(\\mathbf{x}^*)-\\beta\\|\\nabla f(\\mathbf{x}^*)\\|^2+\\mathcal{O}(\\beta\\|\\nabla f(\\mathbf{x}^*)\\|) \\tag{1.16} \\end{equation}\\] Now, dividing Eq.(1.16)-\\(f(\\mathbf{x}^*)\\) by \\(\\beta\\), we get, \\[\\begin{equation} \\frac{f(\\mathbf{x}^*-\\beta \\nabla f(\\mathbf{x}^*))-f(\\mathbf{x}^*)}{\\beta} = -\\|\\nabla f(\\mathbf{x}^*)\\|^2 + \\frac{\\mathcal{O}(\\beta \\|\\nabla f(\\mathbf{x}^*)\\|)}{\\beta} \\geq 0 \\tag{1.17} \\end{equation}\\] Now, considering the limit \\(\\beta \\to 0^{+}\\), we get, \\[\\begin{equation} -\\|\\nabla f(\\mathbf{x}^*)\\|^2 \\leq 0 \\tag{1.18} \\end{equation}\\] Combining which along with Eq.(1.18), we get, \\[\\begin{equation} 0 \\leq -\\|\\nabla f(\\mathbf{x}^*)\\|^2 \\leq 0\\tag{1.19} \\end{equation}\\] This ultimately gives \\(\\nabla f(\\mathbf{x}^*)=0\\), proving the first-order necessary condition. Example 2.1 The Rosenbrock function of \\(n\\)-variables is given by: \\[\\begin{equation} f(\\mathbf{x}) = \\sum_{i=1}^{n-1}(100(x_{i+1}-x_i^2)^2 + (1-x_i)^2) \\tag{1.20} \\end{equation}\\] where, \\(\\mathbf{x} \\in \\mathbb{R}^n\\). For this example let us consider the Rosenbrock function for two variables, given by: \\[\\begin{equation} f(\\mathbf{x}) = 100(x_2-x_1^2)^2+(1-x_1)^2 \\tag{1.21} \\end{equation}\\] We will show that the first order necessary condition is satisfied for the local minimizer \\(\\mathbf{x^*}=\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\). We first check whether \\(\\mathbf{x}^*\\) is a minimizer or not. Putting \\(x_1=x_2=1\\) in \\(f(\\mathbf{x})\\), we get \\(f(\\mathbf{x})=0\\). Now, we check whether the \\(\\mathbf{x^*}\\) satisfies the first order necessary condition. For that we calculate \\(\\nabla f(\\mathbf{x}^*)\\). \\[\\begin{equation} \\nabla f(\\mathbf{x}^*) = \\begin{bmatrix} -400x_1(x_2-x_1)^2-2(1-x_1) \\\\ 200(x_2-x_1^2)\\end{bmatrix}_{\\mathbf{x}^*} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\tag{1.22} \\end{equation}\\] So, we see that the first order necessary condition is satisfied. We can do similar analysis using the scipy.optimize package in Python. The Scipy official reference states that the scipy.optimize package provides the user with many commonly used optimization algorithms and test functions. It packages the following functionalities and aspects: Minimization of multivariate scalar objective functions covering both the unconstrained and constrained domains, using a range of optimization algorithms, Algorithms for minimization of scalar univariate functions, A variety of brute-force optimization algorithms, also called global optimization algorithms, Algorithms like minimization of least-squares and curve-fitting, Root finding algorithms, and Algorithms for solving multivariate equation systems. We will build upon the basic concepts of optimization using this package and cover most of the concepts one by one as we advance in the book. Note that, this is not the only package that we are going to use throughout the book. As we advance, we will be required to look into other Python resources according to our needs. But to keep in mind, scipy.optimize is the most important package that we will be using. Now, going back to the example, scipy.optimize already provides with the test function, its gradient and its Hessian. We will import them first and check whether the given point \\(\\mathbf{x}^*\\) is a minimizer or not: import numpy as np import scipy # Import the Rosenbrock function, its gradient and Hessian respectively from scipy.optimize import rosen, rosen_der, rosen_hess x_m = np.array([1, 1]) #given local minimizer rosen(x_m) # check whether x_m is a minimizer ## 0.0 the result is \\(0.0\\). So \\(\\mathbf{x}^*\\) is a minimizer. We then check for the first order necessary condition, using the gradient: rosen_der(x_m) # calculates the gradient at the point x_m ## array([0, 0]) This matches with our calculations and also satisfies the first-order necessary condition. 2.4.2 Second-Order Necessary Conditions If there exists a local minimizer \\(\\mathbf{x}^*\\) for a real-valued smooth function \\(f(\\mathbf{x}): \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), in an open neighborhood \\(\\subset \\mathbb{R}^n\\) of \\(\\mathbf{x}^*\\) along the feasible direction \\(\\mathbf{\\delta}\\), and \\(\\mathbf{H} f(\\mathbf{x})\\) exists and is continuous in the open neighborhood, then the second order necessary conditions for the minimizer are given by: \\[\\begin{equation} \\nabla^T f(\\mathbf{x}^*)\\mathbf{\\delta} = 0, \\forall\\ \\mathbf{\\delta} \\neq 0 \\tag{1.23} \\end{equation}\\] and \\[\\begin{equation} \\mathbf{\\delta}^T\\mathbf{H}f(\\mathbf{x}^*)\\mathbf{\\delta} \\geq 0, \\forall\\ \\mathbf{\\delta} \\neq 0 \\tag{1.24} \\end{equation}\\] which reduces to the following: \\[\\begin{equation} \\nabla f(\\mathbf{x}^*) = 0 \\tag{1.25} \\end{equation}\\] and \\[\\begin{equation} \\mathbf{\\delta}\\mathbf{H}f(\\mathbf{x}^*) \\geq 0 \\tag{1.26} \\end{equation}\\] where equation Eq.(1.26) means that the Hessian matrix should be positive semi-definite. Proof. From the proof of first order necessary condition, we know that \\(\\nabla f(\\mathbf{x}^*) = 0\\), which satisfies equation Eq.(1.25). Now for proving equation Eq.(1.26), we use the Taylor series expansion again. We have, \\[\\begin{equation} f(\\mathbf{x}) = f(\\mathbf{x}^*)+\\nabla^T f(\\mathbf{x}^*)(\\mathbf{x} - \\mathbf{x}^*) + \\frac{1}{2}\\mathbf{H} f(\\mathbf{x}^*)(\\mathbf{x} - \\mathbf{x}^*) + \\mathcal{O}(\\|\\mathbf{x} - \\mathbf{x}^*\\|^2) \\tag{2.1} \\end{equation}\\] Given, the feasible direction \\(\\mathbf{\\delta}\\), we take \\(\\beta \\in [0,1]\\), such that \\(\\mathbf{x} = \\mathbf{x}^* + \\beta \\mathbf{\\delta}\\). Putting this in Eq.(2.1) and rearranging, we get, \\[\\begin{equation} f(\\mathbf{x}^* + \\beta \\mathbf{\\delta}) - f(\\mathbf{x}^*) = \\beta \\nabla^T f(\\mathbf{x}^*)\\mathbf{\\delta} + \\frac{\\beta^2}{2}\\mathbf{\\delta}^T \\mathbf{H} f(\\mathbf{x}^*)\\mathbf{\\delta} + \\mathcal{O}(\\beta^2) \\tag{2.2} \\end{equation}\\] As, \\(\\nabla f(\\mathbf{x}^*)=0\\), we have \\[\\begin{equation} f(\\mathbf{x}^* + \\beta \\mathbf{\\delta}) - f(\\mathbf{x}^*) = \\frac{\\beta^2}{2}\\mathbf{\\delta}^T \\mathbf{H} f(\\mathbf{x}^*)\\mathbf{\\delta} + \\mathcal{O}(\\beta^2) \\tag{2.3} \\end{equation}\\] Now, dividing Eq.(2.3) by \\(\\beta^2\\), we have, \\[\\begin{equation} \\frac{f(\\mathbf{x}^* + \\beta \\mathbf{\\delta}) - f(\\mathbf{x}^*)}{\\beta^2} = \\frac{1}{2}\\mathbf{\\delta}^T \\mathbf{H} f(\\mathbf{x}^*)\\mathbf{\\delta} + \\frac{\\mathcal{O}(\\beta^2)}{\\beta^2} \\geq 0 \\tag{2.4} \\end{equation}\\] Taking the limit \\(\\beta \\to 0\\), we have, \\[\\begin{equation} \\mathbf{\\delta}^T \\mathbf{H} f(\\mathbf{x}^*) \\mathbf{\\delta} \\geq 0 \\tag{2.5} \\end{equation}\\] Which ultimately reduces to, \\[\\begin{equation} \\mathbf{H} f(\\mathbf{x}^*) \\geq 0 \\tag{2.6} \\end{equation}\\] We see that the Hessian matrix is positive semi-definite. This completes the proof of the second order necessary conditions. [refer to chapter 1, optimality conditions] 2.4.3 Second-Order Sufficient Conditions For a real-valued smooth objective function \\(f(\\mathbf{x})\\), if \\(\\mathbf{x}^*\\) is its local minimizer and \\(\\mathbf{H} f(\\mathbf{x})\\) exists and is continuous in an open neighborhood \\(\\subset \\mathbb{R}^n\\) of \\(\\mathbf{x}^*\\) along the feasible direction \\(\\mathbf{\\delta}\\), then the conditions: \\[\\begin{equation} \\nabla^T f(\\mathbf{x}^*)\\delta = 0 \\tag{2.7} \\end{equation}\\] i.e, \\[\\begin{equation} \\nabla f(\\mathbf{x}^*) = 0 \\tag{2.8} \\end{equation}\\] and \\[\\begin{equation} \\mathbf{\\delta}^T \\mathbf{H} f(\\mathbf{x}^*) \\mathbf{\\delta} &gt; 0 \\tag{2.9} \\end{equation}\\] i.e, a positive definite Hessian matrix imply that \\(\\mathbf{x}^*\\) is a strong local minimizer of \\(f(\\mathbf{x})\\). Proof. Let \\(\\mathfrak{r} &gt;0\\) be a radius such that the Hessian of the objective function, \\(\\mathbf{H} f(\\mathbf{x})\\) is positive definite \\(\\forall\\ \\mathbf{x}\\) in the open ball defined by \\(\\mathfrak{B} = \\{\\mathbf{y} \\mid \\|\\mathbf{y} - \\mathbf{x}^*\\| \\leq \\mathfrak{r}\\}\\). This comes from the fact that \\(\\mathbf{H} f(\\mathbf{x})\\) is positive definite at the local minimizer \\(\\mathbf{x}^*\\). Now, considering a nonzero vector \\(\\mathbf{\\eta}\\), such that \\(\\|\\mathbf{\\eta}\\| &lt; \\mathfrak{r}\\) and \\(c \\in [0,1]\\), we will have, \\(\\mathbf{x}=\\mathbf{x}^*+c\\mathbf{\\eta} \\in \\mathfrak{B}\\). Now, using the Taylors expansion, i.e, Eq.(2.1), we will have, \\[\\begin{equation} f(\\mathbf{x}^* + \\mathbf{\\eta}) = f(\\mathbf{x}^*) + c\\nabla^Tf(\\mathbf{x}^*)\\mathbf{\\eta} + \\frac{c^2}{2}\\mathbf{\\eta}^T\\mathbf{H} f(\\mathbf{y})\\mathbf{\\eta} + \\mathcal{O}(c^2) \\tag{2.10} \\end{equation}\\] Since, \\(\\nabla f(\\mathbf{x}^*) = 0\\), we have, \\[\\begin{equation} \\frac{f(\\mathbf{x}^* + \\mathbf{\\eta}) - f(\\mathbf{x}^*)}{c^2} = \\frac{1}{2}\\mathbf{\\eta}^T\\mathbf{H}f(\\mathbf{y})\\mathbf{\\eta}+\\frac{\\mathcal{O}(c^2)}{c^2} \\tag{2.11} \\end{equation}\\] Here, \\(\\mathbf{y} = \\mathbf{x}^* + \\beta \\mathbf{\\eta}\\) with \\(\\beta \\in [0,1]\\). As, \\(\\mathbf{y} \\in \\mathfrak{B}\\), we have, \\[\\begin{equation} \\mathbf{\\eta}^T\\mathbf{H}f(\\mathbf{y})\\mathbf{\\eta} &gt; 0 \\tag{2.12} \\end{equation}\\] So, taking the limit, \\(c \\to 0\\), we have, \\[\\begin{equation} f(\\mathbf{x}^* + \\mathbf{\\eta}) - f(\\mathbf{x}^*) &gt; 0 \\tag{2.13} \\end{equation}\\] this proves our claim that, \\[\\begin{equation} f(\\mathbf{x}^* + \\mathbf{\\eta}) &gt; f(\\mathbf{x}^*) \\tag{2.14} \\end{equation}\\] that is, \\(\\mathbf{x}^*\\) is a strict local minimizer of the objective function \\(f(\\mathbf{x})\\). Example 2.2 Let us now work with a new test function called Himmelblaus function, given by, \\[\\begin{equation} f(\\mathbf{x}) = (x_1^2+x_2-11)^2+(x_1+x_2^2-7)^2 \\tag{2.15} \\end{equation}\\] where, \\(\\mathbf{x} \\in \\mathbb{R}^2\\). We will check whether \\(\\mathbf{x}^*=\\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}\\) satisfies the second-order sufficient conditions satisfying the fact that it is a strong local minimizer. We will again use the autograd package to do the analyses for this objective function. Let us first define the function and the local minimizer as x_star in Python: def Himm(x): return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2 x_star = np.array([3, 2], dtype=&#39;float&#39;) #local minimizer We then check whether x_star is a minimizer. print(&quot;function at x_star:&quot;, Himm(x_star)) ## function at x_star: 0.0 Now, we calculate the gradient vector and the Hessian matrix of the function at x_star and look at the results, # import the necessary packages import autograd.numpy as au from autograd import grad, jacobian # gradient vector of the Himmelblau&#39;s function Himm_grad=grad(Himm) print(&quot;gradient vector at x_star:&quot;, Himm_grad(x_star)) # Hessian matrix of the Himmelblau&#39;s function ## gradient vector at x_star: [0. 0.] Himm_hess = jacobian(Himm_grad) M = Himm_hess(x_star) eigs = np.linalg.eigvals(M) print(&quot;The eigenvalues of M:&quot;, eigs) ## The eigenvalues of M: [82.28427125 25.71572875] if (np.all(eigs&gt;0)): print(&quot;M is positive definite&quot;) elif (np.all(eigs&gt;=0)): print(&quot;M is positive semi-definite&quot;) else: print(&quot;M is negative definite&quot;) ## M is positive definite We see that x1 satisfies the second order sufficient conditions and is a strong local minimizer. We wanted to perform the analyses using autograd package instead of scipy.optimize, because there might be cases when we need to use test functions that are not predefined in scipy.optimize package, unlike the Rosenbrock function. 2.5 Algorithms for Solving Unconstrained Minimization Tasks An optimization algorithm for solving an unconstrained minimization problem requires an initial point \\(\\mathbf{x}_0\\) to start with. The choice of \\(\\mathbf{x}_0\\) depends either on the applicant who has some idea about the data and the task at hand or it can be set by the algorithm in charge. Starting with \\(\\mathbf{x}_0\\), the optimization algorithm iterates through a sequence of successive points \\(\\{\\mathbf{x}_0, \\mathbf{x}_1, \\ldots, \\mathbf{x}_{\\infty}\\}\\), which stops at the point where all the termination conditions are met for approximating the minimizer \\(\\mathbf{x}^*\\). The algorithm generates this sequence taking into consideration the objective function \\(f(\\mathbf{x})\\) at a particular point \\(f(\\mathbf{x}_n)\\). A new iterate \\(\\mathbf{x}_{n+1}\\) is added in the sequence if the condition \\(f(\\mathbf{x}_{n+1}) &lt; f(\\mathbf{x}_n)\\). Although in many special cases, the algorithm might fail to find a new point in each and every step following the above condition, it must satisfy that after some stipulated number \\(k\\) of steps, the following condition is met: \\[f(\\mathbf{x}_{n+k}) &lt; f(\\mathbf{x}_n)\\]. One of the important terminating conditions, for example, is to check whether the first order necessary condition is sufficiently accurate, for a smooth objective function, i.e, \\(\\|\\nabla f(\\mathbf{x}_{\\infty})\\| &lt; \\epsilon\\), where \\(\\epsilon\\) is the infinitesimal tolerance value. We will discuss these conditions further in the subsequent chapters. Fundamentally, there are two approaches available to generate \\(f(\\mathbf{x}_{n+1})\\) from \\(f(\\mathbf{x}_n)\\): Line Search Descent Method: Using this method, the optimization algorithm first picks a direction \\(\\mathbf{\\delta}_n\\) for the \\(n^{th}\\) step and performs a search along this direction from the previous generated iterate \\(\\mathbf{x}_{n-1}\\) to find a new iterate \\(\\mathbf{x}_n\\) such that the condition \\(f(\\mathbf{x}_n) &lt; f(\\mathbf{x}_{n-1})\\) is satisfied. A direction \\(\\mathbf{\\delta}_n\\) is selected for the next iterate if the following condition is satisfied: \\[\\begin{equation} \\nabla^T f(\\mathbf{x}_{n-1})\\mathbf{\\delta}_n &lt; 0 \\tag{2.16} \\end{equation}\\] i.e, if the directional derivative in the direction \\(\\mathbf{\\delta}_n\\) is negative. Here \\(f\\) is the objective function. In view of that, the algorithm then needs to ascertain a distance by which it has to move along the direction \\(\\mathbf{\\delta}_n\\) to figure out \\(\\mathbf{x}_n\\). The distance \\(\\beta &gt;0\\), which is called the step length, can be figured out by solving the one-dimensional minimization problem formulated as: \\[\\begin{equation} \\underset{\\beta &gt; 0}{\\min} \\tilde{f}(\\beta) = \\underset{\\beta &gt; 0}{\\min} f(\\mathbf{x}_{n-1} + \\beta \\mathbf{\\delta}_n) \\tag{2.17} \\end{equation}\\] Line Search Descent Method Trust Region Method: Using this method, the optimization algorithm develops a model function [refer to Nocedal &amp; Wright], \\(M_n\\), such that its behavior inside a boundary set around the current iterate \\(\\mathbf{x}_n\\) matches that of the objective function \\(f(\\mathbf{x}_n)\\) at that point. The model function is not expected to give a reasonable approximation to the behavior of the objective function at a point \\(\\mathbf{x}_t\\) which is far away from \\(\\mathbf{x}_n\\), i.e, not lying inside the boundary defined around \\(\\mathbf{x}_n\\). As a result, the algorithm obstructs the search for the minimizer of \\(M_n\\) inside the boundary region, which is actually called the trust region, denoted by \\(\\mathcal{T}\\), before finding the step \\(\\mathbf{\\zeta}\\), by solving the minimization problem formulated by: \\[\\begin{equation} \\underset{\\mathbf{\\zeta}}{\\min\\ } M_n(\\mathbf{x}_n+\\mathbf{\\zeta}),\\text{ where }\\mathbf{x}_n+\\mathbf{\\zeta}\\in \\mathcal{T} \\tag{2.18} \\end{equation}\\] Using this \\(\\mathbf{\\zeta}\\), if the decrease in the value of \\(f(\\mathbf{x}_{n+1})\\) from \\(f(\\mathbf{x}_n)\\) is not sufficient, it can be inferred that the selected trust region is unnecessarily large. The algorithm then reduces the size of \\(\\mathcal{T}\\) accordingly and re-solves the problem given by equation Eq.(2.18). Most often, the trust region \\(\\mathcal{T}\\) is defined by a circle in case of a two dimensional problem or a sphere in case of a three dimensional problem of radius \\(\\mathcal{T_r}&gt;0\\), which follows the condition \\(\\|\\mathbf{\\zeta}\\| \\leq \\mathcal{T_r}\\). In special cases, the shape of the trust region might vary. The form of the model function is given by a quadratic function, given by, \\[\\begin{equation} M_n(\\mathbf{x}_n+\\mathbf{\\zeta}) = f(\\mathbf{x}_n) + \\mathbf{\\zeta}^T\\nabla f(\\mathbf{x}_n)+\\frac{1}{2}\\mathbf{\\zeta}^T\\mathbf{B} f(\\mathbf{x}_n)\\mathbf{\\zeta} \\tag{2.19} \\end{equation}\\] Where, \\(\\mathbf{B}f(\\mathbf{x}_n)\\) is either the Hessian matrix \\(\\mathbf{H}f(\\mathbf{x}_n)\\) or an approximation to it. Before moving into detailed discussions on line search descent methods and trust region methods in the later chapters, we will first deal with solving equation Eq.(2.17) in the immediate next chapter, which is itself an unconstrained one dimensional minimization problem, where we have to solve for \\[\\underset{\\beta&gt;0}{\\min\\ }\\tilde{f}(\\beta)\\] and deduce the value of \\(\\beta^*\\), which is the minimizer for \\(\\tilde{f}(\\beta)\\). "],["solving-one-dimensional-optimization-problems.html", "Chapter 3 Solving One Dimensional Optimization Problems 3.1 One Dimensional Optimization Problems 3.2 What is a Unimodal Function? 3.3 Fibonacci Search Method 3.4 Golden Section Search Method 3.5 Powells Quadratic Interpolation Method 3.6 Inverse Quadratic Interpolation Method 3.7 Newtons Method", " Chapter 3 Solving One Dimensional Optimization Problems This chapter introduces the detailed study on various algorithms for solving one dimensional optimization problems. The classes of methods that have been discussed are: Elimination method, Interpolation method and Direct Root Finding method. The Elimination method covers the Fibonacci Search method and the Golden Section Search method; the Interpolation method covers Quadratic Interpolation and Inverse Quadratic Interpolation methods; and the Direct Root Finding method covers Newtons method, Halleys method, Secant method and Bisection method. Finally a combination of some of these methods called the Brents method has also been discussed. Python programs involving the functions provided by the scipy.optimize module for solving problems using the above algorithms have also been provided. For the Fibonacci Search method and the Inverse Quadratic Interpolation method, full Python programs have been written for assisting the readers. 3.1 One Dimensional Optimization Problems The aim of this chapter is to introduce methods for solving one-dimensional optimization tasks, formulated in the following way: \\[\\begin{equation} f(x^*)=\\underset{x}{\\min\\ }f(x), x \\in \\mathbb{R} \\tag{1.1} \\end{equation}\\] where, \\(f\\) is a nonlinear function. The understanding of these optimization tasks and algorithms will be generalized in solving unconstrained optimization tasks involving objective function of multiple variables, in the future chapters. 3.2 What is a Unimodal Function? Definition 3.1 A function \\(f(x)\\), where \\(x \\in \\mathbb{R}\\) is said to be unimodal [refer to https://math.mit.edu/~rmd/465/unimod-dip-bds.pdf] if for a value \\(x^*\\) on the real line, the following conditions are satisfied: * \\(f\\) is monotonically decreasing for \\(x \\leq v\\), * \\(f\\) is monotonically increasing for \\(x \\geq v\\), and * if the above two conditions are satisfied, then \\(f(x^*)\\) is the minimum value of \\(f(x)\\), and \\(x^*\\) is the minimizer of \\(f\\). Let us have a look into the figure below. We have taken the quadratic function of one variable: \\(f(x) = 5x^2-3x+2\\). It is a nonlinear unimodal function defined over the interval \\([-2,2]\\), denoted by the dotted lines on either side.. The minimizer \\(x^*=0.3\\) (which can be solved analytically!), given by the middle dotted line, lies inside the interval \\([x_l, x_r]=[-2,2]\\). We notice that \\(f(x)\\) strictly decreases for \\(f(x) &lt; f(x^*)\\) and strictly increases for \\(f(x) &gt; f(x^*)\\). The interval \\([x_l, x_r]\\) that has the minimizer within it, is called the interval of uncertainty and the goal of an optimization algorithm is to reduce this interval as much as possible to converge towards the minimizer. A good algorithm completes the convergence very fast. In each step of this reduction of the interval, the algorithm finds a new unimodal interval following the following procedures: Choose two new points, \\(x_1 \\in [x_l, x^*]\\) and another point \\(x_2 \\in [x^*, x_r]\\) (denoted by the two filled straight lines in the figure, If \\(f(x_2) &gt; f(x_1)\\), the new interval becomes \\([x_l, x_2]\\) and \\(x_r\\) becomes \\(x_2\\), i.e, \\(x_r=x_2\\), Next pick a new \\(x_2\\), If condition in step (2) is not satisfied, we set the new interval as \\([x_1, x_r]\\) directly after step (1) and set \\(x_l=x_1\\), and Next pick a new \\(x_1\\). The given steps continue iteratively until the convergence is satisfied to a given limit of the minimizer. These class of methods is called an Elimination Method and we study two categories under this kind: Fibonacci Search, and Golden Section Search. Raos book Engineering Optimization [Rao, Singiresu S. Engineering optimization: theory and practice. John Wiley &amp; Sons, 2019.] also has some detailed studies on these kinds of optimization methods. 3.3 Fibonacci Search Method Instead of finding the exact minimizer \\(x^*\\) of \\(f(x)\\), the Fibonacci search strategy works by reducing the interval of uncertainty in every step, ultimately converging the interval, containing the minimizer, to a desired size as small as possible. One caveat is that, the initial interval containing, such that the interval lies in it, has to be known beforehand. However, the algorithm works on a nonlinear function, even if it is discontinuous. The name comes from the fact that the algorithm makes use of the famous sequence of Fibonacci numbers [http://oeis.org/A000045]. This sequence is defined in the following way: \\[\\begin{align} F_0&amp;=0,F_1=1, \\\\ F_n&amp;=F_{n-1} + F_{n-2},\\text{ where }n=2,3,\\ldots \\end{align}\\] We write a Python code to generate the first 16 Fibonacci numbers and display them as a table: import pandas as pd import numpy as np def fibonacci(n): # define the function fn = [0, 1,] for i in range(2, n+1): fn.append(fn[i-1] + fn[i-2]) return fn N = np.arange(16) data = {&#39;n&#39;: N, &#39;Fibonacci(n)&#39;: fibonacci(15)} df = pd.DataFrame(data) df looks like this: ## +----+-----+----------------+ ## | | n | Fibonacci(n) | ## |----+-----+----------------| ## | 0 | 0 | 0 | ## | 1 | 1 | 1 | ## | 2 | 2 | 1 | ## | 3 | 3 | 2 | ## | 4 | 4 | 3 | ## | 5 | 5 | 5 | ## | 6 | 6 | 8 | ## | 7 | 7 | 13 | ## | 8 | 8 | 21 | ## | 9 | 9 | 34 | ## | 10 | 10 | 55 | ## | 11 | 11 | 89 | ## | 12 | 12 | 144 | ## | 13 | 13 | 233 | ## | 14 | 14 | 377 | ## | 15 | 15 | 610 | ## +----+-----+----------------+ Let \\(n\\) be the total number of experiments to be conducted and \\([x_l, x_r]\\) be the initial interval the algorithm starts with. Let \\[\\begin{eqnarray} L_0 = x_r - x_l \\tag{1.2} \\end{eqnarray}\\] be the initial level of uncertainty and let us define, \\[\\begin{eqnarray} L_j = \\frac{F_{n-2}}{F_n}L_0 \\tag{1.3} \\end{eqnarray}\\] where, \\(F_{n-2}\\) and \\(F_n\\) are the \\((n-2)^{th}\\) and \\(n^{th}\\) Fibonacci numbers respectively. We see from the formulation of the Fibonacci numbers that, (1.3) shows the following property: \\[\\begin{equation} L_j = \\frac{F_{n-2}}{F_n}L_0 \\leq \\frac{L_0}{2} \\text{ for } n\\geq 2 \\end{equation}\\] Now, the initial two experiments are set at points \\(x_1\\) and \\(x_2\\), where, \\(L_j = x_1 - x_l\\) and \\(L_j = x_r - x_2\\). So, combining these with Eq.(1.3), we have: \\[\\begin{equation} x_1 = x_l + \\frac{F_{n-2}}{F_n}L_0 \\tag{1.4} \\end{equation}\\] and \\[\\begin{equation} x_2 = x_r - \\frac{F_{n-2}}{F_n}L_0 \\tag{1.5} \\end{equation}\\] Now taking into consideration the unimodality assumption, a part of the interval of uncertainty is rejected, shrinking it to a smaller size, given by, \\[\\begin{equation} L_i = L_0 - L_j = L_0(1-\\frac{F_{n-2}}{F_n}) = \\frac{F_{n-1}}{F_n}L_0 \\tag{1.6} \\end{equation}\\] where, we have used the fact that, \\(F_n - F_{n-2} = F_{n-1}\\) from the formulation of the Fibonacci numbers. This procedure leaves us with only one experiment, which, from one end, is situated at a distance of \\[\\begin{equation} L_j = \\frac{F_{n-2}}{F_n}L_0 = \\frac{F_{n-2}}{F_{n-1}}L_i \\tag{1.7} \\end{equation}\\] where, we have used Eq.(1.3). From the other end, the same experiment point is situated at a distance give by, \\[\\begin{equation} L_i-L_j = \\frac{F_{n-3}}{F_n}L_0 = \\frac{F_{n-3}}{F_n}L_0 = \\frac{F_{n-3}}{F_{n-1}}L_2 \\tag{1.8} \\end{equation}\\] where, we have again used Eq.(1.3). We now place a new experiment point in the interval \\(L_i\\) so that both the present experiment points are situated at a distance given by Eq.(1.7). We again reduce the size of the interval of uncertainty using the unimodality conditions. This whole process is continued so that for the \\(k^{th}\\) experiment point, its location is given by, \\[\\begin{equation} L_{k[j]} = \\frac{F_{n-k}}{F_{n-(k-2)}}L_{k-1} \\tag{1.9} \\end{equation}\\] and the interval of uncertainty is given by, \\[\\begin{equation} l_{k[i]} = \\frac{F_{n-(k-1)}}{F_n}L_0 \\tag{1.10} \\end{equation}\\] after \\(k\\) iterations are completed. Now, the reduction ratio given by the ratio of the present interval of uncertainty after conduction \\(k\\) iterations out of the \\(n\\) experiments to be performed, \\(L_{k[i]}\\) to the initial interval of uncertainty, \\(L_0\\): \\[\\begin{equation} R = \\frac{L_{k[i]}}{L_0} = \\frac{F_{n-(k-1)}}{F_n} \\tag{1.11} \\end{equation}\\] The purpose of this algorithm is to bring \\(R \\sim 0\\). The Fibonacci Search Algorithm has been shown below: We will write a Python function that implements the above algorithm def fib_search(f, xl, xr, n): F = fibonacci(n) # Call the fibonnaci number function L0 = xr - xl # Initial interval of uncertainty R1 = L0 # Initial Reduction Ratio Li = (F[n-2]/F[n])*L0 R = [Li/L0] for i in range(2, n+1): if Li &gt; L0/2: x1 = xr - Li x2 = xl + Li else: x1 = xl + Li x2 = xr - Li f1, f2 = f(x1), f(x2) if f1 &lt; f2: xr = x2 Li = (F[n - i]/F[n - (i - 2)])*L0 # New interval of uncertainty elif f1 &gt; f2: xl = x1 Li = (F[n - i]/F[n - (i - 2)])*L0 # New interval of uncertainty else: xl, xr = x1, x2 Li = (F[n - i]/F[n - (i - 2)])*(xr - xl) # New interval of uncertainty L0 = xr - xl R += [Li/R1,] # Append the new reduction ratio if f1 &lt;= f2: return [x1, f(x1), R] # Final result else: return [x2, f(x2), R] # Final result Example 3.1 Let an objective function be: \\[\\begin{equation} f(x) = x^5 - 5x^3 - 20x + 5 \\tag{1.12} \\end{equation}\\] We will use the Fibonacci search algorithm to find the minimizer \\(x^*\\), taking \\(n=25\\) and the initial interval of uncertainty \\([-2.5, 2.5]\\). Lets write a Python function to define the given objective function and visualize the same: def f(x): # Objective function return x**5 - 5*x**3 - 20*x + 5 x = np.linspace(-3, 3, 100) plt.plot(x, f(x), &#39;r-&#39;) plt.xlabel(&#39;x -&gt;&#39;) plt.ylabel(&#39;f(x) -&gt;&#39;) plt.title(&quot;Graph of $f(x) = x^5-5x^3-20x+5$&quot;) plt.show() Now, we consider \\(n=25\\) and use the function fib_search(f, -2.5, 2.5, 25) to run the optimization and print the results: Fib = fib_search(f, -2.5, 2.5, 25) x_star, f_x_star, R = Fib print (&quot;x*:&quot;, x_star) ## x*: 1.999966677774075 print (&quot;f(x*):&quot;, f_x_star) ## f(x*): -42.99999994448275 print (&quot;Final Reduction Ratio:&quot;, R[-1]) ## Final Reduction Ratio: 0.0 We see that \\(x^* \\sim 2\\), \\(f(x^*) \\sim -43\\) and the final Reduction Ration is 0. Now, to show the positions of \\(x^*\\) and \\(f(x^*)\\) on the graph of the objective function, we write the following code: x = np.linspace(-3, 3, 100) plt.plot(x, f(x), &#39;r-&#39;) plt.plot([x_star], [f_x_star], &#39;ko&#39;) plt.axvline(x=x_star, color=&#39;b&#39;, linestyle=&#39;--&#39;) plt.xlabel(&#39;x -&gt;&#39;) plt.ylabel(&#39;f(x) -&gt;&#39;) plt.title(&quot;$x^*$ denoted as broken blue line and $f(x^*)$ denoted as the black dot&quot;) plt.show() We can modify our function in such a way that all the optimization data in every step are collected and displayed as a DataFrame: def fib_search(f, xl, xr, n): F = fibonacci(n) L0 = xr - xl ini = L0 Li = (F[n-2]/F[n])*L0 R = [Li/L0] a = [xl] b = [xr] F1 = [f(xl)] F2 = [f(xr)] for i in range(2, n+1): #print(&quot;reduction ratio:&quot;, Li/ini) if Li &gt; L0/2: x1 = xr - Li x2 = xl + Li else: x1 = xl + Li x2 = xr - Li f1, f2 = f(x1), f(x2) if f1 &lt; f2: xr = x2 Li = (F[n - i]/F[n - (i - 2)])*L0 elif f1 &gt; f2: xl = x1 Li = (F[n - i]/F[n - (i - 2)])*L0 else: xl, xr = x1, x2 Li = (F[n - i]/F[n - (i - 2)])*(xr - xl) L0 = xr - xl R += [Li/ini,] a += [xl, ] b += [xr, ] F1 += [f1, ] F2 += [f2, ] data = {&#39;n&#39; : range(0, n), &#39;xl&#39;: a, &#39;xr&#39;: b, &#39;f(x1)&#39;: F1, &#39;f(x2)&#39;: F2, &#39;Reduction Ratio&#39;: R} df = pd.DataFrame(data, columns = [&#39;n&#39;, &#39;xl&#39;, &#39;xr&#39;, &#39;f(x1)&#39;, &#39;f(x2)&#39;, &#39;Reduction Ratio&#39;]) return df df = fib_search(f, -2.5, 2.5, 25) Where df looks like this: ## +----+-----+----------+---------+-----------+-----------+-------------------+ ## | | n | xl | xr | f(x1) | f(x2) | Reduction Ratio | ## |----+-----+----------+---------+-----------+-----------+-------------------| ## | 0 | 0 | -2.5 | 2.5 | 35.4688 | -25.4688 | 0.381966 | ## | 1 | 1 | -0.59017 | 2.5 | 17.7596 | -7.75959 | 0.381966 | ## | 2 | 2 | 0.59017 | 2.5 | -7.75959 | -28.8819 | 0.236068 | ## | 3 | 3 | 1.31966 | 2.5 | -28.8819 | -40.7626 | 0.145898 | ## | 4 | 4 | 1.77051 | 2.5 | -40.7626 | -42.875 | 0.0901699 | ## | 5 | 5 | 1.77051 | 2.22136 | -42.875 | -40.1458 | 0.0557281 | ## | 6 | 6 | 1.94272 | 2.22136 | -42.8424 | -42.875 | 0.0344419 | ## | 7 | 7 | 1.94272 | 2.11493 | -42.875 | -42.2847 | 0.0212862 | ## | 8 | 8 | 1.94272 | 2.04915 | -42.9964 | -42.875 | 0.0131556 | ## | 9 | 9 | 1.98337 | 2.04915 | -42.9863 | -42.9964 | 0.00813062 | ## | 10 | 10 | 1.98337 | 2.02403 | -42.9964 | -42.9707 | 0.00502499 | ## | 11 | 11 | 1.98337 | 2.0085 | -42.9999 | -42.9964 | 0.00310563 | ## | 12 | 12 | 1.99297 | 2.0085 | -42.9975 | -42.9999 | 0.00191936 | ## | 13 | 13 | 1.99297 | 2.00257 | -42.9999 | -42.9997 | 0.00118627 | ## | 14 | 14 | 1.99663 | 2.00257 | -42.9994 | -42.9999 | 0.000733089 | ## | 15 | 15 | 1.9989 | 2.00257 | -42.9999 | -43 | 0.000453182 | ## | 16 | 16 | 1.9989 | 2.00117 | -43 | -42.9999 | 0.000279907 | ## | 17 | 17 | 1.9989 | 2.0003 | -43 | -43 | 0.000173276 | ## | 18 | 18 | 1.99943 | 2.0003 | -43 | -43 | 0.000106631 | ## | 19 | 19 | 1.99977 | 2.0003 | -43 | -43 | 6.66445e-05 | ## | 20 | 20 | 1.99977 | 2.0001 | -43 | -43 | 3.99867e-05 | ## | 21 | 21 | 1.9999 | 2.0001 | -43 | -43 | 2.66578e-05 | ## | 22 | 22 | 1.9999 | 2.00003 | -43 | -43 | 1.33289e-05 | ## | 23 | 23 | 1.99997 | 1.99997 | -43 | -43 | 0 | ## | 24 | 24 | 1.99997 | 1.99997 | -43 | -43 | 0 | ## +----+-----+----------+---------+-----------+-----------+-------------------+ The graph of the reduction ratio at each \\(n\\) can be plotted with the following code: plt.xlabel(&quot;n-&gt;&quot;) plt.ylabel(&quot;Reduction Ratio -&gt;&quot;) plt.plot(range(0, len(df)), df[&#39;Reduction Ratio&#39;]) plt.title(&quot;Graph of change of reduction ratio at each $n$&quot;) plt.show() 3.4 Golden Section Search Method The golden section search method is a modified version of the Fibonacci search method. One advantage of the former over the later is that, we do not need to keep a record of the total number of experiment points \\(n\\) beforehand. While selecting \\(x_1\\) and \\(x_2\\) inside the interval of uncertainty, we make use of the golden ratio, \\(\\phi = \\frac{\\sqrt{5} - 1}{2}\\) which is the positive root of the quadratic equation given by: \\[\\begin{equation} \\phi^2+\\phi-1=0 \\tag{1.13} \\end{equation}\\] Given the initial interval \\([x_l, x_r]\\), we have the initial interval of uncertainty as, \\[\\begin{equation} L_0 = x_r - x_l \\tag{1.14} \\end{equation}\\] The new interior points \\(x_1\\) and \\(x_2\\) are chosen in such a way that both of them lies at a distance \\(\\phi^2L_0\\) from either side, i.e, \\[\\begin{equation} x_1-x_l = \\phi^2L_0 \\tag{1.15} \\end{equation}\\] and \\[\\begin{equation} x_r-x_2 = \\phi^2L_0 \\tag{1.16} \\end{equation}\\] Now from Eq.(1.13) we know \\[\\begin{equation} L_0 = (\\phi^2 + \\phi)L_0 \\tag{1.17} \\end{equation}\\] The above computations leave us with: \\[\\begin{equation} x_r - x_1 = \\phi L_0 \\tag{1.18} \\end{equation}\\] and \\[\\begin{equation} x_2 - x_l = \\phi L_0 \\tag{1.19} \\end{equation}\\] Given, \\(f(x)\\) is the nonlinear objective function, we now check whether \\(f(x_1) &gt; f(x_2)\\). If this is the case, we set \\(x_l = x_1\\), otherwise if \\(f(x_1) &gt; f(x_2)\\), we set \\(x_r=x_2\\). The new interval of uncertainty is set to be \\(L_i = \\phi L_0\\) and the previous interval is shrunk. This process of choosing new experimental points and shrinking the interval of uncertainty is continued until the termination condition is satisfied. The termination condition is to check whether the interval of uncertainty is less than a particular tolerance \\(\\epsilon\\) usually provided by the user. The golden section search algorithm is given below: Example 3.2 Let us consider an objective function: \\[\\begin{equation} f(x) = \\frac{1}{16}x^3 - \\frac{27}{4}x \\tag{1.20} \\end{equation}\\] We will use the golden section search method to find the minimizer \\(x^*\\) of this function and compute \\(f(x^*)\\). Suppose the initial interval be \\([-10, 10]\\) and the tolerance for the termination condition for the algorithm be \\(\\epsilon = 10^{-5}\\). Let us first define the function in Python: def f(x): # objective function return x**3/16 - 27*x/4 The graph of the objective function: x = np.linspace(-20, 20, 100) plt.plot(x, f(x), &#39;r-&#39;) plt.xlabel(&#39;x -&gt;&#39;) plt.ylabel(&#39;f(x) -&gt;&#39;) plt.title(&quot;Graph of $f(x) = \\\\frac{1}{16}x^3 - \\\\frac{27}{4}x$&quot;) plt.show() For tackling this problem, we will not write our own Python function. As already stated in the last chapter, the scipy.optimize package too equips us with solvers to solve these tasks. For this problem, we use the minimize_scalar() function provided by scipy.optimize, which is used for minimization of a scalar function of one variable. The minimize_scalar() function provides the user with the following parameter: fun: The objective function which must be callable, bracket: This is an optional parameter and defines the bracketing interval. This is a sequence, and consists of either three points \\((x_a, x_b, x_c)\\), such that \\(x_a &lt; x_b &lt; x_c\\) and \\(f(x_b) &lt; f(x_a), f(x_c)\\), or two points \\((x_a, x_b)\\) that are considered as the starting interval for any elimination search method, bounds: This is an optional parameter too (important for our analysis!) and is a sequence. This defines the optimization bound, i.e, the initial interval of uncertainty, \\([x_l, x_r]\\), args: This is a tuple and an optional parameter that defines the extra arguments that might be needed to pass to the objective function, method: This is the most important parameter that defines the various solvers provided by minimize_scalar(). This should be either a string (str) or a callable object. As of writing this book, the solvers that minimize_scalar() provides are: 'golden': Uses the golden section search method for finding the local minimum, 'brent': Uses the Brents algorithm (will be discussed in the next section) for finding the local minimum, 'bounded': For performing bounded minimization and uses the Brent's algorithm to find the local minima specified in the 'bounds' parameter. The method parameter is optional too, and if not provided, the minimize_scalar() function uses the 'brent' method by default. The user can also write and pass a custom solver which must be a Python callable object. tol: This parameter represents the tolerance (\\(\\epsilon\\)) of the optimization algorithm. This must be a float and is optional too, and options: This is an optional parameter and is a Python dictionary which specifies the solver options: maxiter: This is an int object and denotes the maximum number of iterations to be performed by the solver, disp: This must be a boolean (bool) object, and if set to True, prints a detailed information about the convergence of the algorithm The minimize_scalar() function returns the optimization result as a specific Python object designed specifically for the scipy.optimize module called, OptimizeResult. It has the following important attributes: x: The solution (\\(x^*\\)) of the optimization. This is a numpy array object, i.e, ndarray and can return a scalar or a vector, success: This is a bool object and states whether the optimization process has completed successfully or not, fun, jac, hess: Provides the objective function, Jacobian and the Hessian matrix at the solution \\(x^*\\) as ndarray objects, nfev, njev, nhev: Provides the number of evaluations of the objective function, its Jacobian and Hessian matrix during the running of the optimization solver and are int objects, This is an int object and states the number of iterations that have been performed by the solver, and maxcv: This is a float object and represents the maximum constraint evaluation. Now returning back to our example, we have \\(f(x)\\) defined by Eq.(1.20), we use the golden section search method to find its minimizer. Using the minimize_scalar() function and setting parameters method = 'golden', bounds = (-10, 10) and tol = 10**-5 we can get our solution. We see that the initial interval has been set to \\([-10, 10]\\) and the tolerance \\(\\epsilon\\) has been set \\(10^{-5}\\). We write the Python code: from scipy.optimize import minimize_scalar result = minimize_scalar(f, bounds = (-10, 10), method = &#39;golden&#39;, tol = 10**-5) print(result) ## fun: -26.99999999991886 ## nfev: 32 ## nit: 26 ## success: True ## x: 5.99999150720724 We notice that \\(x^* \\sim 6\\), \\(f(x^*) \\sim -27\\), the number of iterations it took to converge to \\(x^*\\) is 26 and other attributes that have been listed methodically. With some little extra Python codes, the user can also collect the data of the optimization steps, given below: ## +----+----------+------------+ ## | | x | f(x) | ## |----+----------+------------| ## | 0 | 0 | 0 | ## | 1 | 1 | -6.6875 | ## | 2 | 2.61803 | -16.5502 | ## | 3 | 15.287 | 120.093 | ## | 4 | 2.61803 | -16.5502 | ## | 5 | 7.45716 | -24.4179 | ## | 6 | 10.4479 | 0.756678 | ## | 7 | 5.60878 | -26.8316 | ## | 8 | 4.46642 | -24.5796 | ## | 9 | 6.3148 | -26.8866 | ## | 10 | 6.75114 | -26.3388 | ## | 11 | 6.04512 | -26.9977 | ## | 12 | 5.87846 | -26.9835 | ## | 13 | 6.14813 | -26.9751 | ## | 14 | 5.98146 | -26.9996 | ## | 15 | 5.94212 | -26.9962 | ## | 16 | 6.00578 | -27 | ## | 17 | 6.02081 | -26.9995 | ## | 18 | 5.99649 | -27 | ## | 19 | 5.99075 | -26.9999 | ## | 20 | 6.00004 | -27 | ## | 21 | 6.00223 | -27 | ## | 22 | 5.99868 | -27 | ## | 23 | 6.00088 | -27 | ## | 24 | 5.99952 | -27 | ## | 25 | 6.00036 | -27 | ## | 26 | 5.99984 | -27 | ## | 27 | 6.00016 | -27 | ## | 28 | 5.99996 | -27 | ## | 29 | 5.99992 | -27 | ## | 30 | 5.99999 | -27 | ## | 31 | 6.00001 | -27 | ## +----+----------+------------+ The optimization steps can be plotted too. The graph with all the function evaluations along with the minimizer \\(f(x^*)\\) at \\(x^*\\) has been denoted as a blue dotted line in the below figure which can be generated using the following Python code: plt.plot(np.linspace(-20, 20, 100), f(np.linspace(-20, 20, 100)), &#39;r-&#39;) plt.plot(df[&#39;x&#39;], df[&#39;f(x)&#39;], &#39;ko-&#39;) plt.axvline(x=result.x, color=&#39;b&#39;, linestyle=&#39;--&#39;) plt.axhline(y=result.fun, color=&#39;g&#39;, linestyle=&#39;--&#39;) plt.xlabel(&#39;x -&gt;&#39;) plt.ylabel(&#39;f(x) -&gt;&#39;) plt.title(&quot;optimization of $f(x) = \\\\frac{1}{16}x^3 - \\\\frac{27}{4}x$&quot;) If we analyse closely the optimization data in the dataframe and look at the third, fourth and fifth steps, we see that \\(f(x[3]) ~\\sim -16.55\\), \\(f(x[4]) \\sim 120.1\\) and again \\(f(x[5]) ~\\sim -16.55\\). This interesting overshooting can be also seen in the visualization given by the above figure. We next discuss interpolation methods to find the minimum of a nonlinear unimodal objective function. This methods use polynomial approximation for modeling the objective function. We will study two methods under this class of methods: Powells quadratic interpolation method, and Inverse quadratic interpolation method 3.5 Powells Quadratic Interpolation Method Suppose, the objective function is \\(f(x), x\\in \\mathbb{R}\\) and the minimizer is \\(x^*\\). Powells method use successive quadratic interpolation curves for fitting to the objective function data. This gives a sequence of approximations to \\(x^*\\), denoted by \\(x_t\\). Initially three data points \\(x_0, x_1, x_2 \\in \\mathbb{R}\\) are provided. The interpolating quadratic polynomial through these data points \\(P(x)\\) is as followed: \\[\\begin{equation} P(x) = f(x_0) + (x - x_0)f[x_0, x_1] + (x-x_0)(x-x_1)f[x_0, x_1, x_2] \\tag{1.21} \\end{equation}\\] where, \\[\\begin{equation} f[x, y] = \\frac{f(y) - f(x)}{y - x} \\tag{1.22} \\end{equation}\\] is the first order forward divided difference, and \\[\\begin{equation} f[x, y, z] = \\frac{f[y, z] - f[x, y]}{z - x} \\tag{1.23} \\end{equation}\\] is the second order forward divided difference. \\(x_t\\) is the point where the slope of \\(P(x)\\) curve is \\(0\\). To find it, we set, \\[\\begin{align} &amp; \\frac{dP(x)}{dx} = 0 \\\\ &amp;\\Rightarrow f[x_0, x_1] + f[x_0, x_1, x_t](2x_t - x_0 - x_1) = 0 \\tag{1.24} \\end{align}\\] So we end up with \\(x_t\\), \\[\\begin{equation} x_t = \\frac{f[x_0, x_1, x_2](x_0, x_1) - f[x_0, x_1]}{2f[x_0, x_1, x_2]} \\tag{1.25} \\end{equation}\\] For \\(x_t\\) to be minimum, the following condition regarding the second order forward divided difference should be satisfied, \\[\\begin{equation} f[x_0, x_1, x_2] &gt; 0 \\tag{1.26} \\end{equation}\\] We can now say that \\(x_t\\) is a good approximation to \\(x^*\\). The algorithm for Powells quadratic interpolation method is given below: Example 3.3 Let us consider an objective function: \\[\\begin{equation} f(x) = x^4 - 2x^2 + \\frac{1}{4} \\tag{2.1} \\end{equation}\\] We will use Powells quadratic interpolation method to find out the minimizer \\(x^*\\) and the function value at this point, \\(f(x^*)\\). Let the initial starting point be \\(x=0.5\\), the discrete step size be \\(s=10^{-3}\\), the maximum step size be \\(m=30\\) and the tolerance be \\(\\epsilon = 10^{-5}\\). To start with the optimization process, let us first define the objective function given by Eq.(2.1)in Python and plot the function: def f(x): # define the objective function return x**4 - 2*x**2 + 1/4 l = np.linspace(-2, 2, 100) plt.plot(l, f(l), &#39;r-&#39;) plt.xlabel(&#39;x -&gt;&#39;) plt.ylabel(&#39;f(x) -&gt;&#39;) plt.title(&quot;Graph of $f(x) = x^4 - 2x^2 + \\\\frac{1}{4}$&quot;) plt.show() We will now write functions for the first order forward divided difference and the second order forward divided difference given by Eq.(1.22) and Eq.(1.22). def f1(x, y): # First order forward divided difference return (f(y) - f(x)) / (y - x) def f2(x, y, z): # Second order forward divided difference return (f1(y, z) - f1(x, y))/(z - x) Next, we write a function to find out the nearest value to a number n from a list, seq and a function to find out the furthest value to a number n from a list, seq. def nearest_to(seq, n): # Picks the nearest value to a number entered from a list return min(seq, key = lambda x: abs(x - n)) def furthest_to(seq, n): # Picks the furthest value to a number entered from a list return max(seq, key = lambda x: abs(x - n)) Let us consider a list be \\(L = \\{1.1, 2.7, 3.3, 3.2, 1.8, -0.9, -0.5, -6.33\\}\\) and a number be \\(0.7\\), we need to find the nearest value to \\(0.7\\) from \\(L\\) and the furthest value to \\(0.7\\) from \\(L\\). We use the functions: L = [1.1, 2.7, 3.3, 3.2, 1.8, -0.9, -0.5, -6.33] n = 0.7 print(&quot;The nearest value to&quot;, n, &quot;from &quot;, L, &quot;:&quot;, nearest_to(L, n)) ## The nearest value to 0.7 from [1.1, 2.7, 3.3, 3.2, 1.8, -0.9, -0.5, -6.33] : 1.1 print(&quot;The furthest value to&quot;, n, &quot;from &quot;, L, &quot;:&quot;, furthest_to(L, n)) ## The furthest value to 0.7 from [1.1, 2.7, 3.3, 3.2, 1.8, -0.9, -0.5, -6.33] : -6.33 We now require to write a function that returns the element from a list that has the maximum value of \\(f(x)\\) where \\(f\\) is the objective function. def maximum_fvalue(seq): fu = f(np.array(seq)) # Converts a Python list to a ndarray object return seq[np.where(fu==np.amax(fu))[0][0]] # Picks up the index from the ndarray sequence, the element at which has the maximum f(x) value and returns the element from the sequence Let us use the same sequence and find out the element from it that fas the maximum value of \\(f\\), L = [1.1, 2.7, 3.3, 3.2, 1.8, -0.9, -0.5, -6.33] print(f(np.array(L))) # Prints the f(x)&#39;s at all the x&#39;s from the sequence ## [-7.05900000e-01 3.88141000e+01 9.70621000e+01 8.46276000e+01 ## 4.26760000e+00 -7.13900000e-01 -1.87500000e-01 1.52562895e+03] print(maximum_fvalue(L)) # Prints the element with the highest f(x) value ## -6.33 We see that from the sequence \\(-6.33\\) has the highest \\(f\\) value, that is \\(f(-6.33) = 1.52562895\\times 10^3\\) is the maximum value as can be seen in the printed ndarray. We finally write the function that implements **Powells Quadratic Interpolation Algorithm\" and name it powell_quad() with the parameters x, s, m, and tol: def powell_quad(x, s, m, tol): if f(x) &lt; f(x + s): x0 = x - s x1 = x x2 = x + s else: x0 = x x1 = x + s x2 = x + 2 * s L = [x0, x1, x2] # Set x0, x1 and x2 XT = [] while True: M = f2(L[0], L[1], L[2]) xt = (M * (L[0] + L[1]) - f1(L[0], L[1]))/(2 * M) # The approximate minimizer xn = nearest_to(L, xt) # Picks the point from [x0, x1, x2] which is the nearest to xt xf = furthest_to(L, xt) # Picks the point from [x0, x1, x2] which is the furthest to xt if M &gt; 0 and abs(xt - xn) &gt; m: # Checks for equation 3.26 L.remove(xf) # Remove xf from [x0, x1, x2] value = min(L) + m L += [value, ] L.sort() # Take a step of size m towards the direction of descent from the point with the lowest value elif M &lt; 0 : L.remove(xn) # Remove xn from [x0, x1, x2] value = min(L) + m L += [value, ] L.sort() # Take a step of size m towards the direction of descent from the point with the lowest value else: # print((xt + xn) / 2, f((xt + xn) / 2)) # If the user wants to print the steps at all function evaluations at the approximate minimizers, uncomment the command XT += [(xt + xn) / 2, ] if abs(xt - xn) &lt; tol: # Check for the terminating condition return [(xt + xn) / 2, f((xt + xn) / 2), XT] # Return the results else: mx = maximum_fvalue(L) L.remove(mx) L += [xt, ] L.sort() # Replace the element from [x0, x1, x2] having the maximum function value with xt Now returning back to our original example problem, we have \\(x = 0.5\\), \\(s=10^{-3}\\), \\(m=30\\) and \\(\\epsilon=10^{-5}\\). We will use these values as parameter values for our function powell_quad() and check the result: res = powell_quad(0.5, 10**-3, 30, 10**-5) x_star = res[0] f_x_star = res[1] print(&#39;x*:&#39;, x_star) ## x*: 1.000000513307495 print(&#39;f(x*):&#39;, f_x_star) ## f(x*): -0.7499999999989462 Now let us collect the optimization data and store in a dataframe df: XT = np.array(res[2]) F = f(XT) data = {&#39;xt&#39;: XT, &#39;f(xt)&#39;: F} df = pd.DataFrame(data, columns=[&#39;xt&#39;, &#39;f(xt)&#39;]) The last few rows of df looks like: ## +-----+----------+-----------+ ## | | xt | f(xt) | ## |-----+----------+-----------| ## | 174 | 1.24152 | -0.456924 | ## | 175 | 0.77715 | -0.593154 | ## | 176 | 1.04659 | -0.740909 | ## | 177 | 0.896613 | -0.711551 | ## | 178 | 1.00525 | -0.749889 | ## | 179 | 0.978394 | -0.748173 | ## | 180 | 0.998358 | -0.749989 | ## | 181 | 0.999505 | -0.749999 | ## | 182 | 1.00004 | -0.75 | ## | 183 | 1 | -0.75 | ## +-----+----------+-----------+ To plot the optimization data, type: plt.plot(np.linspace(-2, 2, 100), f(np.linspace(-2, 2, 100)), &#39;r-&#39;) plt.plot(df[&#39;xt&#39;], df[&#39;f(xt)&#39;], &#39;ko-&#39;) plt.axvline(x=x_star, color=&#39;b&#39;, linestyle=&#39;--&#39;) plt.axhline(y=f_x_star, color=&#39;g&#39;, linestyle=&#39;--&#39;) plt.xlabel(&#39;x -&gt;&#39;) plt.ylabel(&#39;f(x) -&gt;&#39;) plt.xlim(-2, 2) plt.ylim(-2, 10) plt.title(&quot;optimization of $f(x) = x^4 - 2x^2 + \\\\frac{1}{4}$&quot;) plt.show() We notice that the blue dashed line gives the position of \\(x^*\\) and the green dashed line gives the corresponding \\(f(x^*)\\) value. Now, we change our original problem a little bit and consider the initial starting point to be \\(x=-0.5\\) and reduce the maximum step size to \\(m=10\\) , keeping all the other parameters same. res = powell_quad(-0.5, 10**-3, 10, 10**-5) x_star = res[0] f_x_star = res[1] print(&#39;x*:&#39;, x_star) ## x*: -1.0000000470794883 print(&#39;f(x*):&#39;, f_x_star) ## f(x*): -0.7499999999999911 Now let us collect the optimization data and store in a dataframe df: XT = np.array(res[2]) F = f(XT) data = {&#39;xt&#39;: XT, &#39;f(xt)&#39;: F} df = pd.DataFrame(data, columns=[&#39;xt&#39;, &#39;f(xt)&#39;]) The last few rows of df looks like: ## +----+-----------+-----------+ ## | | xt | f(xt) | ## |----+-----------+-----------| ## | 23 | -1.17087 | -0.612412 | ## | 24 | -0.807657 | -0.629111 | ## | 25 | -1.02543 | -0.747348 | ## | 26 | -0.924304 | -0.728783 | ## | 27 | -0.983006 | -0.748864 | ## | 28 | -0.995355 | -0.749914 | ## | 29 | -1.00137 | -0.749992 | ## | 30 | -1.00004 | -0.75 | ## | 31 | -0.999992 | -0.75 | ## | 32 | -1 | -0.75 | ## +----+-----------+-----------+ To plot the optimization data, type: plt.plot(np.linspace(-2, 2, 100), f(np.linspace(-2, 2, 100)), &#39;r-&#39;) plt.plot(df[&#39;xt&#39;], df[&#39;f(xt)&#39;], &#39;ko-&#39;) plt.axvline(x=x_star, color=&#39;b&#39;, linestyle=&#39;--&#39;) plt.axhline(y=f_x_star, color=&#39;g&#39;, linestyle=&#39;--&#39;) plt.xlabel(&#39;x -&gt;&#39;) plt.ylabel(&#39;f(x) -&gt;&#39;) plt.xlim(-2, 2) plt.ylim(-2, 10) plt.title(&quot;optimization of $f(x) = x^4 - 2x^2 + \\\\frac{1}{4}$&quot;) plt.show() We have \\(x^* \\sim -1\\) and \\(f(x^*) \\sim -0.75\\), i.e, now the \\(x^*\\) converges to the local minimum on the left side. 3.6 Inverse Quadratic Interpolation Method The main motivation of using the inverse quadratic interpolation method [refer to An Introduction to Numerical Methods and Analysis by James F. Epperson] is to use the quadratic interpolation method to find the inverse of the objective function \\(f(x)\\). This algorithm forms an integral part of the Brents method for optimization, which we will study later in this chapter. In this method, we have three initial points to start with, given by, \\(x_0, x_1, x_2 \\in \\mathbb{R}\\). Our aim is to find the polynomial \\(q(f(x))\\), such that, \\[\\begin{equation} q(f(x_j)) = x_j \\tag{2.2} \\end{equation}\\] which identifies, \\[\\begin{equation} q = f^{-1} \\tag{2.3} \\end{equation}\\] Now, using the divided difference forms we can write, \\[\\begin{align} q(y) &amp;= f^{-1}(y_0) + (y - y_0)\\frac{f^{-1}(y_1) - f^{-1}(y_0)}{y_1 - y_0}\\\\ &amp;+ \\frac{(y-y_0)(y-y_1)}{(y_2-y_0)}(\\frac{f^{-1}(y_2) - f^{-1}(y_1)}{y_2 - y_1} - \\frac{f^{-1}(y_1) - f^{-1}(y_0)}{y_1 - y_0})\\\\ &amp;= x_0 + (y-y_0) \\frac{(x-x_0}{(y_1-y_0)}\\\\ &amp;+\\frac{(y-y_0)(y-y_1)}{(y_2-y_0)}(\\frac{(x_2 - x_1)}{(y_2-y_1)} - \\frac{(x_1-x_0)}{(y_1-y_0)}) \\tag{2.4} \\end{align}\\] where, \\[\\begin{equation} y_j = f(x_j) \\tag{2.5} \\end{equation}\\] Now, the approximate minimizer \\(x_t\\) is the value \\(q(0)=x_t\\), i.e, at \\(y=0\\). Putting \\(y=0\\) in Eq.(2.4), we have, \\[\\begin{equation} x_t = q(0) = x_0 - y_0 \\frac{(x_1 - x_0)}{(y_1 - y_0)} + \\frac{y_0y_1}{(y_2 - y_0)}(\\frac{(x_2 - x_1)}{(y_2 - y_1)} - \\frac{(x_1 - x_0)}{(y_1 - y_0)}) \\tag{2.6} \\end{equation}\\] \\(x_t\\) can be considered as a good approximate of \\(x^*\\) after satisfactory number of optimizations steps of the algorithm. The algorithm for inverse quadratic interpolation method is given below: The above alogorithm looks very simple and it will be left as an exercise for the reader to implement this using Python. An objective function \\(f(x)\\) can be selected to test the results using this algorithm. Let \\[\\begin{equation} f(x) = 2 - e^x \\tag{2.7} \\end{equation}\\] One can take the initial experimental point to be \\(x=1\\), the step size to be \\(s=10^{-5}\\) and the tolerance to be \\(\\epsilon=10^{-8}\\). \\end{example} In the next section we discuss two direct root finding methods: Newtons method, Halleys method, Secant Method, and Bisection Method 3.7 Newtons Method For a given objective function \\(f(x), x\\in \\mathbb{R}\\), the necessary condition for it to contain a minimizer \\(x^*\\), is that \\(\\frac{df}{dx}(x^*)=0\\). The aim of these direct root-finding methods is thus to obtain the solution of the equation, \\[\\begin{equation} \\frac{df}{dx}(x)=0 \\tag{2.8} \\end{equation}\\] At point \\(x_j\\), the Taylors expansion of the objective function, up to the second order terms is given by, \\[\\begin{equation} f(x) = f(x_j) + (x - x_j)\\frac{df}{dx}(x_j) + \\frac{1}{2}(x - x_j)^2\\frac{d^2f}{dx^2}(x_j) \\tag{2.9} \\end{equation}\\] Now, \\[\\begin{equation} \\frac{df}{dx}(x_j) = 0 \\tag{2.10} \\end{equation}\\] so, Eq.(2.9) reduces to \\[\\begin{equation} f(x) = f(x_j) + \\frac{1}{2}(x - x_j)^2\\frac{d^2f}{dx^2}(x_j) \\tag{2.11} \\end{equation}\\] Here \\(\\frac{d^2f}{dx^2}(x_j)\\) is a constant. Now, we find the derivative of Eq.(2.11) and set it to \\(0\\) following Eq.(2.8) \\[\\begin{align} \\frac{df}{dx}(x) &amp;= 0 \\\\ \\frac{df}{dx}(x_j) + (x - x_j)\\frac{d^2f}{dx^2}(x_j) &amp;= 0 \\tag{2.12} \\end{align}\\] We get, \\[\\begin{equation} x = x_j - \\frac{\\frac{df}{dx}(x_j)}{\\frac{d^2f}{dx^2}(x_j)} \\tag{2.13} \\end{equation}\\] \\(x_j\\) denotes an approximation to the minimizer \\(x^*\\) of \\(f(x)\\). An improved approximation in the form of an iterative process can be given using Eq.(2.13), \\[\\begin{equation} x_{j+1} = x_j - \\frac{\\frac{df}{dx}(x_j)}{\\frac{d^2f}{dx^2}(x_j)} \\tag{2.14} \\end{equation}\\] The termination condition for convergence after a sufficient number of large iterations to \\(x^*\\) is given by: \\[\\begin{equation} |\\frac{df}{dx}(x_{j+1})| \\leq \\epsilon \\tag{2.15} \\end{equation}\\] where, \\(\\epsilon\\) is the tolerance set by the user for the optimization algorithm. In numerical analysis literature, the Newtons method is also sometimes called the Newton-Raphson method, because it was originally designed by Newton and was later improved by Raphson. The Newtons method has a fast convergence property called the quadratic convergence. Definition 3.2 For a twice differentiable objective function \\(f(x)\\), assuming that its minimizer \\(x^*\\) lies in the interval \\((x_l, x_r)\\), with \\(\\frac{df}{dx}(x^*) \\neq 0\\) and \\(f(x^*) = 0\\), for the Newtons iteration, if \\(x_j\\) converges to \\(x^*\\) for a large number of iterations, \\(j \\to \\infty\\), \\(x_j\\) is said to be to \\(x^*\\) if the following condition is satisfied: \\[\\begin{equation} |x_{j+1} - x^*| \\leq M|x_j - x^*|^2, \\text{ if } M &gt; \\frac{|\\frac{d^2f}{dx^2}(x^*)|}{2|\\frac{df}{dx}(x^*)|} \\tag{2.16} \\end{equation}\\] This chapter is under development "]]
