[["solving-one-dimensional-optimization-problems.html", "Chapter 3 Solving One Dimensional Optimization Problems 3.1 One Dimensional Optimization Problems 3.2 What is a Unimodal Function? 3.3 Fibonacci Search Method 3.4 Golden Section Search Method", " Chapter 3 Solving One Dimensional Optimization Problems This chapter introduces the detailed study on various algorithms for solving one dimensional optimization problems. The classes of methods that have been discussed are: Elimination method, Interpolation method and Direct Root Finding method. The Elimination method covers the Fibonacci Search method and the Golden Section Search method; the Interpolation method covers Quadratic Interpolation and Inverse Quadratic Interpolation methods; and the Direct Root Finding method covers Newtons method, Halleys method, Secant method and Bisection method. Finally a combination of some of these methods called the Brents method has also been discussed. Python programs involving the functions provided by the scipy.optimize module for solving problems using the above algorithms have also been provided. For the Fibonacci Search method and the Inverse Quadratic Interpolation method, full Python programs have been written for assisting the readers. 3.1 One Dimensional Optimization Problems The aim of this chapter is to introduce methods for solving one-dimensional optimization tasks, formulated in the following way: \\[\\begin{equation} f(x^*)=\\underset{x}{\\min\\ }f(x), x \\in \\mathbb{R} \\tag{3.1} \\end{equation}\\] where, \\(f\\) is a nonlinear function. The understanding of these optimization tasks and algorithms will be generalized in solving unconstrained optimization tasks involving objective function of multiple variables, in the future chapters. 3.2 What is a Unimodal Function? Definition 3.1 A function \\(f(x)\\), where \\(x \\in \\mathbb{R}\\) is said to be unimodal [refer to https://math.mit.edu/~rmd/465/unimod-dip-bds.pdf] if for a value \\(x^*\\) on the real line, the following conditions are satisfied: * \\(f\\) is monotonically decreasing for \\(x \\leq v\\), * \\(f\\) is monotonically increasing for \\(x \\geq v\\), and * if the above two conditions are satisfied, then \\(f(x^*)\\) is the minimum value of \\(f(x)\\), and \\(x^*\\) is the minimizer of \\(f\\). Let us have a look into the figure below. We have taken the quadratic function of one variable: \\(f(x) = 5x^2-3x+2\\). It is a nonlinear unimodal function defined over the interval \\([-2,2]\\), denoted by the dotted lines on either side.. The minimizer \\(x^*=0.3\\) (which can be solved analytically!), given by the middle dotted line, lies inside the interval \\([x_l, x_r]=[-2,2]\\). We notice that \\(f(x)\\) strictly decreases for \\(f(x) &lt; f(x^*)\\) and strictly increases for \\(f(x) &gt; f(x^*)\\). The interval \\([x_l, x_r]\\) that has the minimizer within it, is called the interval of uncertainty and the goal of an optimization algorithm is to reduce this interval as much as possible to converge towards the minimizer. A good algorithm completes the convergence very fast. In each step of this reduction of the interval, the algorithm finds a new unimodal interval following the following procedures: Choose two new points, \\(x_1 \\in [x_l, x^*]\\) and another point \\(x_2 \\in [x^*, x_r]\\) (denoted by the two filled straight lines in the figure, If \\(f(x_2) &gt; f(x_1)\\), the new interval becomes \\([x_l, x_2]\\) and \\(x_r\\) becomes \\(x_2\\), i.e, \\(x_r=x_2\\), Next pick a new \\(x_2\\), If condition in step (2) is not satisfied, we set the new interval as \\([x_1, x_r]\\) directly after step (1) and set \\(x_l=x_1\\), and Next pick a new \\(x_1\\). The given steps continue iteratively until the convergence is satisfied to a given limit of the minimizer. These class of methods is called an and we study two categories under this kind: Fibonacci Search, and Golden Section Search. Raos book Engineering Optimization [Rao, Singiresu S. Engineering optimization: theory and practice. John Wiley &amp; Sons, 2019.] also has some detailed studies on these kinds of optimization methods. 3.3 Fibonacci Search Method Instead of finding the exact minimizer \\(x^*\\) of \\(f(x)\\), the works by reducing the interval of uncertainty in every step, ultimately converging the interval, containing the minimizer, to a desired size as small as possible. One caveat is that, the initial interval containing, such that the interval lies in it, has to be known beforehand. However, the algorithm works on a nonlinear function, even if it is discontinuous. The name comes from the fact that the algorithm makes use of the famous sequence of Fibonacci numbers [http://oeis.org/A000045]. This sequence is defined in the following way: \\[\\begin{align} F_0&amp;=0,F_1=1, \\\\ F_n&amp;=F_{n-1} + F_{n-2},\\text{ where }n=2,3,\\ldots \\end{align}\\] We write a Python code to generate the first 16 Fibonacci numbers and display them as a table: import pandas as pd import numpy as np def fibonacci(n): # define the function fn = [0, 1,] for i in range(2, n+1): fn.append(fn[i-1] + fn[i-2]) return fn N = np.arange(16) data = {&#39;n&#39;: N, &#39;Fibonacci(n)&#39;: fibonacci(15)} df = pd.DataFrame(data) df looks like this: ## +----+-----+----------------+ ## | | n | Fibonacci(n) | ## |----+-----+----------------| ## | 0 | 0 | 0 | ## | 1 | 1 | 1 | ## | 2 | 2 | 1 | ## | 3 | 3 | 2 | ## | 4 | 4 | 3 | ## | 5 | 5 | 5 | ## | 6 | 6 | 8 | ## | 7 | 7 | 13 | ## | 8 | 8 | 21 | ## | 9 | 9 | 34 | ## | 10 | 10 | 55 | ## | 11 | 11 | 89 | ## | 12 | 12 | 144 | ## | 13 | 13 | 233 | ## | 14 | 14 | 377 | ## | 15 | 15 | 610 | ## +----+-----+----------------+ Let \\(n\\) be the total number of experiments to be conducted and \\([x_l, x_r]\\) be the initial interval the algorithm starts with. Let \\[\\begin{eqnarray} L_0 = x_r - x_l \\tag{3.2} \\end{eqnarray}\\] be the initial level of uncertainty and let us define, \\[\\begin{eqnarray} L_j = \\frac{F_{n-2}}{F_n}L_0 \\tag{3.3} \\end{eqnarray}\\] where, \\(F_{n-2}\\) and \\(F_n\\) are the \\((n-2)^{th}\\) and \\(n^{th}\\) Fibonacci numbers respectively. We see from the formulation of the Fibonacci numbers that, (3.3) shows the following property: \\[\\begin{equation} L_j = \\frac{F_{n-2}}{F_n}L_0 \\leq \\frac{L_0}{2} \\text{ for } n\\geq 2 \\end{equation}\\] Now, the initial two experiments are set at points \\(x_1\\) and \\(x_2\\), where, \\(L_j = x_1 - x_l\\) and \\(L_j = x_r - x_2\\). So, combining these with Eq.(3.3), we have: \\[\\begin{equation} x_1 = x_l + \\frac{F_{n-2}}{F_n}L_0 \\tag{3.4} \\end{equation}\\] and \\[\\begin{equation} x_2 = x_r - \\frac{F_{n-2}}{F_n}L_0 \\tag{3.5} \\end{equation}\\] Now taking into consideration the unimodality assumption, a part of the interval of uncertainty is rejected, shrinking it to a smaller size, given by, \\[\\begin{equation} L_i = L_0 - L_j = L_0(1-\\frac{F_{n-2}}{F_n}) = \\frac{F_{n-1}}{F_n}L_0 \\tag{3.6} \\end{equation}\\] where, we have used the fact that, \\(F_n - F_{n-2} = F_{n-1}\\) from the formulation of the Fibonacci numbers. This procedure leaves us with only one experiment, which, from one end, is situated at a distance of \\[\\begin{equation} L_j = \\frac{F_{n-2}}{F_n}L_0 = \\frac{F_{n-2}}{F_{n-1}}L_i \\tag{3.7} \\end{equation}\\] where, we have used Eq.(3.3). From the other end, the same experiment point is situated at a distance give by, \\[\\begin{equation} L_i-L_j = \\frac{F_{n-3}}{F_n}L_0 = \\frac{F_{n-3}}{F_n}L_0 = \\frac{F_{n-3}}{F_{n-1}}L_2 \\tag{3.8} \\end{equation}\\] where, we have again used Eq.(3.3). We now place a new experiment point in the interval \\(L_i\\) so that both the present experiment points are situated at a distance given by Eq.(3.7). We again reduce the size of the interval of uncertainty using the unimodality conditions. This whole process is continued so that for the \\(k^{th}\\) experiment point, its location is given by, \\[\\begin{equation} L_{k[j]} = \\frac{F_{n-k}}{F_{n-(k-2)}}L_{k-1} \\tag{3.9} \\end{equation}\\] and the interval of uncertainty is given by, \\[\\begin{equation} l_{k[i]} = \\frac{F_{n-(k-1)}}{F_n}L_0 \\tag{3.10} \\end{equation}\\] after \\(k\\) iterations are completed. Now, the given by the ratio of the present interval of uncertainty after conduction \\(k\\) iterations out of the \\(n\\) experiments to be performed, \\(L_{k[i]}\\) to the initial interval of uncertainty, \\(L_0\\): \\[\\begin{equation} R = \\frac{L_{k[i]}}{L_0} = \\frac{F_{n-(k-1)}}{F_n} \\tag{3.11} \\end{equation}\\] The purpose of this algorithm is to bring \\(R \\sim 0\\). The Fibonacci Search Algorithm has been shown below: We will write a Python function that implements the above algorithm def fib_search(f, xl, xr, n): F = fibonacci(n) # Call the fibonnaci number function L0 = xr - xl # Initial interval of uncertainty R1 = L0 # Initial Reduction Ratio Li = (F[n-2]/F[n])*L0 R = [Li/L0] for i in range(2, n+1): if Li &gt; L0/2: x1 = xr - Li x2 = xl + Li else: x1 = xl + Li x2 = xr - Li f1, f2 = f(x1), f(x2) if f1 &lt; f2: xr = x2 Li = (F[n - i]/F[n - (i - 2)])*L0 # New interval of uncertainty elif f1 &gt; f2: xl = x1 Li = (F[n - i]/F[n - (i - 2)])*L0 # New interval of uncertainty else: xl, xr = x1, x2 Li = (F[n - i]/F[n - (i - 2)])*(xr - xl) # New interval of uncertainty L0 = xr - xl R += [Li/R1,] # Append the new reduction ratio if f1 &lt;= f2: return [x1, f(x1), R] # Final result else: return [x2, f(x2), R] # Final result Example 3.1 Let an objective function be: \\[\\begin{equation} f(x) = x^5 - 5x^3 - 20x + 5 \\tag{3.12} \\end{equation}\\] We will use the Fibonacci search algorithm to find the minimizer \\(x^*\\), taking \\(n=25\\) and the initial interval of uncertainty \\([-2.5, 2.5]\\). Lets write a Python function to define the given objective function and visualize the same: def f(x): # Objective function return x**5 - 5*x**3 - 20*x + 5 x = np.linspace(-3, 3, 100) plt.plot(x, f(x), &#39;r-&#39;) plt.xlabel(&#39;x -&gt;&#39;) plt.ylabel(&#39;f(x) -&gt;&#39;) plt.title(&quot;Graph of $f(x) = x^5-5x^3-20x+5$&quot;) plt.show() Now, we consider \\(n=25\\) and use the function fib_search(f, -2.5, 2.5, 25) to run the optimization and print the results: Fib = fib_search(f, -2.5, 2.5, 25) x_star, f_x_star, R = Fib print (&quot;x*:&quot;, x_star) ## x*: 1.999966677774075 print (&quot;f(x*):&quot;, f_x_star) ## f(x*): -42.99999994448275 print (&quot;Final Reduction Ratio:&quot;, R[-1]) ## Final Reduction Ratio: 0.0 We see that \\(x^* \\sim 2\\), \\(f(x^*) \\sim -43\\) and the final Reduction Ration is 0. Now, to show the positions of \\(x^*\\) and \\(f(x^*)\\) on the graph of the objective function, we write the following code: x = np.linspace(-3, 3, 100) plt.plot(x, f(x), &#39;r-&#39;) plt.plot([x_star], [f_x_star], &#39;ko&#39;) plt.axvline(x=x_star, color=&#39;b&#39;, linestyle=&#39;--&#39;) plt.xlabel(&#39;x -&gt;&#39;) plt.ylabel(&#39;f(x) -&gt;&#39;) plt.title(&quot;$x^*$ denoted as broken blue line and $f(x^*)$ denoted as the black dot&quot;) plt.show() We can modify our function in such a way that all the optimization data in every step are collected and displayed as a DataFrame: def fib_search(f, xl, xr, n): F = fibonacci(n) L0 = xr - xl ini = L0 Li = (F[n-2]/F[n])*L0 R = [Li/L0] a = [xl] b = [xr] F1 = [f(xl)] F2 = [f(xr)] for i in range(2, n+1): #print(&quot;reduction ratio:&quot;, Li/ini) if Li &gt; L0/2: x1 = xr - Li x2 = xl + Li else: x1 = xl + Li x2 = xr - Li f1, f2 = f(x1), f(x2) if f1 &lt; f2: xr = x2 Li = (F[n - i]/F[n - (i - 2)])*L0 elif f1 &gt; f2: xl = x1 Li = (F[n - i]/F[n - (i - 2)])*L0 else: xl, xr = x1, x2 Li = (F[n - i]/F[n - (i - 2)])*(xr - xl) L0 = xr - xl R += [Li/ini,] a += [xl, ] b += [xr, ] F1 += [f1, ] F2 += [f2, ] data = {&#39;n&#39; : range(0, n), &#39;xl&#39;: a, &#39;xr&#39;: b, &#39;f(x1)&#39;: F1, &#39;f(x2)&#39;: F2, &#39;Reduction Ratio&#39;: R} df = pd.DataFrame(data, columns = [&#39;n&#39;, &#39;xl&#39;, &#39;xr&#39;, &#39;f(x1)&#39;, &#39;f(x2)&#39;, &#39;Reduction Ratio&#39;]) return df df = fib_search(f, -2.5, 2.5, 25) Where df looks like this: ## +----+-----+----------+---------+-----------+-----------+-------------------+ ## | | n | xl | xr | f(x1) | f(x2) | Reduction Ratio | ## |----+-----+----------+---------+-----------+-----------+-------------------| ## | 0 | 0 | -2.5 | 2.5 | 35.4688 | -25.4688 | 0.381966 | ## | 1 | 1 | -0.59017 | 2.5 | 17.7596 | -7.75959 | 0.381966 | ## | 2 | 2 | 0.59017 | 2.5 | -7.75959 | -28.8819 | 0.236068 | ## | 3 | 3 | 1.31966 | 2.5 | -28.8819 | -40.7626 | 0.145898 | ## | 4 | 4 | 1.77051 | 2.5 | -40.7626 | -42.875 | 0.0901699 | ## | 5 | 5 | 1.77051 | 2.22136 | -42.875 | -40.1458 | 0.0557281 | ## | 6 | 6 | 1.94272 | 2.22136 | -42.8424 | -42.875 | 0.0344419 | ## | 7 | 7 | 1.94272 | 2.11493 | -42.875 | -42.2847 | 0.0212862 | ## | 8 | 8 | 1.94272 | 2.04915 | -42.9964 | -42.875 | 0.0131556 | ## | 9 | 9 | 1.98337 | 2.04915 | -42.9863 | -42.9964 | 0.00813062 | ## | 10 | 10 | 1.98337 | 2.02403 | -42.9964 | -42.9707 | 0.00502499 | ## | 11 | 11 | 1.98337 | 2.0085 | -42.9999 | -42.9964 | 0.00310563 | ## | 12 | 12 | 1.99297 | 2.0085 | -42.9975 | -42.9999 | 0.00191936 | ## | 13 | 13 | 1.99297 | 2.00257 | -42.9999 | -42.9997 | 0.00118627 | ## | 14 | 14 | 1.99663 | 2.00257 | -42.9994 | -42.9999 | 0.000733089 | ## | 15 | 15 | 1.9989 | 2.00257 | -42.9999 | -43 | 0.000453182 | ## | 16 | 16 | 1.9989 | 2.00117 | -43 | -42.9999 | 0.000279907 | ## | 17 | 17 | 1.9989 | 2.0003 | -43 | -43 | 0.000173276 | ## | 18 | 18 | 1.99943 | 2.0003 | -43 | -43 | 0.000106631 | ## | 19 | 19 | 1.99977 | 2.0003 | -43 | -43 | 6.66445e-05 | ## | 20 | 20 | 1.99977 | 2.0001 | -43 | -43 | 3.99867e-05 | ## | 21 | 21 | 1.9999 | 2.0001 | -43 | -43 | 2.66578e-05 | ## | 22 | 22 | 1.9999 | 2.00003 | -43 | -43 | 1.33289e-05 | ## | 23 | 23 | 1.99997 | 1.99997 | -43 | -43 | 0 | ## | 24 | 24 | 1.99997 | 1.99997 | -43 | -43 | 0 | ## +----+-----+----------+---------+-----------+-----------+-------------------+ The graph of the reduction ratio at each \\(n\\) can be plotted with the following code: plt.xlabel(&quot;n-&gt;&quot;) plt.ylabel(&quot;Reduction Ratio -&gt;&quot;) plt.plot(range(0, len(df)), df[&#39;Reduction Ratio&#39;]) plt.title(&quot;Graph of change of reduction ratio at each $n$&quot;) plt.show() 3.4 Golden Section Search Method The golden section search method is a modified version of the Fibonacci search method. One advantage of the former over the later is that, we do not need to keep a record of the total number of experiment points \\(n\\) beforehand. While selecting \\(x_1\\) and \\(x_2\\) inside the interval of uncertainty, we make use of the golden ratio, \\(\\phi = \\frac{\\sqrt{5} - 1}{2}\\) which is the positive root of the quadratic equation given by: \\[\\begin{equation} \\phi^2+\\phi-1=0 \\tag{3.13} \\end{equation}\\] Given the initial interval \\([x_l, x_r]\\), we have the initial interval of uncertainty as, \\[\\begin{equation} L_0 = x_r - x_l \\tag{3.14} \\end{equation}\\] The new interior points \\(x_1\\) and \\(x_2\\) are chosen in such a way that both of them lies at a distance \\(\\phi^2L_0\\) from either side, i.e, \\[\\begin{equation} x_1-x_l = \\phi^2L_0 \\tag{3.15} \\end{equation}\\] and \\[\\begin{equation} x_r-x_2 = \\phi^2L_0 \\tag{3.16} \\end{equation}\\] Now from Eq.(3.13) we know \\[\\begin{equation} L_0 = (\\phi^2 + \\phi)L_0 \\tag{3.17} \\end{equation}\\] The above computations leave us with: \\[\\begin{equation} x_r - x_1 = \\phi L_0 \\tag{3.18} \\end{equation}\\] and \\[\\begin{equation} x_2 - x_l = \\phi L_0 \\tag{3.19} \\end{equation}\\] Given, \\(f(x)\\) is the nonlinear objective function, we now check whether \\(f(x_1) &gt; f(x_2)\\). If this is the case, we set \\(x_l = x_1\\), otherwise if \\(f(x_1) &gt; f(x_2)\\), we set \\(x_r=x_2\\). The new interval of uncertainty is set to be \\(L_i = \\phi L_0\\) and the previous interval is shrunk. This process of choosing new experimental points and shrinking the interval of uncertainty is continued until the termination condition is satisfied. The termination condition is to check whether the interval of uncertainty is less than a particular tolerance \\(\\epsilon\\) usually provided by the user. The golden section search algorithm is given below: Example 3.2 Let us consider an objective function: \\[\\begin{equation} f(x) = \\frac{1}{16}x^3 - \\frac{27}{4}x \\tag{3.20} \\end{equation}\\] We will use the golden section search method to find the minimizer \\(x^*\\) of this function and compute \\(f(x^*)\\). Suppose the initial interval be \\([-10, 10]\\) and the tolerance for the termination condition for the algorithm be \\(\\epsilon = 10^{-5}\\). Let us first define the function in Python: def f(x): # objective function return x**3/16 - 27*x/4 The graph of the objective function: x = np.linspace(-20, 20, 100) plt.plot(x, f(x), &#39;r-&#39;) plt.xlabel(&#39;x -&gt;&#39;) plt.ylabel(&#39;f(x) -&gt;&#39;) plt.title(&quot;Graph of $f(x) = \\\\frac{1}{16}x^3 - \\\\frac{27}{4}x$&quot;) plt.show() For tackling this problem, we will not write our own Python function. As already stated in the last chapter, the scipy.optimize package too equips us with solvers to solve these tasks. For this problem, we use the minimize_scalar() function provided by scipy.optimize, which is used for minimization of a scalar function of one variable. The minimize_scalar() function provides the user with the following parameter: fun: The objective function which must be callable, bracket: This is an optional parameter and defines the bracketing interval. This is a sequence, and consists of either three points \\((x_a, x_b, x_c)\\), such that \\(x_a &lt; x_b &lt; x_c\\) and \\(f(x_b) &lt; f(x_a), f(x_c)\\), or two points \\((x_a, x_b)\\) that are considered as the starting interval for any elimination search method, bounds: This is an optional parameter too (important for our analysis!) and is a sequence. This defines the optimization bound, i.e, the initial interval of uncertainty, \\([x_l, x_r]\\), args: This is a tuple and an optional parameter that defines the extra arguments that might be needed to pass to the objective function, method: This is the most important parameter that defines the various solvers provided by minimize_scalar(). This should be either a string (str) or a callable object. As of writing this book, the solvers that minimize_scalar() provides are: 'golden': Uses the golden section search method for finding the local minimum, 'brent': Uses the Brents algorithm (will be discussed in the next section) for finding the local minimum, 'bounded': For performing bounded minimization and uses the Brent's algorithm to find the local minima specified in the 'bounds' parameter. The method parameter is optional too, and if not provided, the minimize_scalar() function uses the 'brent' method by default. The user can also write and pass a custom solver which must be a Python callable object. tol: This parameter represents the tolerance (\\(\\epsilon\\)) of the optimization algorithm. This must be a float and is optional too, and options: This is an optional parameter and is a Python dictionary which specifies the solver options: maxiter: This is an int object and denotes the maximum number of iterations to be performed by the solver, disp: This must be a boolean (bool) object, and if set to True, prints a detailed information about the convergence of the algorithm The minimize_scalar() function returns the optimization result as a specific Python object designed specifically for the scipy.optimize module called, OptimizeResult. It has the following important attributes: x: The solution (\\(x^*\\)) of the optimization. This is a numpy array object, i.e, ndarray and can return a scalar or a vector, success: This is a bool object and states whether the optimization process has completed successfully or not, fun, jac, hess: Provides the objective function, Jacobian and the Hessian matrix at the solution \\(x^*\\) as objects, nfev, njev, nhev: Provides the number of evaluations of the objective function, its Jacobian and Hessian matrix during the running of the optimization solver and are int objects, This is an int object and states the number of iterations that have been performed by the solver, and maxcv: This is a object and represents the maximum constraint evaluation. "]]
