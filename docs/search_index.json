[["line-search-descent-methods.html", "Chapter 4 Line Search Descent Methods 4.1 Introduction to Line Search Descent Methods for Unconstrained Minimization 4.2 Selection of Step Length 4.3 First Order Line Search Gradient Descent Method: The Steepest Descent Algorithm", " Chapter 4 Line Search Descent Methods This chapter starts with an outline of a simple line-search descent algorithm, before introducing the Wolfe conditions and how to use them to design an algorithm for selecting a step length at a chosen descent direction at each step of the line search algorithms. Then a first order line search descent algorithm called the Steepest Descent Algorithm and a second order line search descent algorithm, called the Modified Newton Method have been discussed. Examples, Python programs and proofs accompanying each section of the chapter have been provided, wherever required. Finally, before ending the chapter with Marquardt method, motivations to study Conjugate Gradient Methods and Quasi-Newton Methods have been explored, which will be introduced in detailed manner in the next and later chapters. 4.1 Introduction to Line Search Descent Methods for Unconstrained Minimization In the line search descent methods, the optimization technique picks a direction \\(\\mathbb{\\delta_j}\\) to begin with, for the \\(j^{th}\\) step and carries out a search along this direction from the previous experimental point, to generate a new iterate. The iterative process looks like: \\[\\begin{equation} \\mathbb{x}_j = \\mathbb{x}_{j-1}+\\beta_{j}\\mathbb{\\delta}_j, \\mathbb{x} \\in \\mathbb{R}^n \\tag{4.1} \\end{equation}\\] Here, \\(\\beta_j\\) is a positive scalar number at the \\(j^{th}\\) step , called the step length. The performance of a line search descent algorithm depends on the selection of both the step length \\(\\beta_j\\) and the descent direction \\(\\mathbb{\\delta}_j\\). The condition for selecting the direction \\(\\mathbb{\\delta}_j\\)for the next iterate : \\[\\begin{equation} \\nabla^T f(\\mathbb{x}_{j-1})\\mathbb{\\delta}_j &lt; 0 \\tag{4.2} \\end{equation}\\] i.e, the directional derivative in the direction \\(\\mathbb{\\delta}_j\\) should be negative. The step length \\(\\beta_j\\) is computed by solving the one dimensional optimization problem formulated as: \\[\\begin{equation} \\underset{\\beta_j &gt; 0}{\\min} \\tilde{f}(\\beta_j) = \\underset{\\beta_j &gt; 0}{\\min} f(\\mathbb{x}_{j-1} + \\beta_j \\mathbb{\\delta}_j) \\tag{4.3} \\end{equation}\\] The general algorithm for a line search descent method is given below: 4.2 Selection of Step Length While finding a suitable step length \\(\\beta_j\\) at the \\(j^{th}\\) iteration, we should keep in mind that the choice should be such that there is an acceptable reduction in the objective function value. We work towards solving a minimization task formulated as: \\[\\begin{equation} \\tilde{f}(\\beta) = f(\\mathbb{x}_{j-1} + \\beta\\mathbb{\\delta}_j),\\ \\ \\beta &gt; 0 \\tag{4.4} \\end{equation}\\] The algorithm should be designed in such a way that too many computations of the objective function and its gradient should be avoided. This can be achieved by performing inexact line searches, to compute the local minimizer of \\(\\tilde{f}\\). As we discussed earlier, there should be a condition for choosing \\(\\beta_j\\) at each iterate. The condition: \\[\\begin{equation} f(\\mathbb{x}_j) &gt; f(\\mathbb{x}_{j-1} + \\beta_j\\mathbb{\\delta}_j) \\tag{4.5} \\end{equation}\\] does not suffice alone. We need to have a sufficient decrease condition known as the . 4.2.1 The Wolfe Conditions The step length \\(\\beta_j\\), chosen at each iteration, must result in a in the objective function \\(f(\\mathbb{x})\\) given by: \\[\\begin{equation} f(\\mathbb{x}_{j-1} +\\beta_j\\mathbb{\\delta}_j) \\leq f(\\mathbb{x}_j) + \\alpha_1 \\beta_j\\nabla^Tf(\\mathbb{x}_j)\\mathbb{\\delta}_j,\\ \\ \\ \\ \\alpha_1 \\in (0, 1) \\tag{4.6} \\end{equation}\\] This is also called the Armijo condition. Practically, the value of \\(\\alpha_1\\) should be very small, for example in the order of \\(10^{-4}\\). But the Armijo condition itself is not enough to guarantee a reasonable progress in the algorithm. To avoid unacceptably short step lengths, there is another condition given by: \\[\\begin{equation} \\nabla^Tf(\\mathbb{x}_{j-1} + \\beta_j\\mathbb{\\delta}_j)\\mathbb{\\delta}_j \\geq \\alpha_2\\nabla^Tf(\\mathbb{x}_{j-1})\\mathbb{\\delta}_j,\\ \\ \\ \\ \\alpha_2 \\in (\\alpha_1, 1) \\tag{4.7} \\end{equation}\\] This is also called the curvature condition. Practically \\(\\alpha_2\\) is chosen between \\(0.1 - 0.9\\) depending on the algorithms we consider. Eq. (4.6) and Eq. (4.7) together form the Wolfe conditions. Further more, the curvature condition can be modified to steer away from cases where a step length might satisfy the Wolfe conditions without being close to the minimizer of \\(\\tilde{f}(\\beta)\\). The modified version of Eq. (4.7) can be written as: \\[\\begin{equation} |\\nabla^Tf(\\mathbb{x}_{j-1} + \\beta_j\\mathbb{\\delta}_j)\\mathbb{\\delta}_j| \\leq \\alpha_2|\\nabla^Tf(\\mathbb{x}_{j-1})\\mathbb{\\delta}_j| \\tag{4.8} \\end{equation}\\] So, Eq. (4.6) and Eq. (4.7) together form the . For the the term \\(\\nabla^Tf(\\mathbb{x}_{j-1} + \\beta_j\\mathbb{\\delta}_j)\\mathbb{\\delta}_j\\) is no longer ``too positive unlike the case for the . 4.2.2 An Algorithm for the Strong Wolfe Conditions Before moving on to the line search algorithm for the strong wolfe conditions, we discuss a straightforward algorithm called zoom which takes in two values \\(\\beta_l\\) and \\(\\beta_r\\) that bounds the interval \\([\\beta_l, \\beta_r]\\) containing the step lengths that satisfy the strong Wolfe conditions. This algorithm has been originally introduced in the classic book by Nocedal and Wright. The purpose of this algorithm is to generate an iterate \\(\\beta_j \\in [\\beta_l, \\beta_r]\\) at each step and replaces either \\(\\beta_l\\) or \\(\\beta_r\\) with \\(\\beta_j\\) in such a way that the following properties are maintained: The step length \\(\\beta_j\\) satisfying the lies in the interval \\([\\beta_l, \\beta_r]\\), \\(\\beta_l\\) is the step length which after a particular iteration gives the lowest function value besides satisfying the Armijo conditions, \\(\\beta_r\\) is chosen such that the following condition is satisfied: \\[\\begin{equation} (\\beta_r -\\beta_l)[\\nabla^Tf(\\mathbb{x}_{j-1} + \\beta_l\\mathbb{\\delta}_j)\\mathbb{\\delta}_j] &lt; 0 \\tag{4.9} \\end{equation}\\] The algorithm for zoom is given below: Now, we describe the algorithm for finding an optimized step length \\(\\beta_j\\) at the \\(j^{th}\\) iterate in a line search algorithm, solving the minimization task formulated by Eq. (4.3). The step-length selection algorithm satisfying the strong Wolfe conditions is given below: The first part of the above algorithm, starts with a trial estimate of the step length and keeps on increasing it at each step until it finds either an acceptable length or an interval bracketing the optimized step lengths. The zoom() function , given by Algorithm 11, is called in the second part which reduces the size of this interval until the optimized step length is reached. Example 4.1 Let us consider the Himmelblaus function as the objective function, given by, \\[\\begin{equation} f(x_1, x_2) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2 \\tag{4.10} \\end{equation}\\] Let the starting point be \\(\\mathbb{x}=\\begin{bmatrix}-2.5 \\\\ 2.8 \\end{bmatrix}\\), the descent direction be \\(\\mathbb{\\delta}=\\begin{bmatrix}-2.5 \\\\ -1 \\end{bmatrix}\\), \\(\\alpha_1\\) and \\(\\alpha_2\\) be \\(10^{-4}\\) and \\(0.325\\) respectively, and the upper bound be \\(\\beta_{max}=0.6\\). For this descent direction, we will compute the step length for generating the next iterate from the starting point using Algorithm 11. The step-length selection algorithm solves the optimization problem given by Eq. (4.4) for the parameters provided in this example. Let us first define the objective function and its gradient using Python. # First let us import all the necessary packages import matplotlib.pyplot as plt import numpy as np import autograd.numpy as au from autograd import grad, jacobian import scipy def himm(x): # Objective function return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2 grad_himm = grad(himm) # Gradient of the objective function We will now plot the objective function. x = np.linspace(-6, 6, 100) y = np.linspace(-6, 6, 100) z = np.zeros(([len(x), len(y)])) for i in range(0, len(x)): for j in range(0, len(y)): z[j, i] = himm([x[i], y[j]]) contours=plt.contour(x, y, z, 100, cmap=plt.cm.gnuplot) plt.clabel(contours, inline=1, fontsize=10) plt.show() We will use the line_search() function from the scipy.optimize module which is a Python implementation of the step-length selection algorithm. The attributes for the line_search() function are: f: This is the objective function, which is a callable datatype, myfprime: This is the gradient of the objective function and is callable, xk: This is the starting iterate, given by an ndarray datatype, pk: This is the descent direction given by \\(\\mathbb{\\delta}\\), used for generating the next iterate point from a given starting point. This is also given by an ndarray datatype, c1: This is the \\(\\alpha_1\\) value from the Armijo condition given by Eq. (4.6). This is an optional datatype and is a float, c2: This is the \\(\\alpha_2\\) value from the curvature condition given by Eq. (4.7). This is an optional datatype and is a float, amax: This is the upper bound set for the step lengths, given by \\(\\beta_{max}\\) in step-length selection algorithm. This is an optional datatype and is a float. There are other attributes that one can pass to the function, but they are less important but can provide with more flexibilities if provided. These are: gfk: Gives the gradient value at the current iterate point, \\(\\mathbb{x}_j\\). This is an optional parameter and is a float, old_fval: This gives the function value at the current iterate point, \\(\\mathbb{x}_j\\). The parameter is optional and is a float, old_old_fval: This gives the function value at the point preceding the current iterate point,i.e, \\(\\mathbb{x}_{j-1}\\). This is optional and is a float, args: These are the additional arguments that might be passed to the objective function. This is optional and is a Python tuple, maxiter: This is the maximum number of iterations that are needed to be performed by the optimization algorithm. This is optional too and is an int datatype, and extra_condition: This is a callable function having the following form: extra_condition(beta, current_iterate, function, gradient). The step length beta is accepted if only this function returns True, otherwise the algorithm continues with the new iterates. This callable function is only invoked for those iterates which satisfy the strong Wolfe conditions. The scipy.optimize.line_search() method returns the following: The optimized step length \\(\\beta\\) solving Eq. (4.3) for the next iterate from the current iterate. This will either be a float or a None, The number of function evaluations made. This is an int, The number of gradient evaluations made. This is an int, The function value given by Eq. (4.4) with the computed step length. This will be a float if the algorithm converges, otherwise this will be a None, The function value at the starting point, the algorithm starts with. This is a float too, and The local slope along the descent direction at the new value. This will be a float if the algorithm converges, otherwise, this will be a None. Now, for our example, we enter the values of the starting point, the descent direction, the constants \\(\\alpha_1\\) and \\(\\alpha_2\\) and the upper bound on the step lengths for the scipy.optimize.line_search() and print the results. from scipy.optimize import line_search start_point = np.array([-2.5, 2.8]) delta = np.array([-2.5, -1]) alpha_1 = 10**-4 alpha_2 = 0.325 beta_max = 0.6 res=line_search(f = himm, myfprime = grad_himm, xk = start_point, pk = delta, c1 = alpha_1, c2 = alpha_2, amax = beta_max) res ## (0.04234665754870197, 4, 1, 6.112599989468139, 6.5581000000000005, array([ 11.13041873, -24.97823686])) We see that the optimized step length is \\(\\sim 0.04\\), the number of function evaluations made is \\(4\\), the number of gradient evaluations made is \\(1\\), the function value at the new step length is \\(\\tilde{f(\\beta)}\\sim 6.11\\), the function value at the starting point is \\(\\sim 6.56\\) and the local slope along the descent direction is \\(\\sim \\begin{bmatrix} 11.13 \\\\ -25 \\end{bmatrix}\\). 4.3 First Order Line Search Gradient Descent Method: The Steepest Descent Algorithm This chapter is under development "]]
