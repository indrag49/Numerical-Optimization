[["index.html", "Introduction to Mathematical Optimization with Python Chapter 1 What is Numerical Optimization? 1.1 Introduction to Optimization 1.2 A Solution 1.3 Maximization", " Introduction to Mathematical Optimization with Python Indranil Ghosh 2021-05-17 Chapter 1 What is Numerical Optimization? This chapter gives an introduction to the basics of numerical optimization and will help build the tools required for our in-depth understanding in the later chapters. Some fundamental linear algebra concepts will be touched which will be required for further studies in optimization along with introduction to simple Python codes. 1.1 Introduction to Optimization Let \\(f(\\mathbf{x})\\) be a scalar function of a vector of variables \\(\\mathbf{x} = \\begin{pmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\in \\mathbb{R}^n\\). Numerical Optimization is the minimization or maximization of this function \\(f\\) subject to constraints on \\(\\mathbf{x}\\). This \\(f\\) is a scalar function of \\(\\mathbf{x}\\), also known as the objective function and the continuous components \\(x_i \\in \\mathbf{x}\\) are called the decision variables. The optimization problem is formulated in the following way: \\[\\begin{align} &amp;\\!\\min_{\\mathbf{x} \\in \\mathbb{R}^n} &amp;\\qquad&amp; f(\\mathbf{x}) \\\\ &amp;\\text{subject to} &amp; &amp; g_k(\\mathbf{x}) \\leq 0,\\ k=1,2,..., m\\\\ &amp; &amp; &amp; h_k(\\mathbf{x}) = 0,\\ k=1,2,..., r\\\\ &amp; &amp; &amp; m,r &lt; n.\\tag{1.1} \\end{align}\\] Here, \\(g_k(\\mathbf{x})\\) and \\(h_k(\\mathbf{x})\\) are scalar functions too (like \\(f(\\mathbf{x})\\)) and are called constraint functions. The constraint functions define some specific equations and/or inequalities that \\(\\mathbf{x}\\) should satisfy. 1.2 A Solution Definition. A solution of \\(f(\\mathbf{x})\\) is a point \\(\\mathbf{x^*}\\) which denotes the optimum vector that solves equation (1.1), corresponding to the optimum value \\(f(\\mathbf{x^*})\\). In case of a minimization problem, the optimum vector \\(\\mathbf{x^*}\\) is referred to as the global minimizer of \\(f\\), and \\(f\\) attains the least possible value at \\(\\mathbf{x^*}\\). To design an algorithm that finds out the global minimizer for a function is quite difficult, as in most cases we do not have the idea of the overall shape of \\(f\\). Mostly our knowledge is restricted to a local portion of \\(f\\). Definition. A point \\(\\mathbf{x^*}\\) is called a global minimizer of \\(f\\) if \\(f(\\mathbf{x^*}) \\leq f(\\mathbf{x}) \\forall\\ x\\). Definition. A point \\(\\mathbf{x^*}\\) is called a local minimizer of \\(f\\) if there is a neighborhood \\(\\mathcal{N}\\) of \\(\\mathbf{x^*}\\) such that \\(f(\\mathbf{x^*}) \\leq f(\\mathbf{x}) \\forall\\ x \\in \\mathcal{N}\\). Definition. A point \\(\\mathbf{x^*}\\) is called a strong local minimizer of \\(f\\) if there is a neighborhood \\(\\mathcal{N}\\) of \\(\\mathbf{x^*}\\) such that \\(f(\\mathbf{x^*}) &lt; f(\\mathbf{x}) \\forall\\ x \\in \\mathcal{N}\\), with \\(\\mathbf{x} \\neq \\mathbf{x}^*\\). Definition. For an objective function \\(f(\\mathbf{x})\\) where, \\(\\mathbf{x} \\in \\mathbb{R}^2\\), a point \\(\\mathbf{x}^s=\\begin{bmatrix} x_1^s \\\\ x_2^s \\end{bmatrix}\\) is called a saddle point if \\(\\forall\\ \\mathbf{x}\\), there exists an \\(\\epsilon&gt;0\\), such that the following conditions are satisfied: \\(\\frac{\\partial f}{\\partial x_1}(\\mathbf{x}) \\mid_{(x_1^s, x_2^s)} &lt; \\epsilon\\), \\(\\frac{\\partial f}{\\partial x_2}(\\mathbf{x}) \\mid_{(x_1^s, x_2^s)} &lt; \\epsilon\\), and \\([\\frac{\\partial^2 f}{\\partial x_1^2}(\\mathbf{x}) \\frac{\\partial^2 f}{\\partial x_2^2}(\\mathbf{x}) - (\\frac{\\partial^2f}{\\partial x_1 \\partial x_2}(\\mathbf{x}))^2]\\mid_{(x_1^s, x_2^s)} &lt; 0\\) generating the following chain of inequalities: \\(f(\\mathbf{x})\\mid_{(x_1, x_2^s)} \\leq f(\\mathbf{x})\\mid_{(x_1^s, x_2^s)} \\leq f(\\mathbf{x})\\mid_{(x_1^s, x_2)}\\). An example of a saddle point is shown below: 1.3 Maximization We just defined a problem as our optimization task. We could do the same with a problem with little tweaks. The problem \\(\\underset{\\mathbf{x} \\in \\mathbb{R}^n}{max} f(\\mathbf{x})\\) can be formulated as: \\[\\begin{equation} \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{max} f(\\mathbf{x}) = - \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{min}\\{- f(\\mathbf{x})\\} \\tag{1.2} \\end{equation}\\] We then apply any minimization technique after setting \\(\\hat{f}(\\mathbf{x}) = - f(\\mathbf{x})\\). Further, for the inequality constraints for the maximization problem, given by \\(g_k(\\mathbf{x}) \\geq 0\\), we set \\[\\begin{equation} \\hat{g}_k(\\mathbf{x})=-g_k(\\mathbf{x}) \\tag{1.3} \\end{equation}\\] The problem thus has become, \\[\\begin{align} &amp;\\!\\min_{\\mathbf{x} \\in \\mathbb{R}^n} &amp;\\qquad&amp; \\hat{f}(\\mathbf{x})\\\\ &amp;\\text{subject to} &amp; &amp; \\hat{g}_k(\\mathbf{x}) \\leq 0,\\ k=1,2,..., m\\\\ &amp; &amp; &amp; h_k(\\mathbf{x}) = 0,\\ k=1,2,..., r\\\\ &amp; &amp; &amp; m,r &lt; n.\\tag{1.4} \\end{align}\\] After the solution \\(\\mathbf{x^*}\\) is computed, the maximum value of the problem is given by: \\(-\\hat{f}(\\mathbf{x^*})\\). "]]
