<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Solving One Dimensional Optimization Problems | Introduction to Mathematical Optimization</title>
  <meta name="description" content="A book for teaching introductory numerical optimization algorithms with Python" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Solving One Dimensional Optimization Problems | Introduction to Mathematical Optimization" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://indrag49.github.io/Numerical-Optimization/" />
  
  <meta property="og:description" content="A book for teaching introductory numerical optimization algorithms with Python" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Solving One Dimensional Optimization Problems | Introduction to Mathematical Optimization" />
  
  <meta name="twitter:description" content="A book for teaching introductory numerical optimization algorithms with Python" />
  

<meta name="author" content="Indranil Ghosh" />


<meta name="date" content="2021-07-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction-to-unconstrained-optimization.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Mathematical Optimization</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> What is Numerical Optimization?</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction-to-optimization"><i class="fa fa-check"></i><b>1.1</b> Introduction to Optimization</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#a-solution"><i class="fa fa-check"></i><b>1.2</b> A Solution</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#maximization"><i class="fa fa-check"></i><b>1.3</b> Maximization</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#feasible-region"><i class="fa fa-check"></i><b>1.4</b> Feasible Region</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#discrete-optimization-problems"><i class="fa fa-check"></i><b>1.5</b> Discrete Optimization Problems</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#linear-programming-problems"><i class="fa fa-check"></i><b>1.6</b> Linear Programming Problems</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#stochastic-optimization-problems"><i class="fa fa-check"></i><b>1.7</b> Stochastic Optimization Problems</a></li>
<li class="chapter" data-level="1.8" data-path="index.html"><a href="index.html#scaling-of-decision-variables"><i class="fa fa-check"></i><b>1.8</b> Scaling of Decision Variables</a></li>
<li class="chapter" data-level="1.9" data-path="index.html"><a href="index.html#gradient-vector-and-hessian-matrix-of-the-objective-function"><i class="fa fa-check"></i><b>1.9</b> Gradient Vector and Hessian Matrix of the Objective Function</a></li>
<li class="chapter" data-level="1.10" data-path="index.html"><a href="index.html#directional-derivative-of-the-objective-function"><i class="fa fa-check"></i><b>1.10</b> Directional Derivative of the Objective Function</a></li>
<li class="chapter" data-level="1.11" data-path="index.html"><a href="index.html#positive-definite-and-positive-semi-definite-matrices"><i class="fa fa-check"></i><b>1.11</b> Positive Definite and Positive Semi-definite Matrices</a></li>
<li class="chapter" data-level="1.12" data-path="index.html"><a href="index.html#what-is-convexity"><i class="fa fa-check"></i><b>1.12</b> What is Convexity?</a></li>
<li class="chapter" data-level="1.13" data-path="index.html"><a href="index.html#numerical-optimization-algorithms"><i class="fa fa-check"></i><b>1.13</b> Numerical Optimization Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html"><i class="fa fa-check"></i><b>2</b> Introduction to Unconstrained Optimization</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#the-unconstrained-optimization-problem"><i class="fa fa-check"></i><b>2.1</b> The Unconstrained Optimization Problem</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#smooth-functions"><i class="fa fa-check"></i><b>2.2</b> Smooth Functions</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#taylors-theorem"><i class="fa fa-check"></i><b>2.3</b> Taylorâ€™s Theorem</a></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#necessary-and-sufficient-conditions-for-local-minimizer-in-unconstrained-optimization"><i class="fa fa-check"></i><b>2.4</b> Necessary and Sufficient Conditions for Local Minimizer in Unconstrained Optimization</a><ul>
<li class="chapter" data-level="2.4.1" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#first-order-necessary-condition"><i class="fa fa-check"></i><b>2.4.1</b> First-Order Necessary Condition</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#second-order-necessary-conditions"><i class="fa fa-check"></i><b>2.4.2</b> Second-Order Necessary Conditions</a></li>
<li class="chapter" data-level="2.4.3" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#second-order-sufficient-conditions"><i class="fa fa-check"></i><b>2.4.3</b> Second-Order Sufficient Conditions</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#algorithms-for-solving-unconstrained-minimization-tasks"><i class="fa fa-check"></i><b>2.5</b> Algorithms for Solving Unconstrained Minimization Tasks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html"><i class="fa fa-check"></i><b>3</b> Solving One Dimensional Optimization Problems</a><ul>
<li class="chapter" data-level="3.1" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#one-dimensional-optimization-problems"><i class="fa fa-check"></i><b>3.1</b> One Dimensional Optimization Problems</a></li>
<li class="chapter" data-level="3.2" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#what-is-a-unimodal-function"><i class="fa fa-check"></i><b>3.2</b> What is a Unimodal Function?</a></li>
<li class="chapter" data-level="3.3" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#fibonacci-search-method"><i class="fa fa-check"></i><b>3.3</b> Fibonacci Search Method</a></li>
<li class="chapter" data-level="3.4" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#golden-section-search-method"><i class="fa fa-check"></i><b>3.4</b> Golden Section Search Method</a></li>
<li class="chapter" data-level="3.5" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#powells-quadratic-interpolation-method"><i class="fa fa-check"></i><b>3.5</b> Powellâ€™s Quadratic Interpolation Method</a></li>
<li class="chapter" data-level="3.6" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#inverse-quadratic-interpolation-method"><i class="fa fa-check"></i><b>3.6</b> Inverse Quadratic Interpolation Method</a></li>
<li class="chapter" data-level="3.7" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#newtons-method"><i class="fa fa-check"></i><b>3.7</b> Newtonâ€™s Method</a></li>
<li class="chapter" data-level="3.8" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#halleys-method"><i class="fa fa-check"></i><b>3.8</b> Halleyâ€™s Method</a></li>
<li class="chapter" data-level="3.9" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#secant-method"><i class="fa fa-check"></i><b>3.9</b> Secant Method</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Mathematical Optimization</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="solving-one-dimensional-optimization-problems" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Solving One Dimensional Optimization Problems</h1>
<p>This chapter introduces the detailed study on various algorithms for solving one dimensional optimization problems. The classes of methods that have been discussed are: <strong>Elimination method</strong>, <strong>Interpolation method</strong> and <strong>Direct Root Finding method</strong>. The <strong>Elimination method</strong> covers the <strong>Fibonacci Search method</strong> and the <strong>Golden Section Search method</strong>; the Interpolation method covers <strong>Quadratic Interpolation</strong> and <strong>Inverse Quadratic Interpolation</strong> methods; and the <strong>Direct Root Finding method</strong> covers <strong>Newtonâ€™s method</strong>, <strong>Halleyâ€™s method</strong>, <strong>Secant method</strong> and <strong>Bisection method</strong>. Finally a combination of some of these methods called the <strong>Brentâ€™s method</strong> has also been discussed. Python programs involving the functions provided by the <code>scipy.optimize</code> module for solving problems using the above algorithms have also been provided. For the Fibonacci Search method and the Inverse Quadratic Interpolation method, full Python programs have been written for assisting the readers.</p>
<hr />
<div id="one-dimensional-optimization-problems" class="section level2">
<h2><span class="header-section-number">3.1</span> One Dimensional Optimization Problems</h2>
<p>The aim of this chapter is to introduce methods for solving one-dimensional optimization tasks, formulated in the following way:
<span class="math display" id="eq:1">\[\begin{equation}
    f(x^*)=\underset{x}{\min\ }f(x), x \in \mathbb{R} \tag{3.1}
\end{equation}\]</span>
where, <span class="math inline">\(f\)</span> is a nonlinear function. The understanding of these optimization tasks and algorithms will be generalized in solving unconstrained optimization tasks involving objective function of multiple variables, in the future chapters.</p>
</div>
<div id="what-is-a-unimodal-function" class="section level2">
<h2><span class="header-section-number">3.2</span> What is a Unimodal Function?</h2>

<div class="definition">
<span id="def:unnamed-chunk-1" class="definition"><strong>Definition 3.1  </strong></span>A function <span class="math inline">\(f(x)\)</span>, where <span class="math inline">\(x \in \mathbb{R}\)</span> is said to be <em>unimodal</em> [refer to <a href="https://math.mit.edu/~rmd/465/unimod-dip-bds.pdf" class="uri">https://math.mit.edu/~rmd/465/unimod-dip-bds.pdf</a>] if for a value <span class="math inline">\(x^*\)</span> on the real line, the following conditions are satisfied:
* <span class="math inline">\(f\)</span> is monotonically decreasing for <span class="math inline">\(x \leq v\)</span>,
* <span class="math inline">\(f\)</span> is monotonically increasing for <span class="math inline">\(x \geq v\)</span>, and
* if the above two conditions are satisfied, then <span class="math inline">\(f(x^*)\)</span> is the minimum value of <span class="math inline">\(f(x)\)</span>, and <span class="math inline">\(x^*\)</span> is the minimizer of <span class="math inline">\(f\)</span>.
</div>

<p>Let us have a look into the figure below.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>We have taken the quadratic function of one variable: <span class="math inline">\(f(x) = 5x^2-3x+2\)</span>. It is a nonlinear unimodal function defined over the interval <span class="math inline">\([-2,2]\)</span>, denoted by the dotted lines on either side.. The minimizer <span class="math inline">\(x^*=0.3\)</span> (which can be solved analytically!), given by the middle dotted line, lies inside the interval <span class="math inline">\([x_l, x_r]=[-2,2]\)</span>. We notice that <span class="math inline">\(f(x)\)</span> strictly decreases for <span class="math inline">\(f(x) &lt; f(x^*)\)</span> and strictly increases for <span class="math inline">\(f(x) &gt; f(x^*)\)</span>. The interval <span class="math inline">\([x_l, x_r]\)</span> that has the minimizer within it, is called the <em>interval of uncertainty</em> and the goal of an optimization algorithm is to reduce this interval as much as possible to converge towards the minimizer. A good algorithm completes the convergence very fast. In each step of this reduction of the interval, the algorithm finds a new unimodal interval following the following procedures:</p>
<ul>
<li>Choose two new points, <span class="math inline">\(x_1 \in [x_l, x^*]\)</span> and another point <span class="math inline">\(x_2 \in [x^*, x_r]\)</span> (denoted by the two filled straight lines in the figure,</li>
<li>If <span class="math inline">\(f(x_2) &gt; f(x_1)\)</span>, the new interval becomes <span class="math inline">\([x_l, x_2]\)</span> and <span class="math inline">\(x_r\)</span> becomes <span class="math inline">\(x_2\)</span>, i.e, <span class="math inline">\(x_r=x_2\)</span>,</li>
<li>Next pick a new <span class="math inline">\(x_2\)</span>,</li>
<li>If condition in step (2) is not satisfied, we set the new interval as <span class="math inline">\([x_1, x_r]\)</span> directly after step (1) and set <span class="math inline">\(x_l=x_1\)</span>, and</li>
<li>Next pick a new <span class="math inline">\(x_1\)</span>.</li>
</ul>
<p>The given steps continue iteratively until the convergence is satisfied to a given limit of the minimizer. These class of methods is called an <em>Elimination Method</em> and we study two categories under this kind:</p>
<ul>
<li><strong>Fibonacci Search</strong>, and</li>
<li><strong>Golden Section Search</strong>.</li>
</ul>
<p>Raoâ€™s book <em>Engineering Optimization</em> [Rao, Singiresu S. Engineering optimization: theory and practice. John Wiley &amp; Sons, 2019.] also has some detailed studies on these kinds of optimization methods.</p>
</div>
<div id="fibonacci-search-method" class="section level2">
<h2><span class="header-section-number">3.3</span> Fibonacci Search Method</h2>
<p>Instead of finding the exact minimizer <span class="math inline">\(x^*\)</span> of <span class="math inline">\(f(x)\)</span>, the <em>Fibonacci search strategy</em> works by reducing the interval of uncertainty in every step, ultimately converging the interval, containing the minimizer, to a desired size as small as possible. One caveat is that, the initial interval containing, such that the interval lies in it, has to be known beforehand. However, the algorithm works on a nonlinear function, even if it is discontinuous. The name comes from the fact that the algorithm makes use of the famous sequence of <em>Fibonacci numbers</em> [<a href="http://oeis.org/A000045" class="uri">http://oeis.org/A000045</a>]. This sequence is defined in the following way:</p>
<p><span class="math display">\[\begin{align}
F_0&amp;=0,F_1=1, \\ 
F_n&amp;=F_{n-1} + F_{n-2},\text{ where }n=2,3,\ldots
\end{align}\]</span></p>
<p>We write a Python code to generate the first 16 Fibonacci numbers and display them as a table:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="solving-one-dimensional-optimization-problems.html#cb1-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="solving-one-dimensional-optimization-problems.html#cb1-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="solving-one-dimensional-optimization-problems.html#cb1-3"></a></span>
<span id="cb1-4"><a href="solving-one-dimensional-optimization-problems.html#cb1-4"></a><span class="kw">def</span> fibonacci(n): <span class="co"># define the  function</span></span>
<span id="cb1-5"><a href="solving-one-dimensional-optimization-problems.html#cb1-5"></a>    fn <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>,]</span>
<span id="cb1-6"><a href="solving-one-dimensional-optimization-problems.html#cb1-6"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, n<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb1-7"><a href="solving-one-dimensional-optimization-problems.html#cb1-7"></a>        fn.append(fn[i<span class="dv">-1</span>] <span class="op">+</span> fn[i<span class="dv">-2</span>])</span>
<span id="cb1-8"><a href="solving-one-dimensional-optimization-problems.html#cb1-8"></a>    <span class="cf">return</span> fn</span>
<span id="cb1-9"><a href="solving-one-dimensional-optimization-problems.html#cb1-9"></a></span>
<span id="cb1-10"><a href="solving-one-dimensional-optimization-problems.html#cb1-10"></a></span>
<span id="cb1-11"><a href="solving-one-dimensional-optimization-problems.html#cb1-11"></a>N <span class="op">=</span> np.arange(<span class="dv">16</span>)</span>
<span id="cb1-12"><a href="solving-one-dimensional-optimization-problems.html#cb1-12"></a>data <span class="op">=</span> {<span class="st">&#39;n&#39;</span>: N, <span class="st">&#39;Fibonacci(n)&#39;</span>: fibonacci(<span class="dv">15</span>)}</span>
<span id="cb1-13"><a href="solving-one-dimensional-optimization-problems.html#cb1-13"></a>df <span class="op">=</span> pd.DataFrame(data)</span></code></pre></div>
<p><code>df</code> looks like this:</p>
<pre><code>## +----+-----+----------------+
## |    |   n |   Fibonacci(n) |
## |----+-----+----------------|
## |  0 |   0 |              0 |
## |  1 |   1 |              1 |
## |  2 |   2 |              1 |
## |  3 |   3 |              2 |
## |  4 |   4 |              3 |
## |  5 |   5 |              5 |
## |  6 |   6 |              8 |
## |  7 |   7 |             13 |
## |  8 |   8 |             21 |
## |  9 |   9 |             34 |
## | 10 |  10 |             55 |
## | 11 |  11 |             89 |
## | 12 |  12 |            144 |
## | 13 |  13 |            233 |
## | 14 |  14 |            377 |
## | 15 |  15 |            610 |
## +----+-----+----------------+</code></pre>
<p>Let <span class="math inline">\(n\)</span> be the total number of experiments to be conducted and <span class="math inline">\([x_l, x_r]\)</span> be the initial interval the algorithm starts with. Let
<span class="math display" id="eq:2">\[\begin{eqnarray}
L_0 = x_r - x_l \tag{3.2}
\end{eqnarray}\]</span>
be the initial level of uncertainty and let us define,
<span class="math display" id="eq:3">\[\begin{eqnarray}
L_j = \frac{F_{n-2}}{F_n}L_0 \tag{3.3}
\end{eqnarray}\]</span>
where, <span class="math inline">\(F_{n-2}\)</span> and <span class="math inline">\(F_n\)</span> are the <span class="math inline">\((n-2)^{th}\)</span> and <span class="math inline">\(n^{th}\)</span> <em>Fibonacci numbers</em> respectively. We see from the formulation of the <em>Fibonacci numbers</em> that, <a href="solving-one-dimensional-optimization-problems.html#eq:3">(3.3)</a> shows the following property:
<span class="math display">\[\begin{equation}
    L_j = \frac{F_{n-2}}{F_n}L_0 \leq \frac{L_0}{2} \text{ for } n\geq 2 
\end{equation}\]</span>
Now, the initial two experiments are set at points <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, where, <span class="math inline">\(L_j = x_1 - x_l\)</span> and <span class="math inline">\(L_j = x_r - x_2\)</span>. So, combining these with Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:3">(3.3)</a>, we have:
<span class="math display" id="eq:4">\[\begin{equation}
    x_1 = x_l + \frac{F_{n-2}}{F_n}L_0 \tag{3.4}
\end{equation}\]</span>
and
<span class="math display" id="eq:5">\[\begin{equation}
    x_2 = x_r - \frac{F_{n-2}}{F_n}L_0 \tag{3.5}
    \end{equation}\]</span>
Now taking into consideration the unimodality assumption, a part of the interval of uncertainty is rejected, shrinking it to a smaller size, given by,
<span class="math display" id="eq:6">\[\begin{equation}
    L_i = L_0 - L_j = L_0(1-\frac{F_{n-2}}{F_n}) = \frac{F_{n-1}}{F_n}L_0 \tag{3.6}
\end{equation}\]</span>
where, we have used the fact that, <span class="math inline">\(F_n - F_{n-2} = F_{n-1}\)</span> from the formulation of the <em>Fibonacci numbers</em>. This procedure leaves us with only one experiment, which, from one end, is situated at a distance of
<span class="math display" id="eq:7">\[\begin{equation}
  L_j = \frac{F_{n-2}}{F_n}L_0 = \frac{F_{n-2}}{F_{n-1}}L_i \tag{3.7}
\end{equation}\]</span>
where, we have used Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:3">(3.3)</a>. From the other end, the same experiment point is situated at a distance give by,
<span class="math display" id="eq:8">\[\begin{equation}
L_i-L_j = \frac{F_{n-3}}{F_n}L_0 = \frac{F_{n-3}}{F_n}L_0 = \frac{F_{n-3}}{F_{n-1}}L_2 \tag{3.8}
\end{equation}\]</span>
where, we have again used Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:3">(3.3)</a>. We now place a new experiment point in the interval <span class="math inline">\(L_i\)</span> so that both the present experiment points are situated at a distance given by Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:7">(3.7)</a>. We again reduce the size of the interval of uncertainty using the unimodality conditions. This whole process is continued so that for the <span class="math inline">\(k^{th}\)</span> experiment point, its location is given by,
<span class="math display" id="eq:9">\[\begin{equation}
    L_{k[j]} = \frac{F_{n-k}}{F_{n-(k-2)}}L_{k-1} \tag{3.9}
\end{equation}\]</span>
and the interval of uncertainty is given by,
<span class="math display" id="eq:10">\[\begin{equation}
    l_{k[i]} = \frac{F_{n-(k-1)}}{F_n}L_0 \tag{3.10}
\end{equation}\]</span>
after <span class="math inline">\(k\)</span> iterations are completed. Now, the <em>reduction ratio</em> given by the ratio of the present interval of uncertainty after conduction <span class="math inline">\(k\)</span> iterations out of the <span class="math inline">\(n\)</span> experiments to be performed, <span class="math inline">\(L_{k[i]}\)</span> to the initial interval of uncertainty, <span class="math inline">\(L_0\)</span>:
<span class="math display" id="eq:11">\[\begin{equation}
    R = \frac{L_{k[i]}}{L_0} = \frac{F_{n-(k-1)}}{F_n} \tag{3.11}
\end{equation}\]</span></p>
<p>The purpose of this algorithm is to bring <span class="math inline">\(R \sim 0\)</span>. The <strong>Fibonacci Search Algorithm</strong> has been shown below:</p>
<p><img src="img%203.png" /></p>
<p>We will write a Python function that implements the above algorithm</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="solving-one-dimensional-optimization-problems.html#cb3-1"></a><span class="kw">def</span> fib_search(f, xl, xr, n):</span>
<span id="cb3-2"><a href="solving-one-dimensional-optimization-problems.html#cb3-2"></a>    F <span class="op">=</span> fibonacci(n) <span class="co"># Call the fibonnaci number function</span></span>
<span id="cb3-3"><a href="solving-one-dimensional-optimization-problems.html#cb3-3"></a>    L0 <span class="op">=</span> xr <span class="op">-</span> xl <span class="co"># Initial interval of uncertainty</span></span>
<span id="cb3-4"><a href="solving-one-dimensional-optimization-problems.html#cb3-4"></a>    R1 <span class="op">=</span> L0 <span class="co"># Initial Reduction Ratio</span></span>
<span id="cb3-5"><a href="solving-one-dimensional-optimization-problems.html#cb3-5"></a>    Li <span class="op">=</span> (F[n<span class="dv">-2</span>]<span class="op">/</span>F[n])<span class="op">*</span>L0 </span>
<span id="cb3-6"><a href="solving-one-dimensional-optimization-problems.html#cb3-6"></a>    </span>
<span id="cb3-7"><a href="solving-one-dimensional-optimization-problems.html#cb3-7"></a>    R <span class="op">=</span> [Li<span class="op">/</span>L0]</span>
<span id="cb3-8"><a href="solving-one-dimensional-optimization-problems.html#cb3-8"></a></span>
<span id="cb3-9"><a href="solving-one-dimensional-optimization-problems.html#cb3-9"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, n<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb3-10"><a href="solving-one-dimensional-optimization-problems.html#cb3-10"></a>        <span class="cf">if</span> Li <span class="op">&gt;</span> L0<span class="op">/</span><span class="dv">2</span>:</span>
<span id="cb3-11"><a href="solving-one-dimensional-optimization-problems.html#cb3-11"></a>            x1 <span class="op">=</span> xr <span class="op">-</span> Li</span>
<span id="cb3-12"><a href="solving-one-dimensional-optimization-problems.html#cb3-12"></a>            x2 <span class="op">=</span> xl <span class="op">+</span> Li</span>
<span id="cb3-13"><a href="solving-one-dimensional-optimization-problems.html#cb3-13"></a>        <span class="cf">else</span>:</span>
<span id="cb3-14"><a href="solving-one-dimensional-optimization-problems.html#cb3-14"></a>            x1 <span class="op">=</span> xl <span class="op">+</span> Li</span>
<span id="cb3-15"><a href="solving-one-dimensional-optimization-problems.html#cb3-15"></a>            x2 <span class="op">=</span> xr <span class="op">-</span> Li</span>
<span id="cb3-16"><a href="solving-one-dimensional-optimization-problems.html#cb3-16"></a>            </span>
<span id="cb3-17"><a href="solving-one-dimensional-optimization-problems.html#cb3-17"></a>        f1, f2 <span class="op">=</span> f(x1), f(x2)</span>
<span id="cb3-18"><a href="solving-one-dimensional-optimization-problems.html#cb3-18"></a>        </span>
<span id="cb3-19"><a href="solving-one-dimensional-optimization-problems.html#cb3-19"></a>        <span class="cf">if</span> f1 <span class="op">&lt;</span> f2:</span>
<span id="cb3-20"><a href="solving-one-dimensional-optimization-problems.html#cb3-20"></a>            xr <span class="op">=</span> x2</span>
<span id="cb3-21"><a href="solving-one-dimensional-optimization-problems.html#cb3-21"></a>            Li <span class="op">=</span> (F[n <span class="op">-</span> i]<span class="op">/</span>F[n <span class="op">-</span> (i <span class="op">-</span> <span class="dv">2</span>)])<span class="op">*</span>L0 <span class="co"># New interval of uncertainty</span></span>
<span id="cb3-22"><a href="solving-one-dimensional-optimization-problems.html#cb3-22"></a>        <span class="cf">elif</span> f1 <span class="op">&gt;</span> f2:</span>
<span id="cb3-23"><a href="solving-one-dimensional-optimization-problems.html#cb3-23"></a>            xl <span class="op">=</span> x1</span>
<span id="cb3-24"><a href="solving-one-dimensional-optimization-problems.html#cb3-24"></a>            Li <span class="op">=</span> (F[n <span class="op">-</span> i]<span class="op">/</span>F[n <span class="op">-</span> (i <span class="op">-</span> <span class="dv">2</span>)])<span class="op">*</span>L0 <span class="co"># New interval of uncertainty</span></span>
<span id="cb3-25"><a href="solving-one-dimensional-optimization-problems.html#cb3-25"></a>        <span class="cf">else</span>:</span>
<span id="cb3-26"><a href="solving-one-dimensional-optimization-problems.html#cb3-26"></a>            xl, xr <span class="op">=</span> x1, x2</span>
<span id="cb3-27"><a href="solving-one-dimensional-optimization-problems.html#cb3-27"></a>            Li <span class="op">=</span> (F[n <span class="op">-</span> i]<span class="op">/</span>F[n <span class="op">-</span> (i <span class="op">-</span> <span class="dv">2</span>)])<span class="op">*</span>(xr <span class="op">-</span> xl) <span class="co"># New interval of uncertainty</span></span>
<span id="cb3-28"><a href="solving-one-dimensional-optimization-problems.html#cb3-28"></a>            </span>
<span id="cb3-29"><a href="solving-one-dimensional-optimization-problems.html#cb3-29"></a>        L0 <span class="op">=</span> xr <span class="op">-</span> xl</span>
<span id="cb3-30"><a href="solving-one-dimensional-optimization-problems.html#cb3-30"></a>        R <span class="op">+=</span> [Li<span class="op">/</span>R1,] <span class="co"># Append the new reduction ratio</span></span>
<span id="cb3-31"><a href="solving-one-dimensional-optimization-problems.html#cb3-31"></a>        </span>
<span id="cb3-32"><a href="solving-one-dimensional-optimization-problems.html#cb3-32"></a>    <span class="cf">if</span> f1 <span class="op">&lt;=</span> f2:</span>
<span id="cb3-33"><a href="solving-one-dimensional-optimization-problems.html#cb3-33"></a>        <span class="cf">return</span> [x1, f(x1), R] <span class="co"># Final result</span></span>
<span id="cb3-34"><a href="solving-one-dimensional-optimization-problems.html#cb3-34"></a>    <span class="cf">else</span>:</span>
<span id="cb3-35"><a href="solving-one-dimensional-optimization-problems.html#cb3-35"></a>        <span class="cf">return</span> [x2, f(x2), R] <span class="co"># Final result</span></span></code></pre></div>

<div class="example">
<span id="exm:unnamed-chunk-6" class="example"><strong>Example 3.1  </strong></span>Let an objective function be:
<span class="math display" id="eq:12">\[\begin{equation}
    f(x) = x^5 - 5x^3 - 20x + 5 \tag{3.12}
\end{equation}\]</span>
We will use the <strong>Fibonacci search algorithm</strong> to find the minimizer <span class="math inline">\(x^*\)</span>, taking <span class="math inline">\(n=25\)</span> and the initial interval of uncertainty <span class="math inline">\([-2.5, 2.5]\)</span>. Letâ€™s write a Python function to define the given objective function and visualize the same:
</div>

<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="solving-one-dimensional-optimization-problems.html#cb4-1"></a><span class="kw">def</span> f(x): <span class="co"># Objective function</span></span>
<span id="cb4-2"><a href="solving-one-dimensional-optimization-problems.html#cb4-2"></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">5</span> <span class="op">-</span> <span class="dv">5</span><span class="op">*</span>x<span class="op">**</span><span class="dv">3</span> <span class="op">-</span> <span class="dv">20</span><span class="op">*</span>x <span class="op">+</span> <span class="dv">5</span></span>
<span id="cb4-3"><a href="solving-one-dimensional-optimization-problems.html#cb4-3"></a></span>
<span id="cb4-4"><a href="solving-one-dimensional-optimization-problems.html#cb4-4"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb4-5"><a href="solving-one-dimensional-optimization-problems.html#cb4-5"></a>plt.plot(x, f(x), <span class="st">&#39;r-&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="solving-one-dimensional-optimization-problems.html#cb5-1"></a>plt.xlabel(<span class="st">&#39;x -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="solving-one-dimensional-optimization-problems.html#cb6-1"></a>plt.ylabel(<span class="st">&#39;f(x) -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="solving-one-dimensional-optimization-problems.html#cb7-1"></a>plt.title(<span class="st">&quot;Graph of $f(x) =  x^5-5x^3-20x+5$&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="solving-one-dimensional-optimization-problems.html#cb8-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-3.png" width="672" /></p>
<p>Now, we consider <span class="math inline">\(n=25\)</span> and use the function <code>fib_search(f, -2.5, 2.5, 25)</code> to run the optimization and print the results:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="solving-one-dimensional-optimization-problems.html#cb9-1"></a>Fib <span class="op">=</span> fib_search(f, <span class="fl">-2.5</span>, <span class="fl">2.5</span>, <span class="dv">25</span>)</span>
<span id="cb9-2"><a href="solving-one-dimensional-optimization-problems.html#cb9-2"></a>x_star, f_x_star, R <span class="op">=</span> Fib</span>
<span id="cb9-3"><a href="solving-one-dimensional-optimization-problems.html#cb9-3"></a><span class="bu">print</span> (<span class="st">&quot;x*:&quot;</span>, x_star)</span></code></pre></div>
<pre><code>## x*: 1.999966677774075</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="solving-one-dimensional-optimization-problems.html#cb11-1"></a><span class="bu">print</span> (<span class="st">&quot;f(x*):&quot;</span>, f_x_star)</span></code></pre></div>
<pre><code>## f(x*): -42.99999994448275</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="solving-one-dimensional-optimization-problems.html#cb13-1"></a><span class="bu">print</span> (<span class="st">&quot;Final Reduction Ratio:&quot;</span>, R[<span class="op">-</span><span class="dv">1</span>])</span></code></pre></div>
<pre><code>## Final Reduction Ratio: 0.0</code></pre>
<p>We see that <span class="math inline">\(x^* \sim 2\)</span>, <span class="math inline">\(f(x^*) \sim -43\)</span> and the final <em>Reduction Ration</em> is 0. Now, to show the positions of <span class="math inline">\(x^*\)</span> and <span class="math inline">\(f(x^*)\)</span> on the graph of the objective function, we write the following code:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="solving-one-dimensional-optimization-problems.html#cb15-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb15-2"><a href="solving-one-dimensional-optimization-problems.html#cb15-2"></a>plt.plot(x, f(x), <span class="st">&#39;r-&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="solving-one-dimensional-optimization-problems.html#cb16-1"></a>plt.plot([x_star], [f_x_star], <span class="st">&#39;ko&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="solving-one-dimensional-optimization-problems.html#cb17-1"></a>plt.axvline(x<span class="op">=</span>x_star, color<span class="op">=</span><span class="st">&#39;b&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="solving-one-dimensional-optimization-problems.html#cb18-1"></a>plt.xlabel(<span class="st">&#39;x -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="solving-one-dimensional-optimization-problems.html#cb19-1"></a>plt.ylabel(<span class="st">&#39;f(x) -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="solving-one-dimensional-optimization-problems.html#cb20-1"></a>plt.title(<span class="st">&quot;$x^*$ denoted as broken blue line and $f(x^*)$ denoted as the black dot&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="solving-one-dimensional-optimization-problems.html#cb21-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-5.png" width="672" /></p>
<p>We can modify our function in such a way that all the optimization data in every step are collected
and displayed as a <code>DataFrame</code>:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="solving-one-dimensional-optimization-problems.html#cb22-1"></a></span>
<span id="cb22-2"><a href="solving-one-dimensional-optimization-problems.html#cb22-2"></a><span class="kw">def</span> fib_search(f, xl, xr, n):    </span>
<span id="cb22-3"><a href="solving-one-dimensional-optimization-problems.html#cb22-3"></a>    F <span class="op">=</span> fibonacci(n)</span>
<span id="cb22-4"><a href="solving-one-dimensional-optimization-problems.html#cb22-4"></a>    L0 <span class="op">=</span> xr <span class="op">-</span> xl</span>
<span id="cb22-5"><a href="solving-one-dimensional-optimization-problems.html#cb22-5"></a>    ini <span class="op">=</span> L0</span>
<span id="cb22-6"><a href="solving-one-dimensional-optimization-problems.html#cb22-6"></a>    Li <span class="op">=</span> (F[n<span class="dv">-2</span>]<span class="op">/</span>F[n])<span class="op">*</span>L0</span>
<span id="cb22-7"><a href="solving-one-dimensional-optimization-problems.html#cb22-7"></a>    </span>
<span id="cb22-8"><a href="solving-one-dimensional-optimization-problems.html#cb22-8"></a>    R <span class="op">=</span> [Li<span class="op">/</span>L0]</span>
<span id="cb22-9"><a href="solving-one-dimensional-optimization-problems.html#cb22-9"></a>    a <span class="op">=</span> [xl]</span>
<span id="cb22-10"><a href="solving-one-dimensional-optimization-problems.html#cb22-10"></a>    b <span class="op">=</span> [xr]</span>
<span id="cb22-11"><a href="solving-one-dimensional-optimization-problems.html#cb22-11"></a>    F1 <span class="op">=</span> [f(xl)]</span>
<span id="cb22-12"><a href="solving-one-dimensional-optimization-problems.html#cb22-12"></a>    F2 <span class="op">=</span> [f(xr)]</span>
<span id="cb22-13"><a href="solving-one-dimensional-optimization-problems.html#cb22-13"></a></span>
<span id="cb22-14"><a href="solving-one-dimensional-optimization-problems.html#cb22-14"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, n<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb22-15"><a href="solving-one-dimensional-optimization-problems.html#cb22-15"></a>        <span class="co">#print(&quot;reduction ratio:&quot;, Li/ini)</span></span>
<span id="cb22-16"><a href="solving-one-dimensional-optimization-problems.html#cb22-16"></a>        <span class="cf">if</span> Li <span class="op">&gt;</span> L0<span class="op">/</span><span class="dv">2</span>:</span>
<span id="cb22-17"><a href="solving-one-dimensional-optimization-problems.html#cb22-17"></a>            x1 <span class="op">=</span> xr <span class="op">-</span> Li</span>
<span id="cb22-18"><a href="solving-one-dimensional-optimization-problems.html#cb22-18"></a>            x2 <span class="op">=</span> xl <span class="op">+</span> Li</span>
<span id="cb22-19"><a href="solving-one-dimensional-optimization-problems.html#cb22-19"></a>        <span class="cf">else</span>:</span>
<span id="cb22-20"><a href="solving-one-dimensional-optimization-problems.html#cb22-20"></a>            x1 <span class="op">=</span> xl <span class="op">+</span> Li</span>
<span id="cb22-21"><a href="solving-one-dimensional-optimization-problems.html#cb22-21"></a>            x2 <span class="op">=</span> xr <span class="op">-</span> Li</span>
<span id="cb22-22"><a href="solving-one-dimensional-optimization-problems.html#cb22-22"></a>            </span>
<span id="cb22-23"><a href="solving-one-dimensional-optimization-problems.html#cb22-23"></a>        f1, f2 <span class="op">=</span> f(x1), f(x2)</span>
<span id="cb22-24"><a href="solving-one-dimensional-optimization-problems.html#cb22-24"></a>        </span>
<span id="cb22-25"><a href="solving-one-dimensional-optimization-problems.html#cb22-25"></a>        <span class="cf">if</span> f1 <span class="op">&lt;</span> f2:</span>
<span id="cb22-26"><a href="solving-one-dimensional-optimization-problems.html#cb22-26"></a>            xr <span class="op">=</span> x2</span>
<span id="cb22-27"><a href="solving-one-dimensional-optimization-problems.html#cb22-27"></a>            Li <span class="op">=</span> (F[n <span class="op">-</span> i]<span class="op">/</span>F[n <span class="op">-</span> (i <span class="op">-</span> <span class="dv">2</span>)])<span class="op">*</span>L0</span>
<span id="cb22-28"><a href="solving-one-dimensional-optimization-problems.html#cb22-28"></a>        <span class="cf">elif</span> f1 <span class="op">&gt;</span> f2:</span>
<span id="cb22-29"><a href="solving-one-dimensional-optimization-problems.html#cb22-29"></a>            xl <span class="op">=</span> x1</span>
<span id="cb22-30"><a href="solving-one-dimensional-optimization-problems.html#cb22-30"></a>            Li <span class="op">=</span> (F[n <span class="op">-</span> i]<span class="op">/</span>F[n <span class="op">-</span> (i <span class="op">-</span> <span class="dv">2</span>)])<span class="op">*</span>L0</span>
<span id="cb22-31"><a href="solving-one-dimensional-optimization-problems.html#cb22-31"></a>        <span class="cf">else</span>:</span>
<span id="cb22-32"><a href="solving-one-dimensional-optimization-problems.html#cb22-32"></a>            xl, xr <span class="op">=</span> x1, x2</span>
<span id="cb22-33"><a href="solving-one-dimensional-optimization-problems.html#cb22-33"></a>            Li <span class="op">=</span> (F[n <span class="op">-</span> i]<span class="op">/</span>F[n <span class="op">-</span> (i <span class="op">-</span> <span class="dv">2</span>)])<span class="op">*</span>(xr <span class="op">-</span> xl)</span>
<span id="cb22-34"><a href="solving-one-dimensional-optimization-problems.html#cb22-34"></a>            </span>
<span id="cb22-35"><a href="solving-one-dimensional-optimization-problems.html#cb22-35"></a>        L0 <span class="op">=</span> xr <span class="op">-</span> xl</span>
<span id="cb22-36"><a href="solving-one-dimensional-optimization-problems.html#cb22-36"></a>        R <span class="op">+=</span> [Li<span class="op">/</span>ini,] </span>
<span id="cb22-37"><a href="solving-one-dimensional-optimization-problems.html#cb22-37"></a>        a <span class="op">+=</span> [xl, ]</span>
<span id="cb22-38"><a href="solving-one-dimensional-optimization-problems.html#cb22-38"></a>        b <span class="op">+=</span> [xr, ]</span>
<span id="cb22-39"><a href="solving-one-dimensional-optimization-problems.html#cb22-39"></a>        F1 <span class="op">+=</span> [f1, ]</span>
<span id="cb22-40"><a href="solving-one-dimensional-optimization-problems.html#cb22-40"></a>        F2 <span class="op">+=</span> [f2, ]</span>
<span id="cb22-41"><a href="solving-one-dimensional-optimization-problems.html#cb22-41"></a>        </span>
<span id="cb22-42"><a href="solving-one-dimensional-optimization-problems.html#cb22-42"></a>    data <span class="op">=</span> {<span class="st">&#39;n&#39;</span> : <span class="bu">range</span>(<span class="dv">0</span>, n),</span>
<span id="cb22-43"><a href="solving-one-dimensional-optimization-problems.html#cb22-43"></a>            <span class="st">&#39;xl&#39;</span>: a,</span>
<span id="cb22-44"><a href="solving-one-dimensional-optimization-problems.html#cb22-44"></a>            <span class="st">&#39;xr&#39;</span>: b,</span>
<span id="cb22-45"><a href="solving-one-dimensional-optimization-problems.html#cb22-45"></a>            <span class="st">&#39;f(x1)&#39;</span>: F1,</span>
<span id="cb22-46"><a href="solving-one-dimensional-optimization-problems.html#cb22-46"></a>            <span class="st">&#39;f(x2)&#39;</span>: F2,</span>
<span id="cb22-47"><a href="solving-one-dimensional-optimization-problems.html#cb22-47"></a>            <span class="st">&#39;Reduction Ratio&#39;</span>: R}</span>
<span id="cb22-48"><a href="solving-one-dimensional-optimization-problems.html#cb22-48"></a></span>
<span id="cb22-49"><a href="solving-one-dimensional-optimization-problems.html#cb22-49"></a>    df <span class="op">=</span> pd.DataFrame(data, columns <span class="op">=</span> [<span class="st">&#39;n&#39;</span>, <span class="st">&#39;xl&#39;</span>, <span class="st">&#39;xr&#39;</span>, <span class="st">&#39;f(x1)&#39;</span>, <span class="st">&#39;f(x2)&#39;</span>, <span class="st">&#39;Reduction Ratio&#39;</span>])</span>
<span id="cb22-50"><a href="solving-one-dimensional-optimization-problems.html#cb22-50"></a>    <span class="cf">return</span> df</span>
<span id="cb22-51"><a href="solving-one-dimensional-optimization-problems.html#cb22-51"></a></span>
<span id="cb22-52"><a href="solving-one-dimensional-optimization-problems.html#cb22-52"></a>df <span class="op">=</span> fib_search(f, <span class="fl">-2.5</span>, <span class="fl">2.5</span>, <span class="dv">25</span>)</span></code></pre></div>
<p>Where <code>df</code> looks like this:</p>
<pre><code>## +----+-----+----------+---------+-----------+-----------+-------------------+
## |    |   n |       xl |      xr |     f(x1) |     f(x2) |   Reduction Ratio |
## |----+-----+----------+---------+-----------+-----------+-------------------|
## |  0 |   0 | -2.5     | 2.5     |  35.4688  | -25.4688  |       0.381966    |
## |  1 |   1 | -0.59017 | 2.5     |  17.7596  |  -7.75959 |       0.381966    |
## |  2 |   2 |  0.59017 | 2.5     |  -7.75959 | -28.8819  |       0.236068    |
## |  3 |   3 |  1.31966 | 2.5     | -28.8819  | -40.7626  |       0.145898    |
## |  4 |   4 |  1.77051 | 2.5     | -40.7626  | -42.875   |       0.0901699   |
## |  5 |   5 |  1.77051 | 2.22136 | -42.875   | -40.1458  |       0.0557281   |
## |  6 |   6 |  1.94272 | 2.22136 | -42.8424  | -42.875   |       0.0344419   |
## |  7 |   7 |  1.94272 | 2.11493 | -42.875   | -42.2847  |       0.0212862   |
## |  8 |   8 |  1.94272 | 2.04915 | -42.9964  | -42.875   |       0.0131556   |
## |  9 |   9 |  1.98337 | 2.04915 | -42.9863  | -42.9964  |       0.00813062  |
## | 10 |  10 |  1.98337 | 2.02403 | -42.9964  | -42.9707  |       0.00502499  |
## | 11 |  11 |  1.98337 | 2.0085  | -42.9999  | -42.9964  |       0.00310563  |
## | 12 |  12 |  1.99297 | 2.0085  | -42.9975  | -42.9999  |       0.00191936  |
## | 13 |  13 |  1.99297 | 2.00257 | -42.9999  | -42.9997  |       0.00118627  |
## | 14 |  14 |  1.99663 | 2.00257 | -42.9994  | -42.9999  |       0.000733089 |
## | 15 |  15 |  1.9989  | 2.00257 | -42.9999  | -43       |       0.000453182 |
## | 16 |  16 |  1.9989  | 2.00117 | -43       | -42.9999  |       0.000279907 |
## | 17 |  17 |  1.9989  | 2.0003  | -43       | -43       |       0.000173276 |
## | 18 |  18 |  1.99943 | 2.0003  | -43       | -43       |       0.000106631 |
## | 19 |  19 |  1.99977 | 2.0003  | -43       | -43       |       6.66445e-05 |
## | 20 |  20 |  1.99977 | 2.0001  | -43       | -43       |       3.99867e-05 |
## | 21 |  21 |  1.9999  | 2.0001  | -43       | -43       |       2.66578e-05 |
## | 22 |  22 |  1.9999  | 2.00003 | -43       | -43       |       1.33289e-05 |
## | 23 |  23 |  1.99997 | 1.99997 | -43       | -43       |       0           |
## | 24 |  24 |  1.99997 | 1.99997 | -43       | -43       |       0           |
## +----+-----+----------+---------+-----------+-----------+-------------------+</code></pre>
<p>The graph of the reduction ratio at each <span class="math inline">\(n\)</span> can be plotted with the following code:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="solving-one-dimensional-optimization-problems.html#cb24-1"></a>plt.xlabel(<span class="st">&quot;n-&gt;&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="solving-one-dimensional-optimization-problems.html#cb25-1"></a>plt.ylabel(<span class="st">&quot;Reduction Ratio -&gt;&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="solving-one-dimensional-optimization-problems.html#cb26-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(df)), df[<span class="st">&#39;Reduction Ratio&#39;</span>])</span></code></pre></div>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="solving-one-dimensional-optimization-problems.html#cb27-1"></a>plt.title(<span class="st">&quot;Graph of change of reduction ratio at each $n$&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="solving-one-dimensional-optimization-problems.html#cb28-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-7.png" width="672" /></p>
</div>
<div id="golden-section-search-method" class="section level2">
<h2><span class="header-section-number">3.4</span> Golden Section Search Method</h2>
<p>The <strong>golden section search method</strong> is a modified version of the <strong>Fibonacci search method</strong>. One advantage of the former over the later is that, we do not need to keep a record of the total number of experiment points <span class="math inline">\(n\)</span> beforehand. While selecting <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> inside the interval of uncertainty, we make use of the golden ratio, <span class="math inline">\(\phi = \frac{\sqrt{5} - 1}{2}\)</span> which is the positive root of the quadratic equation given by:
<span class="math display" id="eq:13">\[\begin{equation}
    \phi^2+\phi-1=0 \tag{3.13}
\end{equation}\]</span></p>
<p>Given the initial interval <span class="math inline">\([x_l, x_r]\)</span>, we have the initial interval of uncertainty as,
<span class="math display" id="eq:14">\[\begin{equation}
L_0 = x_r - x_l \tag{3.14}
\end{equation}\]</span></p>
<p>The new interior points <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are chosen in such a way that both of them lies at a distance <span class="math inline">\(\phi^2L_0\)</span> from either side, i.e,
<span class="math display" id="eq:15">\[\begin{equation}
    x_1-x_l = \phi^2L_0 \tag{3.15}
\end{equation}\]</span>
and
<span class="math display" id="eq:16">\[\begin{equation}
    x_r-x_2 = \phi^2L_0 \tag{3.16}
\end{equation}\]</span></p>
<p>Now from Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:13">(3.13)</a> we know
<span class="math display" id="eq:17">\[\begin{equation}
    L_0 = (\phi^2 + \phi)L_0 \tag{3.17}
\end{equation}\]</span></p>
<p>The above computations leave us with:
<span class="math display" id="eq:18">\[\begin{equation}
x_r - x_1 = \phi L_0 \tag{3.18}
\end{equation}\]</span>
and
<span class="math display" id="eq:19">\[\begin{equation}
    x_2 - x_l = \phi L_0 \tag{3.19}
\end{equation}\]</span></p>
<p>Given, <span class="math inline">\(f(x)\)</span> is the nonlinear objective function, we now check whether <span class="math inline">\(f(x_1) &gt; f(x_2)\)</span>. If this is the case, we set <span class="math inline">\(x_l = x_1\)</span>, otherwise if <span class="math inline">\(f(x_1) &gt; f(x_2)\)</span>, we set <span class="math inline">\(x_r=x_2\)</span>. The new interval of uncertainty is set to be <span class="math inline">\(L_i = \phi L_0\)</span> and the previous interval is shrunk. This process of choosing new experimental points and shrinking the interval of uncertainty is continued until the termination condition is satisfied. The termination condition is to check whether the interval of uncertainty is less than a particular tolerance <span class="math inline">\(\epsilon\)</span> usually provided by the user. The <em>golden section search</em> algorithm is given below:</p>
<p><img src="img%204.png" /></p>

<div class="example">
<p><span id="exm:unnamed-chunk-13" class="example"><strong>Example 3.2  </strong></span>Let us consider an objective function:
<span class="math display" id="eq:20">\[\begin{equation}
    f(x) = \frac{1}{16}x^3 - \frac{27}{4}x \tag{3.20}
\end{equation}\]</span></p>
We will use the <em>golden section search</em> method to find the minimizer <span class="math inline">\(x^*\)</span> of this function and compute <span class="math inline">\(f(x^*)\)</span>. Suppose the initial interval be <span class="math inline">\([-10, 10]\)</span> and the tolerance for the termination condition for the algorithm be <span class="math inline">\(\epsilon = 10^{-5}\)</span>. Let us first define the function in Python:
</div>

<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="solving-one-dimensional-optimization-problems.html#cb29-1"></a><span class="kw">def</span> f(x): <span class="co"># objective function</span></span>
<span id="cb29-2"><a href="solving-one-dimensional-optimization-problems.html#cb29-2"></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">3</span><span class="op">/</span><span class="dv">16</span> <span class="op">-</span> <span class="dv">27</span><span class="op">*</span>x<span class="op">/</span><span class="dv">4</span></span></code></pre></div>
<p>The graph of the objective function:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="solving-one-dimensional-optimization-problems.html#cb30-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">20</span>, <span class="dv">20</span>, <span class="dv">100</span>)</span>
<span id="cb30-2"><a href="solving-one-dimensional-optimization-problems.html#cb30-2"></a>plt.plot(x, f(x), <span class="st">&#39;r-&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="solving-one-dimensional-optimization-problems.html#cb31-1"></a>plt.xlabel(<span class="st">&#39;x -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="solving-one-dimensional-optimization-problems.html#cb32-1"></a>plt.ylabel(<span class="st">&#39;f(x) -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="solving-one-dimensional-optimization-problems.html#cb33-1"></a>plt.title(<span class="st">&quot;Graph of $f(x) = </span><span class="ch">\\</span><span class="st">frac</span><span class="sc">{1}{16}</span><span class="st">x^3 - </span><span class="ch">\\</span><span class="st">frac</span><span class="sc">{27}{4}</span><span class="st">x$&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="solving-one-dimensional-optimization-problems.html#cb34-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-9.png" width="672" /></p>
<p>For tackling this problem, we will not write our own Python function. As already stated in the last chapter, the <code>scipy.optimize</code> package too equips us with solvers to solve these tasks. For this problem, we use the <code>minimize_scalar()</code> function provided by <code>scipy.optimize</code>, which is used for minimization of a scalar function of one variable. The <code>minimize_scalar()</code> function provides the user with the following parameter:</p>
<ul>
<li><code>fun</code>: The objective function which must be callable,</li>
<li><code>bracket</code>: This is an optional parameter and defines the bracketing interval. This is a sequence, and consists of either three points <span class="math inline">\((x_a, x_b, x_c)\)</span>, such that <span class="math inline">\(x_a &lt; x_b &lt; x_c\)</span> and <span class="math inline">\(f(x_b) &lt; f(x_a), f(x_c)\)</span>, or two points <span class="math inline">\((x_a, x_b)\)</span> that are considered as the starting interval for any elimination search method,</li>
<li><code>bounds</code>: This is an optional parameter too (important for our analysis!) and is a sequence. This defines the optimization bound, i.e, the initial interval of uncertainty, <span class="math inline">\([x_l, x_r]\)</span>,</li>
<li><code>args</code>: This is a tuple and an optional parameter that defines the extra arguments that might be needed to pass to the objective function,</li>
<li><code>method</code>: This is the most important parameter that defines the various solvers provided by <code>minimize_scalar()</code>. This should be either a string (<code>str</code>) or a callable object. As of writing this book, the solvers that <code>minimize_scalar()</code> provides are:
<ul>
<li><code>'golden'</code>: Uses the <em>golden section search</em> method for finding the local minimum,</li>
<li><code>'brent'</code>: Uses the <em>Brentâ€™s algorithm</em> (will be discussed in the next section) for finding the local minimum,</li>
<li><code>'bounded'</code>: For performing bounded minimization and uses the <code>Brent's algorithm</code> to find the local minima specified in the <code>'bounds'</code> parameter. The <code>method</code> parameter is optional too, and if not provided, the <code>minimize_scalar()</code> function uses the <code>'brent'</code> method by default. The user can also write and pass a custom solver which must be a Python callable object.</li>
</ul></li>
<li><code>tol</code>: This parameter represents the tolerance (<span class="math inline">\(\epsilon\)</span>) of the optimization algorithm. This must be a <code>float</code> and is optional too, and</li>
<li><code>options</code>: This is an optional parameter and is a Python dictionary which specifies the solver options:
<ul>
<li><code>maxiter</code>: This is an <code>int</code> object and denotes the maximum number of iterations to be performed by the solver,</li>
<li><code>disp</code>: This must be a boolean (<code>bool</code>) object, and if set to <code>True</code>, prints a detailed information about the convergence of the algorithm</li>
</ul></li>
</ul>
<p>The <code>minimize_scalar()</code> function returns the optimization result as a specific Python object designed specifically for the <code>scipy.optimize</code> module called, <code>OptimizeResult</code>. It has the following important attributes:</p>
<ul>
<li><code>x</code>: The solution (<span class="math inline">\(x^*\)</span>) of the optimization. This is a <em>numpy array</em> object, i.e, <code>ndarray</code> and can return a scalar or a vector,</li>
<li><code>success</code>: This is a <code>bool</code> object and states whether the optimization process has completed successfully or not,</li>
<li><code>fun, jac, hess</code>: Provides the objective function, Jacobian and the Hessian matrix at the solution <span class="math inline">\(x^*\)</span> as <code>ndarray</code> objects,</li>
<li><code>nfev, njev, nhev</code>: Provides the number of evaluations of the objective function, its Jacobian and Hessian matrix during the running of the optimization solver and are <code>int</code> objects,</li>
<li>This is an <code>int</code> object and states the number of iterations that have been performed by the solver, and</li>
<li><code>maxcv</code>: This is a <code>float</code> object and represents the maximum constraint evaluation.</li>
</ul>
<p>Now returning back to our example, we have <span class="math inline">\(f(x)\)</span> defined by Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:20">(3.20)</a>, we use the <em>golden section search method</em> to find its minimizer. Using the <code>minimize_scalar()</code> function and setting parameters <code>method = 'golden'</code>, <code>bounds = (-10, 10)</code> and <code>tol = 10**-5</code> we can get our solution. We see that the initial interval has been set to <span class="math inline">\([-10, 10]\)</span> and the tolerance <span class="math inline">\(\epsilon\)</span> has been set <span class="math inline">\(10^{-5}\)</span>. We write the Python code:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="solving-one-dimensional-optimization-problems.html#cb35-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize_scalar</span>
<span id="cb35-2"><a href="solving-one-dimensional-optimization-problems.html#cb35-2"></a>result <span class="op">=</span> minimize_scalar(f, bounds <span class="op">=</span> (<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>), method <span class="op">=</span> <span class="st">&#39;golden&#39;</span>, tol <span class="op">=</span> <span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>)</span>
<span id="cb35-3"><a href="solving-one-dimensional-optimization-problems.html#cb35-3"></a><span class="bu">print</span>(result)</span></code></pre></div>
<pre><code>##      fun: -26.99999999991886
##     nfev: 32
##      nit: 26
##  success: True
##        x: 5.99999150720724</code></pre>
<p>We notice that <span class="math inline">\(x^* \sim 6\)</span>, <span class="math inline">\(f(x^*) \sim -27\)</span>, the number of iterations it took to converge to <span class="math inline">\(x^*\)</span> is 26 and other attributes that have been listed methodically. With some little extra Python codes, the user can also collect the data of the optimization steps, given below:</p>
<pre><code>## +----+----------+------------+
## |    |        x |       f(x) |
## |----+----------+------------|
## |  0 |  0       |   0        |
## |  1 |  1       |  -6.6875   |
## |  2 |  2.61803 | -16.5502   |
## |  3 | 15.287   | 120.093    |
## |  4 |  2.61803 | -16.5502   |
## |  5 |  7.45716 | -24.4179   |
## |  6 | 10.4479  |   0.756678 |
## |  7 |  5.60878 | -26.8316   |
## |  8 |  4.46642 | -24.5796   |
## |  9 |  6.3148  | -26.8866   |
## | 10 |  6.75114 | -26.3388   |
## | 11 |  6.04512 | -26.9977   |
## | 12 |  5.87846 | -26.9835   |
## | 13 |  6.14813 | -26.9751   |
## | 14 |  5.98146 | -26.9996   |
## | 15 |  5.94212 | -26.9962   |
## | 16 |  6.00578 | -27        |
## | 17 |  6.02081 | -26.9995   |
## | 18 |  5.99649 | -27        |
## | 19 |  5.99075 | -26.9999   |
## | 20 |  6.00004 | -27        |
## | 21 |  6.00223 | -27        |
## | 22 |  5.99868 | -27        |
## | 23 |  6.00088 | -27        |
## | 24 |  5.99952 | -27        |
## | 25 |  6.00036 | -27        |
## | 26 |  5.99984 | -27        |
## | 27 |  6.00016 | -27        |
## | 28 |  5.99996 | -27        |
## | 29 |  5.99992 | -27        |
## | 30 |  5.99999 | -27        |
## | 31 |  6.00001 | -27        |
## +----+----------+------------+</code></pre>
<p>The optimization steps can be plotted too. The graph with all the function evaluations along with the minimizer <span class="math inline">\(f(x^*)\)</span> at <span class="math inline">\(x^*\)</span> has been denoted as a blue dotted line in the below figure which can be generated using the following Python code:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="solving-one-dimensional-optimization-problems.html#cb38-1"></a>plt.plot(np.linspace(<span class="op">-</span><span class="dv">20</span>, <span class="dv">20</span>, <span class="dv">100</span>), f(np.linspace(<span class="op">-</span><span class="dv">20</span>, <span class="dv">20</span>, <span class="dv">100</span>)), <span class="st">&#39;r-&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="solving-one-dimensional-optimization-problems.html#cb39-1"></a>plt.plot(df[<span class="st">&#39;x&#39;</span>], df[<span class="st">&#39;f(x)&#39;</span>], <span class="st">&#39;ko-&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="solving-one-dimensional-optimization-problems.html#cb40-1"></a>plt.axvline(x<span class="op">=</span>result.x, color<span class="op">=</span><span class="st">&#39;b&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="solving-one-dimensional-optimization-problems.html#cb41-1"></a>plt.axhline(y<span class="op">=</span>result.fun, color<span class="op">=</span><span class="st">&#39;g&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="solving-one-dimensional-optimization-problems.html#cb42-1"></a>plt.xlabel(<span class="st">&#39;x -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="solving-one-dimensional-optimization-problems.html#cb43-1"></a>plt.ylabel(<span class="st">&#39;f(x) -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="solving-one-dimensional-optimization-problems.html#cb44-1"></a>plt.title(<span class="st">&quot;optimization of $f(x) = </span><span class="ch">\\</span><span class="st">frac</span><span class="sc">{1}{16}</span><span class="st">x^3 - </span><span class="ch">\\</span><span class="st">frac</span><span class="sc">{27}{4}</span><span class="st">x$&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-11.png" width="672" /></p>
<p>If we analyse closely the optimization data in the dataframe and look at the third, fourth and fifth steps, we see that <span class="math inline">\(f(x[3]) ~\sim -16.55\)</span>, <span class="math inline">\(f(x[4]) \sim 120.1\)</span> and again <span class="math inline">\(f(x[5]) ~\sim -16.55\)</span>. This interesting overshooting can be also seen in the visualization given by the above figure.</p>
<p>We next discuss <em>interpolation</em> methods to find the minimum of a nonlinear unimodal objective function. This methods use polynomial approximation for modeling the objective function. We will study two methods under this class of methods:</p>
<ul>
<li><strong>Powellâ€™s quadratic interpolation method</strong>, and</li>
<li><strong>Inverse quadratic interpolation method</strong></li>
</ul>
</div>
<div id="powells-quadratic-interpolation-method" class="section level2">
<h2><span class="header-section-number">3.5</span> Powellâ€™s Quadratic Interpolation Method</h2>
<p>Suppose, the objective function is <span class="math inline">\(f(x), x\in \mathbb{R}\)</span> and the minimizer is <span class="math inline">\(x^*\)</span>. Powellâ€™s method use successive quadratic interpolation curves for fitting to the objective function data. This gives a sequence of approximations to <span class="math inline">\(x^*\)</span>, denoted by <span class="math inline">\(x_t\)</span>.</p>
<p>Initially three data points <span class="math inline">\(x_0, x_1, x_2 \in \mathbb{R}\)</span> are provided. The interpolating quadratic polynomial through these data points <span class="math inline">\(P(x)\)</span> is as followed:
<span class="math display" id="eq:21">\[\begin{equation}
    P(x) = f(x_0) + (x - x_0)f[x_0, x_1] + (x-x_0)(x-x_1)f[x_0, x_1, x_2] \tag{3.21}
\end{equation}\]</span>
where,
<span class="math display" id="eq:22">\[\begin{equation}
    f[x, y] = \frac{f(y) - f(x)}{y - x} \tag{3.22}
\end{equation}\]</span>
is the first order forward divided difference, and
<span class="math display" id="eq:23">\[\begin{equation}
    f[x, y, z] = \frac{f[y, z] - f[x, y]}{z - x} \tag{3.23}
\end{equation}\]</span>
is the second order forward divided difference. <span class="math inline">\(x_t\)</span> is the point where the slope of <span class="math inline">\(P(x)\)</span> curve is <span class="math inline">\(0\)</span>. To find it, we set,
<span class="math display" id="eq:24">\[\begin{align}
&amp; \frac{dP(x)}{dx} = 0 \\
&amp;\Rightarrow f[x_0, x_1] + f[x_0, x_1, x_t](2x_t - x_0 - x_1) = 0 \tag{3.24}
\end{align}\]</span>
So we end up with <span class="math inline">\(x_t\)</span>,
<span class="math display" id="eq:25">\[\begin{equation}
    x_t = \frac{f[x_0, x_1, x_2](x_0, x_1) - f[x_0, x_1]}{2f[x_0, x_1, x_2]} \tag{3.25}
\end{equation}\]</span>
For <span class="math inline">\(x_t\)</span> to be minimum, the following condition regarding the second order forward divided difference should be satisfied,
<span class="math display" id="eq:26">\[\begin{equation}
    f[x_0, x_1, x_2] &gt; 0 \tag{3.26}
\end{equation}\]</span></p>
<p>We can now say that <span class="math inline">\(x_t\)</span> is a good approximation to <span class="math inline">\(x^*\)</span>. The algorithm for <em>Powellâ€™s quadratic interpolation method</em> is given below:</p>
<p><img src="img%205.png" /></p>

<div class="example">
<span id="exm:unnamed-chunk-19" class="example"><strong>Example 3.3  </strong></span>Let us consider an objective function:
<span class="math display" id="eq:27">\[\begin{equation}
    f(x) = x^4 - 2x^2 + \frac{1}{4} \tag{3.27}
\end{equation}\]</span>
We will use <em>Powellâ€™s quadratic interpolation method</em> to find out the minimizer <span class="math inline">\(x^*\)</span> and the function value at this point, <span class="math inline">\(f(x^*)\)</span>. Let the initial starting point be <span class="math inline">\(x=0.5\)</span>, the discrete step size be <span class="math inline">\(s=10^{-3}\)</span>, the maximum step size be <span class="math inline">\(m=30\)</span> and the tolerance be <span class="math inline">\(\epsilon = 10^{-5}\)</span>. To start with the optimization process, let us first define the objective function given by Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:27">(3.27)</a>in Python and plot the function:
</div>

<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="solving-one-dimensional-optimization-problems.html#cb45-1"></a><span class="kw">def</span> f(x): <span class="co"># define the objective function</span></span>
<span id="cb45-2"><a href="solving-one-dimensional-optimization-problems.html#cb45-2"></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">4</span> <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span><span class="op">/</span><span class="dv">4</span></span>
<span id="cb45-3"><a href="solving-one-dimensional-optimization-problems.html#cb45-3"></a>    </span>
<span id="cb45-4"><a href="solving-one-dimensional-optimization-problems.html#cb45-4"></a>l <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb45-5"><a href="solving-one-dimensional-optimization-problems.html#cb45-5"></a>plt.plot(l, f(l), <span class="st">&#39;r-&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="solving-one-dimensional-optimization-problems.html#cb46-1"></a>plt.xlabel(<span class="st">&#39;x -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="solving-one-dimensional-optimization-problems.html#cb47-1"></a>plt.ylabel(<span class="st">&#39;f(x) -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="solving-one-dimensional-optimization-problems.html#cb48-1"></a>plt.title(<span class="st">&quot;Graph of $f(x) =  x^4 - 2x^2 + </span><span class="ch">\\</span><span class="st">frac</span><span class="sc">{1}{4}</span><span class="st">$&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb49"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="solving-one-dimensional-optimization-problems.html#cb49-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-13.png" width="672" /></p>
<p>We will now write functions for the first order forward divided difference and the second order forward divided difference given by Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:22">(3.22)</a> and Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:22">(3.22)</a>.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="solving-one-dimensional-optimization-problems.html#cb50-1"></a><span class="kw">def</span> f1(x, y): <span class="co"># First order forward divided difference</span></span>
<span id="cb50-2"><a href="solving-one-dimensional-optimization-problems.html#cb50-2"></a>    <span class="cf">return</span> (f(y) <span class="op">-</span> f(x)) <span class="op">/</span> (y <span class="op">-</span> x)</span>
<span id="cb50-3"><a href="solving-one-dimensional-optimization-problems.html#cb50-3"></a></span>
<span id="cb50-4"><a href="solving-one-dimensional-optimization-problems.html#cb50-4"></a><span class="kw">def</span> f2(x, y, z): <span class="co"># Second order forward divided difference</span></span>
<span id="cb50-5"><a href="solving-one-dimensional-optimization-problems.html#cb50-5"></a>    <span class="cf">return</span> (f1(y, z) <span class="op">-</span> f1(x, y))<span class="op">/</span>(z <span class="op">-</span> x)</span></code></pre></div>
<p>Next, we write a function to find out the nearest value to a number <code>n</code> from a list, <code>seq</code> and a function to find out the furthest value to a number <code>n</code> from a list, <code>seq</code>.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="solving-one-dimensional-optimization-problems.html#cb51-1"></a><span class="kw">def</span> nearest_to(seq, n): <span class="co"># Picks the nearest value to a number entered from a list</span></span>
<span id="cb51-2"><a href="solving-one-dimensional-optimization-problems.html#cb51-2"></a>    <span class="cf">return</span> <span class="bu">min</span>(seq, key <span class="op">=</span> <span class="kw">lambda</span> x: <span class="bu">abs</span>(x <span class="op">-</span> n))</span>
<span id="cb51-3"><a href="solving-one-dimensional-optimization-problems.html#cb51-3"></a>                </span>
<span id="cb51-4"><a href="solving-one-dimensional-optimization-problems.html#cb51-4"></a><span class="kw">def</span> furthest_to(seq, n): <span class="co"># Picks the furthest value to a number entered from a list</span></span>
<span id="cb51-5"><a href="solving-one-dimensional-optimization-problems.html#cb51-5"></a>    <span class="cf">return</span> <span class="bu">max</span>(seq, key <span class="op">=</span> <span class="kw">lambda</span> x: <span class="bu">abs</span>(x <span class="op">-</span> n))</span></code></pre></div>
<p>Let us consider a list be <span class="math inline">\(L = \{1.1, 2.7, 3.3, 3.2, 1.8, -0.9, -0.5, -6.33\}\)</span> and a number be <span class="math inline">\(0.7\)</span>, we need to find the nearest value to <span class="math inline">\(0.7\)</span> from <span class="math inline">\(L\)</span> and the furthest value to <span class="math inline">\(0.7\)</span> from <span class="math inline">\(L\)</span>. We use the functions:</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="solving-one-dimensional-optimization-problems.html#cb52-1"></a>L <span class="op">=</span> [<span class="fl">1.1</span>, <span class="fl">2.7</span>, <span class="fl">3.3</span>, <span class="fl">3.2</span>, <span class="fl">1.8</span>, <span class="fl">-0.9</span>, <span class="fl">-0.5</span>, <span class="fl">-6.33</span>]</span>
<span id="cb52-2"><a href="solving-one-dimensional-optimization-problems.html#cb52-2"></a>n <span class="op">=</span> <span class="fl">0.7</span></span>
<span id="cb52-3"><a href="solving-one-dimensional-optimization-problems.html#cb52-3"></a></span>
<span id="cb52-4"><a href="solving-one-dimensional-optimization-problems.html#cb52-4"></a><span class="bu">print</span>(<span class="st">&quot;The nearest value to&quot;</span>, n, <span class="st">&quot;from &quot;</span>, L, <span class="st">&quot;:&quot;</span>, nearest_to(L, n))</span></code></pre></div>
<pre><code>## The nearest value to 0.7 from  [1.1, 2.7, 3.3, 3.2, 1.8, -0.9, -0.5, -6.33] : 1.1</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="solving-one-dimensional-optimization-problems.html#cb54-1"></a><span class="bu">print</span>(<span class="st">&quot;The furthest value to&quot;</span>, n, <span class="st">&quot;from &quot;</span>, L, <span class="st">&quot;:&quot;</span>, furthest_to(L, n))</span></code></pre></div>
<pre><code>## The furthest value to 0.7 from  [1.1, 2.7, 3.3, 3.2, 1.8, -0.9, -0.5, -6.33] : -6.33</code></pre>
<p>We now require to write a function that returns the element from a list that has the maximum value of <span class="math inline">\(f(x)\)</span> where <span class="math inline">\(f\)</span> is the objective function.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="solving-one-dimensional-optimization-problems.html#cb56-1"></a><span class="kw">def</span> maximum_fvalue(seq):</span>
<span id="cb56-2"><a href="solving-one-dimensional-optimization-problems.html#cb56-2"></a>    fu <span class="op">=</span> f(np.array(seq)) <span class="co"># Converts a Python list to a ndarray object </span></span>
<span id="cb56-3"><a href="solving-one-dimensional-optimization-problems.html#cb56-3"></a>    <span class="cf">return</span> seq[np.where(fu<span class="op">==</span>np.amax(fu))[<span class="dv">0</span>][<span class="dv">0</span>]] <span class="co"># Picks up the index from the ndarray sequence, the element at which has the maximum f(x) value and returns the element from the sequence</span></span></code></pre></div>
<p>Let us use the same sequence and find out the element from it that fas the maximum value of <span class="math inline">\(f\)</span>,</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="solving-one-dimensional-optimization-problems.html#cb57-1"></a>L <span class="op">=</span> [<span class="fl">1.1</span>, <span class="fl">2.7</span>, <span class="fl">3.3</span>, <span class="fl">3.2</span>, <span class="fl">1.8</span>, <span class="fl">-0.9</span>, <span class="fl">-0.5</span>, <span class="fl">-6.33</span>]</span>
<span id="cb57-2"><a href="solving-one-dimensional-optimization-problems.html#cb57-2"></a><span class="bu">print</span>(f(np.array(L))) <span class="co"># Prints the f(x)&#39;s at all the x&#39;s from the sequence </span></span></code></pre></div>
<pre><code>## [-7.05900000e-01  3.88141000e+01  9.70621000e+01  8.46276000e+01
##   4.26760000e+00 -7.13900000e-01 -1.87500000e-01  1.52562895e+03]</code></pre>
<div class="sourceCode" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="solving-one-dimensional-optimization-problems.html#cb59-1"></a><span class="bu">print</span>(maximum_fvalue(L)) <span class="co"># Prints the element with the highest f(x) value  </span></span></code></pre></div>
<pre><code>## -6.33</code></pre>
<p>We see that from the sequence <span class="math inline">\(-6.33\)</span> has the highest <span class="math inline">\(f\)</span> value, that is <span class="math inline">\(f(-6.33) = 1.52562895\times 10^3\)</span> is the maximum value as can be seen in the printed <code>ndarray</code>. We finally write the function that implements **Powellâ€™s Quadratic Interpolation Algorithm" and name it <code>powell_quad()</code> with the parameters <code>x, s, m,</code> and <code>tol</code>:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="solving-one-dimensional-optimization-problems.html#cb61-1"></a><span class="kw">def</span> powell_quad(x, s, m, tol):</span>
<span id="cb61-2"><a href="solving-one-dimensional-optimization-problems.html#cb61-2"></a>    <span class="cf">if</span> f(x) <span class="op">&lt;</span> f(x <span class="op">+</span> s):</span>
<span id="cb61-3"><a href="solving-one-dimensional-optimization-problems.html#cb61-3"></a>        x0 <span class="op">=</span> x <span class="op">-</span> s</span>
<span id="cb61-4"><a href="solving-one-dimensional-optimization-problems.html#cb61-4"></a>        x1 <span class="op">=</span> x</span>
<span id="cb61-5"><a href="solving-one-dimensional-optimization-problems.html#cb61-5"></a>        x2 <span class="op">=</span> x <span class="op">+</span> s</span>
<span id="cb61-6"><a href="solving-one-dimensional-optimization-problems.html#cb61-6"></a>    <span class="cf">else</span>:</span>
<span id="cb61-7"><a href="solving-one-dimensional-optimization-problems.html#cb61-7"></a>        x0 <span class="op">=</span> x</span>
<span id="cb61-8"><a href="solving-one-dimensional-optimization-problems.html#cb61-8"></a>        x1 <span class="op">=</span> x <span class="op">+</span> s</span>
<span id="cb61-9"><a href="solving-one-dimensional-optimization-problems.html#cb61-9"></a>        x2 <span class="op">=</span> x <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> s</span>
<span id="cb61-10"><a href="solving-one-dimensional-optimization-problems.html#cb61-10"></a></span>
<span id="cb61-11"><a href="solving-one-dimensional-optimization-problems.html#cb61-11"></a>    L <span class="op">=</span> [x0, x1, x2] <span class="co"># Set x0, x1 and x2</span></span>
<span id="cb61-12"><a href="solving-one-dimensional-optimization-problems.html#cb61-12"></a>    XT <span class="op">=</span> []  </span>
<span id="cb61-13"><a href="solving-one-dimensional-optimization-problems.html#cb61-13"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb61-14"><a href="solving-one-dimensional-optimization-problems.html#cb61-14"></a>        M <span class="op">=</span> f2(L[<span class="dv">0</span>], L[<span class="dv">1</span>], L[<span class="dv">2</span>])</span>
<span id="cb61-15"><a href="solving-one-dimensional-optimization-problems.html#cb61-15"></a>        xt <span class="op">=</span> (M <span class="op">*</span> (L[<span class="dv">0</span>] <span class="op">+</span> L[<span class="dv">1</span>]) <span class="op">-</span> f1(L[<span class="dv">0</span>], L[<span class="dv">1</span>]))<span class="op">/</span>(<span class="dv">2</span> <span class="op">*</span> M) <span class="co"># The approximate minimizer </span></span>
<span id="cb61-16"><a href="solving-one-dimensional-optimization-problems.html#cb61-16"></a>        xn <span class="op">=</span> nearest_to(L, xt) <span class="co"># Picks the point from [x0, x1, x2] which is the nearest to xt</span></span>
<span id="cb61-17"><a href="solving-one-dimensional-optimization-problems.html#cb61-17"></a>        xf <span class="op">=</span> furthest_to(L, xt) <span class="co"># Picks the point from [x0, x1, x2] which is the furthest to xt</span></span>
<span id="cb61-18"><a href="solving-one-dimensional-optimization-problems.html#cb61-18"></a>        <span class="cf">if</span> M <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> <span class="bu">abs</span>(xt <span class="op">-</span> xn) <span class="op">&gt;</span> m: <span class="co"># Checks for equation 3.26 </span></span>
<span id="cb61-19"><a href="solving-one-dimensional-optimization-problems.html#cb61-19"></a>            L.remove(xf) <span class="co"># Remove xf from [x0, x1, x2]</span></span>
<span id="cb61-20"><a href="solving-one-dimensional-optimization-problems.html#cb61-20"></a>            value <span class="op">=</span> <span class="bu">min</span>(L) <span class="op">+</span> m </span>
<span id="cb61-21"><a href="solving-one-dimensional-optimization-problems.html#cb61-21"></a>            L <span class="op">+=</span> [value, ]</span>
<span id="cb61-22"><a href="solving-one-dimensional-optimization-problems.html#cb61-22"></a>            L.sort() <span class="co"># Take a step of size m towards the direction of descent from the point with the lowest value</span></span>
<span id="cb61-23"><a href="solving-one-dimensional-optimization-problems.html#cb61-23"></a>        <span class="cf">elif</span> M <span class="op">&lt;</span> <span class="dv">0</span> :</span>
<span id="cb61-24"><a href="solving-one-dimensional-optimization-problems.html#cb61-24"></a>            L.remove(xn) <span class="co"># Remove xn from [x0, x1, x2]</span></span>
<span id="cb61-25"><a href="solving-one-dimensional-optimization-problems.html#cb61-25"></a>            value <span class="op">=</span> <span class="bu">min</span>(L) <span class="op">+</span> m</span>
<span id="cb61-26"><a href="solving-one-dimensional-optimization-problems.html#cb61-26"></a>            L <span class="op">+=</span> [value, ]</span>
<span id="cb61-27"><a href="solving-one-dimensional-optimization-problems.html#cb61-27"></a>            L.sort() <span class="co"># Take a step of size m towards the direction of descent from the point with the lowest value</span></span>
<span id="cb61-28"><a href="solving-one-dimensional-optimization-problems.html#cb61-28"></a>        <span class="cf">else</span>:</span>
<span id="cb61-29"><a href="solving-one-dimensional-optimization-problems.html#cb61-29"></a>            <span class="co"># print((xt + xn) / 2, f((xt + xn) / 2)) # If the user wants to print the steps at all function evaluations at the approximate minimizers, uncomment the command</span></span>
<span id="cb61-30"><a href="solving-one-dimensional-optimization-problems.html#cb61-30"></a>            XT <span class="op">+=</span> [(xt <span class="op">+</span> xn) <span class="op">/</span> <span class="dv">2</span>, ]</span>
<span id="cb61-31"><a href="solving-one-dimensional-optimization-problems.html#cb61-31"></a>            <span class="cf">if</span> <span class="bu">abs</span>(xt <span class="op">-</span> xn) <span class="op">&lt;</span> tol: <span class="co"># Check for the terminating condition</span></span>
<span id="cb61-32"><a href="solving-one-dimensional-optimization-problems.html#cb61-32"></a>                <span class="cf">return</span> [(xt <span class="op">+</span> xn) <span class="op">/</span> <span class="dv">2</span>, f((xt <span class="op">+</span> xn) <span class="op">/</span> <span class="dv">2</span>), XT] <span class="co"># Return the results</span></span>
<span id="cb61-33"><a href="solving-one-dimensional-optimization-problems.html#cb61-33"></a>            <span class="cf">else</span>:</span>
<span id="cb61-34"><a href="solving-one-dimensional-optimization-problems.html#cb61-34"></a>                mx <span class="op">=</span> maximum_fvalue(L) </span>
<span id="cb61-35"><a href="solving-one-dimensional-optimization-problems.html#cb61-35"></a>                L.remove(mx)</span>
<span id="cb61-36"><a href="solving-one-dimensional-optimization-problems.html#cb61-36"></a>                L <span class="op">+=</span> [xt, ]</span>
<span id="cb61-37"><a href="solving-one-dimensional-optimization-problems.html#cb61-37"></a>                L.sort() <span class="co"># Replace the element from [x0, x1, x2] having the maximum function value with xt</span></span></code></pre></div>
<p>Now returning back to our original example problem, we have <span class="math inline">\(x = 0.5\)</span>, <span class="math inline">\(s=10^{-3}\)</span>, <span class="math inline">\(m=30\)</span> and <span class="math inline">\(\epsilon=10^{-5}\)</span>. We will use these values as parameter values for our function <code>powell_quad()</code> and check the result:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="solving-one-dimensional-optimization-problems.html#cb62-1"></a>res <span class="op">=</span> powell_quad(<span class="fl">0.5</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">3</span>, <span class="dv">30</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>)</span>
<span id="cb62-2"><a href="solving-one-dimensional-optimization-problems.html#cb62-2"></a>x_star <span class="op">=</span> res[<span class="dv">0</span>]</span>
<span id="cb62-3"><a href="solving-one-dimensional-optimization-problems.html#cb62-3"></a>f_x_star <span class="op">=</span> res[<span class="dv">1</span>]</span>
<span id="cb62-4"><a href="solving-one-dimensional-optimization-problems.html#cb62-4"></a></span>
<span id="cb62-5"><a href="solving-one-dimensional-optimization-problems.html#cb62-5"></a><span class="bu">print</span>(<span class="st">&#39;x*:&#39;</span>, x_star)</span></code></pre></div>
<pre><code>## x*: 1.000000513307495</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="solving-one-dimensional-optimization-problems.html#cb64-1"></a><span class="bu">print</span>(<span class="st">&#39;f(x*):&#39;</span>, f_x_star)</span></code></pre></div>
<pre><code>## f(x*): -0.7499999999989462</code></pre>
<p>Now let us collect the optimization data and store in a dataframe <code>df</code>:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="solving-one-dimensional-optimization-problems.html#cb66-1"></a>XT <span class="op">=</span> np.array(res[<span class="dv">2</span>])</span>
<span id="cb66-2"><a href="solving-one-dimensional-optimization-problems.html#cb66-2"></a>F <span class="op">=</span> f(XT)</span>
<span id="cb66-3"><a href="solving-one-dimensional-optimization-problems.html#cb66-3"></a></span>
<span id="cb66-4"><a href="solving-one-dimensional-optimization-problems.html#cb66-4"></a>data <span class="op">=</span> {<span class="st">&#39;xt&#39;</span>: XT, <span class="st">&#39;f(xt)&#39;</span>: F}</span>
<span id="cb66-5"><a href="solving-one-dimensional-optimization-problems.html#cb66-5"></a>df <span class="op">=</span> pd.DataFrame(data, columns<span class="op">=</span>[<span class="st">&#39;xt&#39;</span>, <span class="st">&#39;f(xt)&#39;</span>])</span></code></pre></div>
<p>The last few rows of <code>df</code> looks like:</p>
<pre><code>## +-----+----------+-----------+
## |     |       xt |     f(xt) |
## |-----+----------+-----------|
## | 174 | 1.24152  | -0.456924 |
## | 175 | 0.77715  | -0.593154 |
## | 176 | 1.04659  | -0.740909 |
## | 177 | 0.896613 | -0.711551 |
## | 178 | 1.00525  | -0.749889 |
## | 179 | 0.978394 | -0.748173 |
## | 180 | 0.998358 | -0.749989 |
## | 181 | 0.999505 | -0.749999 |
## | 182 | 1.00004  | -0.75     |
## | 183 | 1        | -0.75     |
## +-----+----------+-----------+</code></pre>
<p>To plot the optimization data, type:</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="solving-one-dimensional-optimization-problems.html#cb68-1"></a>plt.plot(np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>), f(np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>)), <span class="st">&#39;r-&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb69"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="solving-one-dimensional-optimization-problems.html#cb69-1"></a>plt.plot(df[<span class="st">&#39;xt&#39;</span>], df[<span class="st">&#39;f(xt)&#39;</span>], <span class="st">&#39;ko-&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb70"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="solving-one-dimensional-optimization-problems.html#cb70-1"></a>plt.axvline(x<span class="op">=</span>x_star, color<span class="op">=</span><span class="st">&#39;b&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb71"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb71-1"><a href="solving-one-dimensional-optimization-problems.html#cb71-1"></a>plt.axhline(y<span class="op">=</span>f_x_star, color<span class="op">=</span><span class="st">&#39;g&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb72"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="solving-one-dimensional-optimization-problems.html#cb72-1"></a>plt.xlabel(<span class="st">&#39;x -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb73"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="solving-one-dimensional-optimization-problems.html#cb73-1"></a>plt.ylabel(<span class="st">&#39;f(x) -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb74"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="solving-one-dimensional-optimization-problems.html#cb74-1"></a>plt.xlim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span></code></pre></div>
<div class="sourceCode" id="cb75"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb75-1"><a href="solving-one-dimensional-optimization-problems.html#cb75-1"></a>plt.ylim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">10</span>)</span></code></pre></div>
<div class="sourceCode" id="cb76"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="solving-one-dimensional-optimization-problems.html#cb76-1"></a>plt.title(<span class="st">&quot;optimization of $f(x) =  x^4 - 2x^2 + </span><span class="ch">\\</span><span class="st">frac</span><span class="sc">{1}{4}</span><span class="st">$&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb77"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="solving-one-dimensional-optimization-problems.html#cb77-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-15.png" width="672" /></p>
<p>We notice that the blue dashed line gives the position of <span class="math inline">\(x^*\)</span> and the green dashed line gives the corresponding <span class="math inline">\(f(x^*)\)</span> value.</p>
<p>Now, we change our original problem a little bit and consider the initial starting point to be <span class="math inline">\(x=-0.5\)</span> and reduce the maximum step size to <span class="math inline">\(m=10\)</span> , keeping all the other parameters same.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="solving-one-dimensional-optimization-problems.html#cb78-1"></a>res <span class="op">=</span> powell_quad(<span class="op">-</span><span class="fl">0.5</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">3</span>, <span class="dv">10</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>)</span>
<span id="cb78-2"><a href="solving-one-dimensional-optimization-problems.html#cb78-2"></a>x_star <span class="op">=</span> res[<span class="dv">0</span>]</span>
<span id="cb78-3"><a href="solving-one-dimensional-optimization-problems.html#cb78-3"></a>f_x_star <span class="op">=</span> res[<span class="dv">1</span>]</span>
<span id="cb78-4"><a href="solving-one-dimensional-optimization-problems.html#cb78-4"></a></span>
<span id="cb78-5"><a href="solving-one-dimensional-optimization-problems.html#cb78-5"></a><span class="bu">print</span>(<span class="st">&#39;x*:&#39;</span>, x_star)</span></code></pre></div>
<pre><code>## x*: -1.0000000470794883</code></pre>
<div class="sourceCode" id="cb80"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb80-1"><a href="solving-one-dimensional-optimization-problems.html#cb80-1"></a><span class="bu">print</span>(<span class="st">&#39;f(x*):&#39;</span>, f_x_star)</span></code></pre></div>
<pre><code>## f(x*): -0.7499999999999911</code></pre>
<p>Now let us collect the optimization data and store in a dataframe <code>df</code>:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb82-1"><a href="solving-one-dimensional-optimization-problems.html#cb82-1"></a>XT <span class="op">=</span> np.array(res[<span class="dv">2</span>])</span>
<span id="cb82-2"><a href="solving-one-dimensional-optimization-problems.html#cb82-2"></a>F <span class="op">=</span> f(XT)</span>
<span id="cb82-3"><a href="solving-one-dimensional-optimization-problems.html#cb82-3"></a></span>
<span id="cb82-4"><a href="solving-one-dimensional-optimization-problems.html#cb82-4"></a>data <span class="op">=</span> {<span class="st">&#39;xt&#39;</span>: XT, <span class="st">&#39;f(xt)&#39;</span>: F}</span>
<span id="cb82-5"><a href="solving-one-dimensional-optimization-problems.html#cb82-5"></a>df <span class="op">=</span> pd.DataFrame(data, columns<span class="op">=</span>[<span class="st">&#39;xt&#39;</span>, <span class="st">&#39;f(xt)&#39;</span>])</span></code></pre></div>
<p>The last few rows of <code>df</code> looks like:</p>
<pre><code>## +----+-----------+-----------+
## |    |        xt |     f(xt) |
## |----+-----------+-----------|
## | 23 | -1.17087  | -0.612412 |
## | 24 | -0.807657 | -0.629111 |
## | 25 | -1.02543  | -0.747348 |
## | 26 | -0.924304 | -0.728783 |
## | 27 | -0.983006 | -0.748864 |
## | 28 | -0.995355 | -0.749914 |
## | 29 | -1.00137  | -0.749992 |
## | 30 | -1.00004  | -0.75     |
## | 31 | -0.999992 | -0.75     |
## | 32 | -1        | -0.75     |
## +----+-----------+-----------+</code></pre>
<p>To plot the optimization data, type:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb84-1"><a href="solving-one-dimensional-optimization-problems.html#cb84-1"></a>plt.plot(np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>), f(np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>)), <span class="st">&#39;r-&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb85"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb85-1"><a href="solving-one-dimensional-optimization-problems.html#cb85-1"></a>plt.plot(df[<span class="st">&#39;xt&#39;</span>], df[<span class="st">&#39;f(xt)&#39;</span>], <span class="st">&#39;ko-&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb86"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="solving-one-dimensional-optimization-problems.html#cb86-1"></a>plt.axvline(x<span class="op">=</span>x_star, color<span class="op">=</span><span class="st">&#39;b&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb87"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb87-1"><a href="solving-one-dimensional-optimization-problems.html#cb87-1"></a>plt.axhline(y<span class="op">=</span>f_x_star, color<span class="op">=</span><span class="st">&#39;g&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb88"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb88-1"><a href="solving-one-dimensional-optimization-problems.html#cb88-1"></a>plt.xlabel(<span class="st">&#39;x -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb89"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb89-1"><a href="solving-one-dimensional-optimization-problems.html#cb89-1"></a>plt.ylabel(<span class="st">&#39;f(x) -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb90"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb90-1"><a href="solving-one-dimensional-optimization-problems.html#cb90-1"></a>plt.xlim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span></code></pre></div>
<div class="sourceCode" id="cb91"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb91-1"><a href="solving-one-dimensional-optimization-problems.html#cb91-1"></a>plt.ylim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">10</span>)</span></code></pre></div>
<div class="sourceCode" id="cb92"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb92-1"><a href="solving-one-dimensional-optimization-problems.html#cb92-1"></a>plt.title(<span class="st">&quot;optimization of $f(x) =  x^4 - 2x^2 + </span><span class="ch">\\</span><span class="st">frac</span><span class="sc">{1}{4}</span><span class="st">$&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb93"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb93-1"><a href="solving-one-dimensional-optimization-problems.html#cb93-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-17.png" width="672" /></p>
<p>We have <span class="math inline">\(x^* \sim -1\)</span> and <span class="math inline">\(f(x^*) \sim -0.75\)</span>, i.e, now the <span class="math inline">\(x^*\)</span> converges to the local minimum on the left side.</p>
</div>
<div id="inverse-quadratic-interpolation-method" class="section level2">
<h2><span class="header-section-number">3.6</span> Inverse Quadratic Interpolation Method</h2>
<p>The main motivation of using the <strong>inverse quadratic interpolation method</strong> [refer to <em>An Introduction to Numerical Methods and Analysis</em> by <em>James F. Epperson</em>] is to use the quadratic interpolation method to find the inverse of the objective function <span class="math inline">\(f(x)\)</span>. This algorithm forms an integral part of the <em>Brentâ€™s method</em> for optimization, which we will study later in this chapter. In this method, we have three initial points to start with, given by, <span class="math inline">\(x_0, x_1, x_2 \in \mathbb{R}\)</span>. Our aim is to find the polynomial <span class="math inline">\(q(f(x))\)</span>, such that,
<span class="math display" id="eq:28">\[\begin{equation}
    q(f(x_j)) = x_j \tag{3.28}
\end{equation}\]</span>
which identifies,
<span class="math display" id="eq:29">\[\begin{equation}
    q = f^{-1} \tag{3.29}
\end{equation}\]</span></p>
<p>Now, using the divided difference forms we can write,
<span class="math display" id="eq:30">\[\begin{align}
    q(y) &amp;= f^{-1}(y_0) + (y - y_0)\frac{f^{-1}(y_1) - f^{-1}(y_0)}{y_1 - y_0}\\ 
    &amp;+ \frac{(y-y_0)(y-y_1)}{(y_2-y_0)}(\frac{f^{-1}(y_2) - f^{-1}(y_1)}{y_2 - y_1} - \frac{f^{-1}(y_1) - f^{-1}(y_0)}{y_1 - y_0})\\
    &amp;= x_0 + (y-y_0) \frac{(x-x_0}{(y_1-y_0)}\\ &amp;+\frac{(y-y_0)(y-y_1)}{(y_2-y_0)}(\frac{(x_2 - x_1)}{(y_2-y_1)} - \frac{(x_1-x_0)}{(y_1-y_0)}) \tag{3.30}
\end{align}\]</span></p>
<p>where,
<span class="math display" id="eq:31">\[\begin{equation}
    y_j = f(x_j) \tag{3.31}
\end{equation}\]</span></p>
<p>Now, the approximate minimizer <span class="math inline">\(x_t\)</span> is the value <span class="math inline">\(q(0)=x_t\)</span>, i.e, at <span class="math inline">\(y=0\)</span>. Putting <span class="math inline">\(y=0\)</span> in Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:30">(3.30)</a>, we have,</p>
<p><span class="math display" id="eq:32">\[\begin{equation}
    x_t = q(0) = x_0 - y_0 \frac{(x_1 - x_0)}{(y_1 - y_0)} + \frac{y_0y_1}{(y_2 - y_0)}(\frac{(x_2 - x_1)}{(y_2 - y_1)} - \frac{(x_1 - x_0)}{(y_1 - y_0)}) \tag{3.32}
\end{equation}\]</span></p>
<p><span class="math inline">\(x_t\)</span> can be considered as a good approximate of <span class="math inline">\(x^*\)</span> after satisfactory number of optimizations steps of the algorithm. The algorithm for <em>inverse quadratic interpolation method</em> is given below:</p>
<p><img src="img%206.png" /></p>
<p>The above alogorithm looks very simple and it will be left as an exercise for the reader to implement this using Python. An objective function <span class="math inline">\(f(x)\)</span> can be selected to test the results using this algorithm. Let
<span class="math display" id="eq:33">\[\begin{equation}
    f(x) = 2 - e^x \tag{3.33}
\end{equation}\]</span>
One can take the initial experimental point to be <span class="math inline">\(x=1\)</span>, the step size to be <span class="math inline">\(s=10^{-5}\)</span> and the tolerance to be <span class="math inline">\(\epsilon=10^{-8}\)</span>.
\end{example}</p>
<p>In the next section we discuss two direct root finding methods:</p>
<ul>
<li><strong>Newtonâ€™s method</strong>,</li>
<li><strong>Halleyâ€™s method</strong>,</li>
<li><strong>Secant Method</strong>, and</li>
<li><strong>Bisection Method</strong></li>
</ul>
</div>
<div id="newtons-method" class="section level2">
<h2><span class="header-section-number">3.7</span> Newtonâ€™s Method</h2>
<p>For a given objective function <span class="math inline">\(f(x), x\in \mathbb{R}\)</span>, the necessary condition for it to contain a minimizer <span class="math inline">\(x^*\)</span>, is that <span class="math inline">\(\frac{df}{dx}(x^*)=0\)</span>. The aim of these direct root-finding methods is thus to obtain the solution of the equation,
<span class="math display" id="eq:34">\[\begin{equation}
    \frac{df}{dx}(x)=0 \tag{3.34}
\end{equation}\]</span></p>
<p>At point <span class="math inline">\(x_j\)</span>, the Taylorâ€™s expansion of the objective function, up to the second order terms is given by,
<span class="math display" id="eq:35">\[\begin{equation}
    f(x) = f(x_j) + (x - x_j)\frac{df}{dx}(x_j) + \frac{1}{2}(x - x_j)^2\frac{d^2f}{dx^2}(x_j) \tag{3.35}
\end{equation}\]</span></p>
<p>Now,
<span class="math display" id="eq:36">\[\begin{equation}
    \frac{df}{dx}(x_j) = 0 \tag{3.36}
\end{equation}\]</span>
so,</p>
<p>Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:35">(3.35)</a> reduces to
<span class="math display" id="eq:37">\[\begin{equation}
    f(x) = f(x_j) + \frac{1}{2}(x - x_j)^2\frac{d^2f}{dx^2}(x_j) \tag{3.37}
\end{equation}\]</span></p>
<p>Here <span class="math inline">\(\frac{d^2f}{dx^2}(x_j)\)</span> is a constant. Now, we find the derivative of Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:37">(3.37)</a> and set it to <span class="math inline">\(0\)</span> following Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:34">(3.34)</a>
<span class="math display" id="eq:38">\[\begin{align}
\frac{df}{dx}(x) &amp;= 0 \\
\frac{df}{dx}(x_j) + (x - x_j)\frac{d^2f}{dx^2}(x_j) &amp;= 0 \tag{3.38}
\end{align}\]</span></p>
<p>We get,
<span class="math display" id="eq:39">\[\begin{equation}
    x = x_j - \frac{\frac{df}{dx}(x_j)}{\frac{d^2f}{dx^2}(x_j)} \tag{3.39}
\end{equation}\]</span></p>
<p><span class="math inline">\(x_j\)</span> denotes an approximation to the minimizer <span class="math inline">\(x^*\)</span> of <span class="math inline">\(f(x)\)</span>. An improved approximation in the form of an iterative process can be given using Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:39">(3.39)</a>,
<span class="math display" id="eq:40">\[\begin{equation}
    x_{j+1} = x_j - \frac{\frac{df}{dx}(x_j)}{\frac{d^2f}{dx^2}(x_j)} \tag{3.40}
\end{equation}\]</span></p>
<p>The termination condition for convergence after a sufficient number of large iterations to <span class="math inline">\(x^*\)</span> is given by:
<span class="math display" id="eq:41">\[\begin{equation}
    |\frac{df}{dx}(x_{j+1})| \leq \epsilon \tag{3.41}
\end{equation}\]</span></p>
<p>where, <span class="math inline">\(\epsilon\)</span> is the tolerance set by the user for the optimization algorithm. In numerical analysis literature, the <em>Newtonâ€™s method</em> is also sometimes called the <em>Newton-Raphson method</em>, because it was originally designed by Newton and was later improved by Raphson. The <em>Newtonâ€™s method</em> has a fast convergence property called the <em>quadratic convergence</em>.</p>

<div class="definition">
<span id="def:unnamed-chunk-35" class="definition"><strong>Definition 3.2  </strong></span>For a twice differentiable objective function <span class="math inline">\(f(x)\)</span>, assuming that its minimizer <span class="math inline">\(x^*\)</span> lies in the interval <span class="math inline">\((x_l, x_r)\)</span>, with <span class="math inline">\(\frac{df}{dx}(x^*) \neq 0\)</span> and <span class="math inline">\(f(x^*) = 0\)</span>, for the Newtonâ€™s iteration, if <span class="math inline">\(x_j\)</span> converges to <span class="math inline">\(x^*\)</span> for a large number of iterations, <span class="math inline">\(j \to \infty\)</span>, <span class="math inline">\(x_j\)</span> is said to be <em>quadratically convergent</em> to <span class="math inline">\(x^*\)</span> if the following condition is satisfied:
<span class="math display" id="eq:42">\[\begin{equation}
    |x_{j+1} - x^*| \leq M|x_j - x^*|^2, \text{ if } M &gt; \frac{|\frac{d^2f}{dx^2}(x^*)|}{2|\frac{df}{dx}(x^*)|} \tag{3.42}
\end{equation}\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> Let us consider
<span class="math display" id="eq:43">\[\begin{equation}
    \epsilon_j = x_j - x^* \tag{3.43}
\end{equation}\]</span>
Now, using Taylorâ€™s expansion up to the linear term, we can write,
<span class="math display" id="eq:44">\[\begin{equation}
    f(x) = f(x_j) + (x - x_j) \frac{df}{dx}(x_j) \tag{3.44}
\end{equation}\]</span>
As <span class="math inline">\(f(x) = 0\)</span>, we have,
<span class="math display" id="eq:45">\[\begin{eqnarray}
0 &amp;=&amp; f(x_j) + (x - x_j) \frac{df}{dx}(x_j) \nonumber \\
x &amp;=&amp; x_j - \frac{f(x_j)}{\frac{df}{dx}(x_j)} \tag{3.45}
\end{eqnarray}\]</span>
This can be written in term of an iteration method:
<span class="math display" id="eq:46">\[\begin{equation}
    x_{j+1} =  x_j - \frac{f(x_j)}{\frac{df}{dx}(x_j)} \tag{3.46}
\end{equation}\]</span>
Now using Taylorâ€™s expansion around the point <span class="math inline">\(x^*\)</span>,
<span class="math display" id="eq:47">\[\begin{equation}
    f(x^*) = f(x_j - \epsilon_j) = f(x_j) - \epsilon_j\frac{df}{dx}(x_j) + \frac{\epsilon_j^2}{2!}\frac{d^2f}{dx^2}(\beta_j) \tag{3.47}
\end{equation}\]</span>
where, <span class="math inline">\(\beta_j \in (x_j, x^*)\)</span>. Along with the fact <span class="math inline">\(f(x^*) = 0\)</span>, we use Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:43">(3.43)</a> in Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:44">(3.44)</a> and get,
<span class="math display" id="eq:48">\[\begin{equation}
    0 = f(x_j) - (x_j - x^*)\frac{df}{dx}(x_j) + \frac{\epsilon_j^2}{2}\frac{d^2f}{dx^2}(\beta_j) \tag{3.48}
\end{equation}\]</span>
As we know, <span class="math inline">\(\frac{df}{dx}\)</span> is continuous and <span class="math inline">\(\frac{df}{dx}(x^*) \neq 0\)</span>, the value <span class="math inline">\(\frac{df}{dx}(x_j) \neq 0\)</span> for those values of <span class="math inline">\(x_j\)</span> where <span class="math inline">\(x_j\)</span> is very close to <span class="math inline">\(x^*\)</span>. Dividing Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:45">(3.45)</a> with <span class="math inline">\(\frac{df}{dx}(x_j)\)</span>, we have,
<span class="math display" id="eq:49">\[\begin{equation}
    0 = \frac{f(x_j)}{\frac{df}{dx}(x_j)} - (x_j -  x^*) + \frac{\epsilon_j^2}{2}\frac{\frac{d^2f}{dx^2}(\beta_j)}{\frac{df}{dx}(x_j)} \tag{3.49}
\end{equation}\]</span>
Now using Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:46">(3.46)</a>,
<span class="math display" id="eq:50">\[\begin{eqnarray}
    0 &amp;=&amp; (x_j - x_{j+1}) - (x_j - x^*) + \frac{\epsilon_j^2}{2}\frac{\frac{d^2f}{dx^2}(\beta_j)}{\frac{df}{dx}(x_j)} \nonumber \\
    x_{j+1} - x^* &amp;=&amp;  \frac{\epsilon_j^2}{2}\frac{\frac{d^2f}{dx^2}(\beta_j)}{\frac{df}{dx}(x_j)} = \frac{1}{2}\frac{\frac{d^2f}{dx^2}(\beta_j)}{\frac{df}{dx}(x_j)}(x_j - x^*)^2 \nonumber \\
    |x_{j+1} - x_j| &amp;\leq&amp; \frac{1}{2}\frac{\frac{d^2f}{dx^2}(\beta_j)}{\frac{df}{dx}(x_j)}|x_j - x^*|^2 \tag{3.50}
\end{eqnarray}\]</span>
Now, <span class="math inline">\(\frac{df}{dx}(x_j)\)</span> converges to <span class="math inline">\(\frac{df}{dx}(x^*)\)</span> due to continuity of <span class="math inline">\(f(x)\)</span>. <span class="math inline">\(\beta_j\)</span>, lying between <span class="math inline">\(x_j\)</span> and <span class="math inline">\(x^*\)</span> converges to <span class="math inline">\(x^*\)</span> too, resulting in the convergence of <span class="math inline">\(\frac{d^2f}{dx^2}(\beta_j)\)</span> to <span class="math inline">\(\frac{d^2f}{dx^2}(x^*)\)</span>. So, As <span class="math inline">\(j \to \infty\)</span>, we have
<span class="math display" id="eq:51">\[\begin{equation}
    |x_{j+1} - x^*| \leq M|x_j - x^*|^2, \text{ if } M &gt; \frac{|\frac{d^2f}{dx^2}(x^*)|}{2|\frac{df}{dx}(x^*)|} \tag{3.51}
\end{equation}\]</span>
This completes our proof of <em>quadratic convergence</em> for the <em>Newtonâ€™s method</em>. [refer <em>quadratic convergence of Newtonâ€™s method</em>, Michael Overton]
</div>

<p>The algorithm for the <em>Newtonâ€™s method</em> is given below:</p>
<p><img src="img%207.png" /></p>

<div class="example">
<span id="exm:unnamed-chunk-37" class="example"><strong>Example 3.4  </strong></span>Let us consider the objective function,
<span class="math display" id="eq:52">\[\begin{equation}
    f(x) = x^3 + x^2 - 1 \tag{3.52}
\end{equation}\]</span>
We will use the <em>Newtonâ€™s method</em> to find the root <span class="math inline">\(x^*\)</span> of the objective function, along with the function value <span class="math inline">\(f(x^*)\)</span> at this point. Let the initial experimental point of the iteration be <span class="math inline">\(x_j = 2.0\)</span> and the tolerance be <span class="math inline">\(\epsilon = 10^{-6}\)</span>. In the algorithm aboave we see that we have to define two functions besides the objective function. They are <code>fprime()</code> for <span class="math inline">\(\frac{df}{dx}\)</span>, i.e, the first derivative and <code>fprime2()</code> for <span class="math inline">\(\frac{d^2f}{dx^2}\)</span>, i.e, the second derivative. We will use the <code>autograd</code> package we discussed before, to define the derivative functions. Let us start with defining the objective function first:
</div>

<div class="sourceCode" id="cb94"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb94-1"><a href="solving-one-dimensional-optimization-problems.html#cb94-1"></a><span class="kw">def</span> f(x): <span class="co"># objective function</span></span>
<span id="cb94-2"><a href="solving-one-dimensional-optimization-problems.html#cb94-2"></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">3</span> <span class="op">+</span> x<span class="op">**</span><span class="dv">2</span> <span class="dv">-1</span></span>
<span id="cb94-3"><a href="solving-one-dimensional-optimization-problems.html#cb94-3"></a></span>
<span id="cb94-4"><a href="solving-one-dimensional-optimization-problems.html#cb94-4"></a>l <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb94-5"><a href="solving-one-dimensional-optimization-problems.html#cb94-5"></a>plt.plot(l, f(l), <span class="st">&#39;r-&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb95"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb95-1"><a href="solving-one-dimensional-optimization-problems.html#cb95-1"></a>plt.xlabel(<span class="st">&#39;x -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb96"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb96-1"><a href="solving-one-dimensional-optimization-problems.html#cb96-1"></a>plt.ylabel(<span class="st">&#39;f(x) -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb97"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb97-1"><a href="solving-one-dimensional-optimization-problems.html#cb97-1"></a>plt.title(<span class="st">&quot;Graph of $f(x) =  x^3 + x^2 - 1$&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb98"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb98-1"><a href="solving-one-dimensional-optimization-problems.html#cb98-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-19.png" width="672" /></p>
<p>We now define the functions for the first derivative and the second derivative of <span class="math inline">\(f(x)\)</span>:</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb99-1"><a href="solving-one-dimensional-optimization-problems.html#cb99-1"></a>fprime <span class="op">=</span> grad(f) <span class="co"># first derivative of f(x)</span></span>
<span id="cb99-2"><a href="solving-one-dimensional-optimization-problems.html#cb99-2"></a>fprime2 <span class="op">=</span> grad(fprime) <span class="co"># second derivative of f(x)</span></span></code></pre></div>
<p>To find the value of these derivatives at a given point, we write our code in the following way:</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb100-1"><a href="solving-one-dimensional-optimization-problems.html#cb100-1"></a><span class="bu">print</span>(fprime(<span class="fl">0.</span>)) <span class="co"># first derivative of f(x) at point x=0</span></span></code></pre></div>
<pre><code>## 0.0</code></pre>
<div class="sourceCode" id="cb102"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb102-1"><a href="solving-one-dimensional-optimization-problems.html#cb102-1"></a><span class="bu">print</span>(fprime2(<span class="fl">0.75</span>)) <span class="co"># second derivative of f(x) at point 0.75</span></span></code></pre></div>
<pre><code>## 6.5</code></pre>
<p>Now, to implement <em>Newtonâ€™s method</em>, we will first introduce the <code>root_scalar()</code> function of the <code>scipy.optimize</code> package, which is used for direct root finding for scalar nonlinear objective functions. We will now discuss the parameters of the <code>root_scalar()</code> function:</p>
<ul>
<li><code>f</code>: This is the objective function whose root we need to find, and should be a <code>callable</code> Python data type. It can also be a function returning the objective function and its derivatives.</li>
<li><code>x0</code>: This is the initial experimental point of the optimization process. This is an optional parameter and is a <code>float</code>.</li>
<li><code>x1</code>: This is the second experimental point of the optimization process. This is an optional parameter and is a <code>float</code> too.</li>
<li><code>fprime</code>: This is an optional parameter and can be of two data types:
<ul>
<li><code>callable</code>: If <code>fprime</code> is a <code>callable</code> function, it should return the first derivative of the objective function, and must accept the same arguments as the objective function,</li>
<li><code>bool</code>: If <code>fprime</code> is a Boolean data type, <code>f</code> should return the objective function and its first derivative.</li>
</ul></li>
<li><code>fprime2</code>:This is an optional parameter and can be of two data types:
<ul>
<li><code>callable</code>: If <code>fprime2</code> is a <code>callable</code> function, it should return the second derivative of the objective function, and must accept the same arguments as the objective function,</li>
<li><code>bool</code>: If <code>fprime</code> is a Boolean data type, <code>f</code> should return the objective function and both its first and second derivatives.</li>
</ul></li>
<li><code>xtol</code>: This is the absolute tolerance value for termination of the optimization process. This is an optional parameter and is a <code>float</code>.</li>
<li><code>rtol</code>: This is the relative tolerance value for termination of the optimization process. This is an optional parameter and is a <code>float</code>.</li>
<li><code>maxiter</code>: This indicates the maximum number of of iterations the optimization process should run before termination. This is an optional parameter and is an <code>int</code>,</li>
<li><code>args</code>: This is a <code>tuple</code> of the extra arguments that can be passed to the objective function and its derivatives. This is an optional parameter too,</li>
<li><code>bracket</code>: This is an optional parameter which indicates an interval that brackets the root of the objective function and is a sequence of two floats,</li>
<li><code>method</code>: Although an optional parameter, the <code>method</code> parameter is one of the most important parameters that indicate the specific solver that we are going to use for our optimization task. This is a <code>str</code> data type and has the following solvers predefined in it:
<ul>
<li><code>'newton'</code>: This is <em>Newtonâ€™s method</em> for root finding,</li>
<li><code>'halley'</code>: This is <em>Halleyâ€™s method</em> for root finding,</li>
<li><code>'secant'</code>: This is the <em>Secant method</em> for root finding,</li>
<li><code>'bisect'</code>: This is the <em>bisection method</em> for root finding,</li>
<li><code>'brentq'</code>: This is the method to find the root of an objective function <span class="math inline">\(f(x)\)</span> using the classic <em>Brentâ€™s method</em>, in a bracketing interval,</li>
<li><code>'brenth'</code>: This is used for finding the root of an objective function in a bracketing interval using <em>Brentâ€™s method</em> with hyperbolic extrapolation,</li>
<li><code>'ridder'</code>: This is the <em>Ridderâ€™s method</em> of root finding, and</li>
<li><code>'toms748'</code>: This is the implementation of the root finding algorithm designed by Alefed, Potro and Shi , called <em>Algorithm 748</em>.</li>
</ul></li>
<li><code>options</code>: This is an optional parameter and is a Python dictionary which specifies the solver options given by the following parameters:
<ul>
<li><code>solver</code>: This is a <code>str</code> and specifies the type of optimization solver, for example, <code>'minimize_scalar'</code>, <code>'root_scalar'</code>, etc.</li>
<li><code>method</code>: This is a <code>str</code> and is an optional parameter. If not provided, it shows all the methods available for a selected solver,</li>
<li><code>disp</code>: This is a <code>bool</code> and is an optional parameter too. If selected <code>True</code>, all the results are printed instead of returning.</li>
</ul></li>
</ul>
<p>The <code>root_scalar()</code> function returns the solution as <code>RootResults</code> object. The important attributes of the <code>RootResults</code> object is defined as:</p>
<ul>
<li><code>root</code>: This is the approximate value of the root of the objective function and is a <code>float</code>,</li>
<li><code>iterations</code>: This indicates the number of iterations the optimization process takes to determine the root of the objective function and is an <code>int</code>,</li>
<li><code>function_calls</code>: This indicates the total number of function evaluations during the optimization process and is an <code>int</code>,</li>
<li><code>converged</code>: This is a <code>bool</code> and denotes whether the optimization routine has converged or not,</li>
<li><code>flag</code>: This is a <code>str</code> and gives a description of the cause of termination of the optimization process.</li>
</ul>
<p>Now, returning back to the original problem, where we intend to find the root of the objective function given by Eq.<a href="solving-one-dimensional-optimization-problems.html#eq:52">(3.52)</a>, using the <em>Newtonâ€™s method</em>, and using the given value of the initial experimental point and the tolerance, we write the following Python code:</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb104-1"><a href="solving-one-dimensional-optimization-problems.html#cb104-1"></a><span class="im">from</span> scipy <span class="im">import</span> optimize</span>
<span id="cb104-2"><a href="solving-one-dimensional-optimization-problems.html#cb104-2"></a><span class="kw">def</span> FP(x): <span class="co"># Returns the objective function, its first and second derivative at the point x</span></span>
<span id="cb104-3"><a href="solving-one-dimensional-optimization-problems.html#cb104-3"></a>    <span class="cf">return</span> f(x), fprime(x), fprime2(x)</span>
<span id="cb104-4"><a href="solving-one-dimensional-optimization-problems.html#cb104-4"></a></span>
<span id="cb104-5"><a href="solving-one-dimensional-optimization-problems.html#cb104-5"></a>sol <span class="op">=</span> optimize.root_scalar(FP, x0<span class="op">=</span><span class="fl">2.</span>, fprime<span class="op">=</span><span class="va">True</span>, fprime2<span class="op">=</span><span class="va">True</span>, xtol<span class="op">=</span><span class="dv">10</span><span class="op">**-</span><span class="dv">6</span>, method<span class="op">=</span><span class="st">&#39;newton&#39;</span>) <span class="co"># The solver code, using the method = &#39;newton&#39;</span></span>
<span id="cb104-6"><a href="solving-one-dimensional-optimization-problems.html#cb104-6"></a><span class="bu">print</span>(sol) <span class="co"># Prints the results</span></span></code></pre></div>
<pre><code>##       converged: True
##            flag: &#39;converged&#39;
##  function_calls: 6
##      iterations: 6
##            root: 0.7548776662468297</code></pre>
<p>We see the root is approximately <span class="math inline">\(0.75\)</span>, the convergence was successful and there have been <span class="math inline">\(6\)</span> iterations in the optimization process and the function has been evaluated <span class="math inline">\(6\)</span> times too. The optimization data has been collected and is shown below:</p>
<pre><code>## +----+----------+--------------+
## |    |        x |         f(x) |
## |----+----------+--------------|
## |  0 | 2        | 11           |
## |  1 | 1.3125   |  2.98364     |
## |  2 | 0.929637 |  0.667639    |
## |  3 | 0.779671 |  0.0818387   |
## |  4 | 0.75548  |  0.00193987  |
## |  5 | 0.754878 |  1.18294e-06 |
## +----+----------+--------------+</code></pre>
<p>The optimization steps can be plotted too. The graph with all the function evaluations along with the minimizer <span class="math inline">\(f(x^*)\)</span> at <span class="math inline">\(x^*\)</span> has been denoted as a blue dotted line in the below figure which can be generated using the following Python code:</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb107-1"><a href="solving-one-dimensional-optimization-problems.html#cb107-1"></a>plt.plot(np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>), f(np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>)), <span class="st">&#39;r-&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb108"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb108-1"><a href="solving-one-dimensional-optimization-problems.html#cb108-1"></a>plt.plot(df[<span class="st">&#39;x&#39;</span>], df[<span class="st">&#39;f(x)&#39;</span>], <span class="st">&#39;ko-&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb109"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb109-1"><a href="solving-one-dimensional-optimization-problems.html#cb109-1"></a>plt.axvline(x<span class="op">=</span>sol.root, color<span class="op">=</span><span class="st">&#39;b&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb110"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb110-1"><a href="solving-one-dimensional-optimization-problems.html#cb110-1"></a>plt.axhline(y<span class="op">=</span>f(sol.root), color<span class="op">=</span><span class="st">&#39;g&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb111"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb111-1"><a href="solving-one-dimensional-optimization-problems.html#cb111-1"></a>plt.xlabel(<span class="st">&#39;x -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb112"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb112-1"><a href="solving-one-dimensional-optimization-problems.html#cb112-1"></a>plt.ylabel(<span class="st">&#39;f(x) -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb113"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb113-1"><a href="solving-one-dimensional-optimization-problems.html#cb113-1"></a>plt.title(<span class="st">&quot;optimization of $f(x) = x^3 + x^2 - 1$&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-21.png" width="672" /></p>
</div>
<div id="halleys-method" class="section level2">
<h2><span class="header-section-number">3.8</span> Halleyâ€™s Method</h2>
<p>For the objective function <span class="math inline">\(f(x), x\in \mathbb{R}\)</span>, the necessary condition that the minimizer <span class="math inline">\(x^*\)</span> exists is that <span class="math inline">\(\frac{df}{dx}(x^*)=0\)</span>. At point <span class="math inline">\(x_j\)</span>, the Taylorâ€™s expansion of the objective function upto the second order term is given by Eq. <a href="solving-one-dimensional-optimization-problems.html#eq:35">(3.35)</a>. A root of <span class="math inline">\(f(x)\)</span> satisfies <span class="math inline">\(f(x)= 0\)</span>, so , we will have,</p>
<p><span class="math display" id="eq:53">\[\begin{align}
    0 &amp;= f(x_j) + (x - x_j)\frac{df}{dx}(x_j) + \frac{1}{2}(x - x_j)^2\frac{s^2f}{dx^2}(x_j) \nonumber \\
    &amp;= f(x_j) + (x - x_j)[\frac{df}{dx}(x_j) +\frac{1}{2} (x - x_j)\frac{d^2f}{dx^2}(x_j)] \tag{3.53}
\end{align}\]</span></p>
<p>This gives,
<span class="math display" id="eq:54">\[\begin{equation}
    x - x_j = -\frac{f(x_j)}{\frac{df}{dx}(x_j) + \frac{1}{2}(x - x_j)\frac{d^2f}{dx^2}(x_j)} \tag{3.54}
\end{equation}\]</span></p>
<p>Now, using the fact from <em>Newtonâ€™s iteration</em> that,</p>
<p><span class="math display">\[\begin{equation}
    x - x_j = -\frac{f(x_j)}{\frac{df}{dx}(x_j)} \nonumber
\end{equation}\]</span>
we can re-write Eq. <a href="solving-one-dimensional-optimization-problems.html#eq:54">(3.54)</a> as,</p>
<p><span class="math display" id="eq:55">\[\begin{align}
    x &amp;= x_j - \frac{f(x_j)}{\frac{df}{dx}(x_j) - \frac{1}{2}\frac{f(x_j) \frac{d^2f}{dx^2}(x_j)}{\frac{df}{dx}(x_j)}} \nonumber \\
    &amp;= x_j - \frac{2f(x_j)\frac{df}{dx}(x_j)}{2[\frac{df}{dx}(x_j)]^2 - f(x_j)\frac{d^2f}{dx^2}(x_j)} \tag{3.55}
\end{align}\]</span></p>
<p>The iterative method [ref, wolfram mathworld, Halleyâ€™s method] can be thus written as
<span class="math display" id="eq:56">\[\begin{equation}
    x_{j+1} = x_j - \frac{2f(x_j)\frac{df}{dx}(x_j)}{2[\frac{df}{dx}(x_j)]^2 - f(x_j)\frac{d^2f}{dx^2}(x_j)} \tag{3.56}
\end{equation}\]</span></p>
<p>The termination condition for convergence after a sufficient number of large iterations to <span class="math inline">\(x^*\)</span> is given by:
<span class="math display" id="eq:57">\[\begin{equation}
    |\frac{df}{dx}(x_{j+1})| \leq \epsilon \tag{3.57}
\end{equation}\]</span>
where, <span class="math inline">\(\epsilon\)</span> is the tolerance set by the user for the optimization algorithm. The <em>Halleyâ€™s method</em> has a fast convergence property called the <em>cubic convergence</em>.</p>

<div class="definition">
<span id="def:unnamed-chunk-45" class="definition"><strong>Definition 3.3  </strong></span>For a thrice differentiable objective function <span class="math inline">\(f(x)\)</span>, assuming that its minimizer <span class="math inline">\(x^*\)</span> lies in the interval <span class="math inline">\((x_l, x_r)\)</span>, with <span class="math inline">\(\frac{df}{dx}(x^*) \neq 0\)</span> and <span class="math inline">\(f(x^*) = 0\)</span>, for the Halleyâ€™s iteration, if <span class="math inline">\(x_j\)</span> converges to <span class="math inline">\(x^*\)</span> for a large number of iterations, <span class="math inline">\(j \to \infty\)</span>, <span class="math inline">\(x_j\)</span> is said to show  to <span class="math inline">\(x^*\)</span> if the following condition is satisfied:
<span class="math display" id="eq:58">\[\begin{align}
    |x_{j+1} - x^*| \leq K|x_j - x^*|^3, \text{ if } K &gt; -\frac{2\frac{df}{dx}(x_j)\frac{d^3f}{dx^3}(\alpha) - 3\frac{d^2f}{dx^2}(x_j)\frac{d^2f}{dx^2}(\beta)}{6(2[\frac{df}{dx}(x_j)]^2 - f(x_j)\frac{d^2f}{dx^2}(x_j))} \tag{3.58}
\end{align}\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Assuming that the third derivative of the objective function exists and is continuous in a neighborhood of <span class="math inline">\(x^*\)</span> and <span class="math inline">\(x_j\)</span> lies in that neighborhood, using the Taylorâ€™s expansion up to the third order term, we get,
<span class="math display" id="eq:59">\[\begin{align}
    &amp; &amp;f(x^*) = f(x_j) + (x^* - x_j)\frac{df}{dx}(x_j) + \frac{1}{2!}(x^* - x_j)^2\frac{d^2f}{dx^2}(x_j)\nonumber \\ &amp; &amp;+ \frac{1}{3!}(x^* - x_j)^3\frac{d^3f}{dx^3}(\alpha) \nonumber \\
    &amp;\implies&amp; 0 = f(x_j) + (x^* - x_j)\frac{df}{dx}(x_j) + \frac{1}{2}(x^* - x_j)^2\frac{d^2f}{dx^2}(x_j)\nonumber \\ &amp; &amp;+ \frac{1}{6}(x^* - x_j)^3\frac{d^3f}{dx^3}(\alpha) \tag{3.59}
\end{align}\]</span>
And also, using Taylorâ€™s theorem up to second order terms,
<span class="math display" id="eq:60">\[\begin{align}
    &amp; &amp;f(x^*) = f(x_j) + (x^* - x_j)\frac{df}{dx}(x_j) + \frac{1}{2!}(x^* - x_j)^2\frac{d^2f}{dx^2}(\beta) \nonumber \\
    &amp;\implies&amp;f(x^*) = f(x_j) + (x^* - x_j)\frac{df}{dx}(x_j) + \frac{1}{2}(x^* - x_j)^2\frac{d^2f}{dx^2}(\beta) \tag{3.60}
\end{align}\]</span>
Here, <span class="math inline">\(\alpha, \beta \in [x^*, x_j]\)</span>. Now, (Eq. <a href="solving-one-dimensional-optimization-problems.html#eq:59">(3.59)</a> <span class="math inline">\(\times 2 \frac{df}{dx}(x_j) -\)</span> (Eq. <a href="solving-one-dimensional-optimization-problems.html#eq:60">(3.60)</a>) <span class="math inline">\(\times (x^* - x_j)\frac{d^2f}{dx^2}(x_j)\)</span> gives,</p>
<p><span class="math display" id="eq:61">\[\begin{align}
0 &amp;=&amp; 2f(x_j)\frac{df}{dx}(x_j) + 2(x^* - x_j)[\frac{df}{dx}(x_j)]^2 + \frac{1}{3}(x^* - x_j)^3\frac{df}{dx}(x_j)\frac{d^3f}{dx^3}(\alpha) \nonumber \\
&amp;-&amp; (x^*-x_j)f(x_j)\frac{d^2f}{dx^2}(x_j) - \frac{1}{2}(x^* - x_J)^3\frac{d^2f}{dx^2}(x_j)\frac{d^2f}{dx^2}(\beta) \nonumber \\
&amp;=&amp; 2f(x_j)\frac{df}{dx}(x_j) + (x^* - x_j)(2[\frac{df}{dx}(x_j)]^2 - f(x_j)\frac{d^2f}{dx^2}(x_j)) \nonumber \\
&amp;+&amp; (x^* - x_J)^3(\frac{1}{3}\frac{df}{dx}(x_j)\frac{d^3f}{dx^3}(\alpha) - \frac{1}{2}\frac{d^2f}{dx^2}(x_j)\frac{d^2f}{dx^2}(\beta)) \tag{3.61}
\end{align}\]</span></p>
<p>Now, dividing Eq. <a href="solving-one-dimensional-optimization-problems.html#eq:61">(3.61)</a> with <span class="math inline">\((2[\frac{df}{dx}(x_j)]^2 - f(x_j)\frac{d^2f}{dx^2}(x_j))\)</span>, we have,
<span class="math display" id="eq:62">\[\begin{align}
x^* - x_j &amp;=&amp; -\frac{2f(x_j)\frac{df}{dx}(x_j)}{2[\frac{df}{dx}(x_j)]^2 - f(x_j)\frac{d^2f}{dx^2}(x_j)} \nonumber \\ &amp;-&amp; (x^* - x_j)^3 \frac{2\frac{df}{dx}(x_j)\frac{d^3f}{dx^3}(\alpha) - 3\frac{d^2f}{dx^2}(x_j)\frac{d^2f}{dx^2}(\beta)}{6(2[\frac{df}{dx}(x_j)]^2 - f(x_j)\frac{d^2f}{dx^2}(x_j))}\tag{3.62}
\end{align}\]</span></p>
<p>Now subtracting Eq. <a href="solving-one-dimensional-optimization-problems.html#eq:56">(3.56)</a> from Eq. <a href="solving-one-dimensional-optimization-problems.html#eq:62">(3.62)</a>, we have,
<span class="math display" id="eq:63">\[\begin{align}
x^* - x_{j+1} = - (x^* - x_j)^3 \frac{2\frac{df}{dx}(x_j)\frac{d^3f}{dx^3}(\alpha) - 3\frac{d^2f}{dx^2}(x_j)\frac{d^2f}{dx^2}(\beta)}{6(2[\frac{df}{dx}(x_j)]^2 - f(x_j)\frac{d^2f}{dx^2}(x_j))} \tag{3.63}
\end{align}\]</span></p>
<p>which can be written as,
<span class="math display" id="eq:64">\[\begin{align}
&amp; &amp;x_{j+1} - x^* = - (x_j - x^*)^3 \frac{2\frac{df}{dx}(x_j)\frac{d^3f}{dx^3}(\alpha) - 3\frac{d^2f}{dx^2}(x_j)\frac{d^2f}{dx^2}(\beta)}{6(2[\frac{df}{dx}(x_j)]^2 - f(x_j)\frac{d^2f}{dx^2}(x_j))} \nonumber \\
&amp; &amp;|x_{j+1} - x^*| \leq -\frac{2\frac{df}{dx}(x_j)\frac{d^3f}{dx^3}(\alpha) - 3\frac{d^2f}{dx^2}(x_j)\frac{d^2f}{dx^2}(\beta)}{6(2[\frac{df}{dx}(x_j)]^2 - f(x_j)\frac{d^2f}{dx^2}(x_j))} |(x_j - x^*)|^3 \tag{3.64}
\end{align}\]</span></p>
<p>Now, <span class="math inline">\(\frac{df}{dx}(x_j)\)</span> converges to <span class="math inline">\(\frac{df}{dx}(x^*)\)</span> and <span class="math inline">\(\frac{d^2f}{dx^2}(x_j)\)</span> converges to <span class="math inline">\(\frac{d^2f}{dx^2}(x^*)\)</span> due to continuity of <span class="math inline">\(f(x)\)</span>. <span class="math inline">\(\beta, \alpha\)</span>, lying between <span class="math inline">\(x_j\)</span> and <span class="math inline">\(x^*\)</span> converges to <span class="math inline">\(x^*\)</span> too, resulting in the convergence of <span class="math inline">\(\frac{d^2f}{dx^2}(\beta)\)</span> to <span class="math inline">\(\frac{d^2f}{dx^2}(x^*)\)</span> and <span class="math inline">\(\frac{d^3f}{dx^3}(\alpha)\)</span> to <span class="math inline">\(\frac{d^3f}{dx^3}(x^*)\)</span>. So, As <span class="math inline">\(j \to \infty\)</span>, we have</p>
<span class="math display" id="eq:65">\[\begin{equation}
    |x_{j+1} - x^*| \leq K |x_j - x^*|^3, \text{ if }K &gt; -\frac{2\frac{df}{dx}(x_j)\frac{d^3f}{dx^3}(\alpha) - 3\frac{d^2f}{dx^2}(x_j)\frac{d^2f}{dx^2}(\beta)}{6(2[\frac{df}{dx}(x_j)]^2 - f(x_j)\frac{d^2f}{dx^2}(x_j))} \tag{3.65}
\end{equation}\]</span>
This completes our proof of <em>cubic convergence</em> for <em>Halleyâ€™s method</em>.
</div>

<p>The algorithm of the <em>Halleyâ€™s method</em> is given below:</p>
<p><img src="img%208.png" /></p>

<div class="example">
<span id="exm:unnamed-chunk-47" class="example"><strong>Example 3.5  </strong></span>Let us consider the objective function,
<span class="math display" id="eq:66">\[\begin{equation}
    f(x) = x^3 -6x^2+11x-6 \tag{3.66}
\end{equation}\]</span>
We will use the  to find the root <span class="math inline">\(x^*\)</span> of the objective function, along with the function value <span class="math inline">\(f(x^*)\)</span> at this point. Let the initial experimental point of the iteration be <span class="math inline">\(x_0 = 3.7\)</span> and the tolerance be <span class="math inline">\(\epsilon = 10^{-6}\)</span>. We define the objective function, and its derivatives in the similar way in Python as we did for the <em>Newtonâ€™s method</em>
</div>

<div class="sourceCode" id="cb114"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb114-1"><a href="solving-one-dimensional-optimization-problems.html#cb114-1"></a><span class="kw">def</span> f(x): <span class="co"># Objective function</span></span>
<span id="cb114-2"><a href="solving-one-dimensional-optimization-problems.html#cb114-2"></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">3</span> <span class="op">-</span> <span class="dv">6</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">11</span><span class="op">*</span>x <span class="op">-</span> <span class="dv">6</span></span>
<span id="cb114-3"><a href="solving-one-dimensional-optimization-problems.html#cb114-3"></a></span>
<span id="cb114-4"><a href="solving-one-dimensional-optimization-problems.html#cb114-4"></a>fprime <span class="op">=</span> grad(f) <span class="co"># First derivative of the objective function</span></span>
<span id="cb114-5"><a href="solving-one-dimensional-optimization-problems.html#cb114-5"></a>fprime2 <span class="op">=</span> grad(fprime) <span class="co"># Second derivative of the objective function </span></span></code></pre></div>
<p>Next, we use the <code>root_scalar()</code> function of the <code>scipy.optimize</code> module and set the attributes <code>x0=3.7</code>, <code>method = 'halley'</code>, <code>xtol=10**-6</code> and so on, according to our need.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb115-1"><a href="solving-one-dimensional-optimization-problems.html#cb115-1"></a><span class="kw">def</span> FP(x): <span class="co"># Returns the objective function and its derivatives</span></span>
<span id="cb115-2"><a href="solving-one-dimensional-optimization-problems.html#cb115-2"></a>    <span class="cf">return</span> f(x), fprime(x), fprime2(x)</span>
<span id="cb115-3"><a href="solving-one-dimensional-optimization-problems.html#cb115-3"></a></span>
<span id="cb115-4"><a href="solving-one-dimensional-optimization-problems.html#cb115-4"></a>sol <span class="op">=</span> optimize.root_scalar(FP, x0<span class="op">=</span><span class="fl">3.7</span>, fprime<span class="op">=</span><span class="va">True</span>, fprime2<span class="op">=</span><span class="va">True</span>, xtol<span class="op">=</span><span class="dv">10</span><span class="op">**-</span><span class="dv">6</span>, method<span class="op">=</span><span class="st">&#39;halley&#39;</span>)</span>
<span id="cb115-5"><a href="solving-one-dimensional-optimization-problems.html#cb115-5"></a><span class="bu">print</span>(sol)</span></code></pre></div>
<pre><code>##       converged: True
##            flag: &#39;converged&#39;
##  function_calls: 4
##      iterations: 4
##            root: 2.9999999999999996</code></pre>
<p>where, we notice that the root is <span class="math inline">\(\sim 3\)</span>. The optimization data has been collected and is shown below:</p>
<pre><code>## +----+---------+-------------+
## |    |       x |        f(x) |
## |----+---------+-------------|
## |  0 | 3.7     | 3.213       |
## |  1 | 3.11936 | 0.283171    |
## |  2 | 3.00207 | 0.00415786  |
## |  3 | 3       | 3.09493e-08 |
## +----+---------+-------------+</code></pre>
<p>The plot of the optimization steps showing the convergence of the root, along with the root being denoted by the vertical broken blue line and its corresponding function value denoted by horizontal broken green line has been shown below:</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb118-1"><a href="solving-one-dimensional-optimization-problems.html#cb118-1"></a>plt.plot(np.linspace(<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">100</span>), f(np.linspace(<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">100</span>)), <span class="st">&#39;r-&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb119"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb119-1"><a href="solving-one-dimensional-optimization-problems.html#cb119-1"></a>plt.plot(df[<span class="st">&#39;x&#39;</span>], df[<span class="st">&#39;f(x)&#39;</span>], <span class="st">&#39;ko-&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb120"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb120-1"><a href="solving-one-dimensional-optimization-problems.html#cb120-1"></a>plt.axvline(x<span class="op">=</span>sol.root, color<span class="op">=</span><span class="st">&#39;b&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb121"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb121-1"><a href="solving-one-dimensional-optimization-problems.html#cb121-1"></a>plt.axhline(y<span class="op">=</span>f(sol.root), color<span class="op">=</span><span class="st">&#39;g&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb122"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb122-1"><a href="solving-one-dimensional-optimization-problems.html#cb122-1"></a>plt.xlabel(<span class="st">&#39;x -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb123"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb123-1"><a href="solving-one-dimensional-optimization-problems.html#cb123-1"></a>plt.ylabel(<span class="st">&#39;f(x) -&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb124"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb124-1"><a href="solving-one-dimensional-optimization-problems.html#cb124-1"></a>plt.title(<span class="st">&quot;optimization of $f(x) = x^3-6x^2+11x-6$&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-23.png" width="672" /></p>
</div>
<div id="secant-method" class="section level2">
<h2><span class="header-section-number">3.9</span> Secant Method</h2>
<p>The formulation of the <em>secant method</em> looks similar to that of <a href="solving-one-dimensional-optimization-problems.html#eq:38">(3.38)</a>.</p>
<p><span class="math display" id="eq:67">\[\begin{equation}
\frac{df}{dx}(x_j) + s\frac{d^2f}{dx^2}(x_j) = 0 \tag{3.67}
\end{equation}\]</span></p>
<p>Where, <span class="math inline">\(s\)</span> is the slope of the line connecting the points <span class="math inline">\((a, \frac{df}{dx}(a))\)</span> and <span class="math inline">\((b, \frac{df}{dx}(b))\)</span> on the <span class="math inline">\(x-\frac{df}{dx}\)</span> plane. <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are two different approximations to the root of the objective function. We know the equation for the slope,
<span class="math display" id="eq:68">\[\begin{align}
s = \frac{\frac{df}{dx}(b) - \frac{df}{dx}(a)}{b - a} \tag{3.68}
\end{align}\]</span></p>
<p>So, <a href="solving-one-dimensional-optimization-problems.html#eq:67">(3.67)</a> can be written as:
<span class="math display" id="eq:69">\[\begin{eqnarray}
    x &amp;-&amp; x_j = -\frac{\frac{df}{dx}(x_j)}{s} \nonumber \\
    x &amp;=&amp; x_j - \frac{\frac{df}{dx}(a)(b - a)}{\frac{df}{dx}(b) - \frac{df}{dx}(a)} = a - \frac{\frac{df}{dx}(a)(a - b)}{\frac{df}{dx}(a) - \frac{df}{dx}(b)} \tag{3.69}
\end{eqnarray}\]</span></p>
<p>The iterative process, known as the <em>secant method</em> is thus given by,
<span class="math display" id="eq:70">\[\begin{equation}
    x_{j+1} = x_j - \frac{\frac{df}{dx}(x_j)(x_j - x_{j-1})}{\frac{df}{dx}(x_j) - \frac{df}{dx}(x_{j+1})} \tag{3.70}
\end{equation}\]</span></p>
<p>The <em>secant</em> of a curve is a line that intersects the curve at minimum of two distinct points. As <span class="math inline">\(b \to a\)</span>, the secant approaches <span class="math inline">\(\frac{d^2f}{dx^2}(a)\)</span>. Due to this reason, the <em>secant method</em> is also called the <em>quasi-Newton method</em>. The root <span class="math inline">\(x^*\)</span> lies between the points <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> if the following condition is satisfied:</p>
<p><span class="math display" id="eq:71">\[\begin{equation}
    \frac{df}{dx}(a)\frac{df}{dx}(b) &lt; 0 \tag{3.71}
\end{equation}\]</span></p>
<p>As we have seen, the iteration process demands two initial points to start with, i.e, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. The iterates <span class="math inline">\(x_j\)</span> converges to the root of the objective function when <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are close to the root. The order of convergence for the process is given by the number <span class="math inline">\(\phi=\frac{\sqrt{5}+1}{2} \approx 1.618\)</span>, which is the golden ratio, i.e, the <em>secant method</em> has a <em>superlinear convergence</em>, unlike a quadratic or a cubic convergence.</p>
<p>The algorithm of the <em>secant method</em> is given below:</p>
<img src="img%209.png" />

<div class="example">
<span id="exm:unnamed-chunk-52" class="example"><strong>Example 3.6  </strong></span>Let us consider an objective function given by,
<span class="math display" id="eq:72">\[\begin{equation}
    f(x) = x\cos(x^2 - 7x) - 2x \tag{3.72}
\end{equation}\]</span>
We will find the root of this objective function, given the two starting points are <span class="math inline">\(a=-1.5\)</span> and <span class="math inline">\(b=1\)</span> and the tolerance <span class="math inline">\(\epsilon = 10^{-7}\)</span>.
</div>

<p>The plot for this objective function can be generated in the following way:</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb125-1"><a href="solving-one-dimensional-optimization-problems.html#cb125-1"></a><span class="kw">def</span> f(x): <span class="cf">return</span> x<span class="op">*</span>au.cos(x<span class="op">**</span><span class="dv">2-7</span><span class="op">*</span>x)<span class="op">-</span><span class="dv">2</span><span class="op">*</span>x</span>
<span id="cb125-2"><a href="solving-one-dimensional-optimization-problems.html#cb125-2"></a>l <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="fl">3.</span>, <span class="dv">1000</span>)</span>
<span id="cb125-3"><a href="solving-one-dimensional-optimization-problems.html#cb125-3"></a>plt.plot(l, f(l), <span class="st">&#39;r-&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb126"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb126-1"><a href="solving-one-dimensional-optimization-problems.html#cb126-1"></a>plt.xlabel(<span class="st">&#39;x-&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb127"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb127-1"><a href="solving-one-dimensional-optimization-problems.html#cb127-1"></a>plt.ylabel(<span class="st">&#39;f(x)-&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb128"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb128-1"><a href="solving-one-dimensional-optimization-problems.html#cb128-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-25.png" width="672" /></p>
<p>The corresponding <span class="math inline">\(x-\frac{df}{dx}(x)\)</span> plot is given below:</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb129-1"><a href="solving-one-dimensional-optimization-problems.html#cb129-1"></a><span class="kw">def</span> f(x): <span class="cf">return</span> x<span class="op">*</span>au.cos(x<span class="op">**</span><span class="dv">2-7</span><span class="op">*</span>x)<span class="op">-</span><span class="dv">2</span><span class="op">*</span>x</span>
<span id="cb129-2"><a href="solving-one-dimensional-optimization-problems.html#cb129-2"></a>l <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="fl">3.</span>, <span class="dv">1000</span>)</span>
<span id="cb129-3"><a href="solving-one-dimensional-optimization-problems.html#cb129-3"></a>G <span class="op">=</span> grad(f)</span>
<span id="cb129-4"><a href="solving-one-dimensional-optimization-problems.html#cb129-4"></a>h<span class="op">=</span>[]</span>
<span id="cb129-5"><a href="solving-one-dimensional-optimization-problems.html#cb129-5"></a><span class="cf">for</span> i <span class="kw">in</span> l:</span>
<span id="cb129-6"><a href="solving-one-dimensional-optimization-problems.html#cb129-6"></a>    h<span class="op">+=</span>[G(i),]</span>
<span id="cb129-7"><a href="solving-one-dimensional-optimization-problems.html#cb129-7"></a>h <span class="op">=</span> np.array(h)</span>
<span id="cb129-8"><a href="solving-one-dimensional-optimization-problems.html#cb129-8"></a>plt.plot(l, h, <span class="st">&#39;b-&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb130"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb130-1"><a href="solving-one-dimensional-optimization-problems.html#cb130-1"></a>plt.xlabel(<span class="st">&#39;x-&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb131"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb131-1"><a href="solving-one-dimensional-optimization-problems.html#cb131-1"></a>plt.ylabel(<span class="st">&#39;$</span><span class="ch">\\</span><span class="st">frac</span><span class="sc">{df}{dx}</span><span class="st">(x)$-&gt;&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb132"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb132-1"><a href="solving-one-dimensional-optimization-problems.html#cb132-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-27.png" width="672" /></p>
<p><strong>This chapter is under development</strong></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-to-unconstrained-optimization.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/indrag49/Numerical-Optimization/edit/master/03-One_Dimensional_Optimization.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/indrag49/Numerical-Optimization/blob/master/03-One_Dimensional_Optimization.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
