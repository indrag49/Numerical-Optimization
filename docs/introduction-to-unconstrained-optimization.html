<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Introduction to Unconstrained Optimization | Introduction to Mathematical Optimization</title>
  <meta name="description" content="Chapter 2 Introduction to Unconstrained Optimization | Introduction to Mathematical Optimization" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Introduction to Unconstrained Optimization | Introduction to Mathematical Optimization" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Introduction to Unconstrained Optimization | Introduction to Mathematical Optimization" />
  
  
  

<meta name="author" content="Indranil Ghosh" />


<meta name="date" content="2021-05-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Mathematical Optimization</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> What is Numerical Optimization?</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction-to-optimization"><i class="fa fa-check"></i><b>1.1</b> Introduction to Optimization</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#a-solution"><i class="fa fa-check"></i><b>1.2</b> A Solution</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#maximization"><i class="fa fa-check"></i><b>1.3</b> Maximization</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#feasible-region"><i class="fa fa-check"></i><b>1.4</b> Feasible Region</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#discrete-optimization-problems"><i class="fa fa-check"></i><b>1.5</b> Discrete Optimization Problems</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#linear-programming-problems"><i class="fa fa-check"></i><b>1.6</b> Linear Programming Problems</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#stochastic-optimization-problems"><i class="fa fa-check"></i><b>1.7</b> Stochastic Optimization Problems</a></li>
<li class="chapter" data-level="1.8" data-path="index.html"><a href="index.html#scaling-of-decision-variables"><i class="fa fa-check"></i><b>1.8</b> Scaling of Decision Variables</a></li>
<li class="chapter" data-level="1.9" data-path="index.html"><a href="index.html#gradient-vector-and-hessian-matrix-of-the-objective-function"><i class="fa fa-check"></i><b>1.9</b> Gradient Vector and Hessian Matrix of the Objective Function</a></li>
<li class="chapter" data-level="1.10" data-path="index.html"><a href="index.html#directional-derivative-of-the-objective-function"><i class="fa fa-check"></i><b>1.10</b> Directional Derivative of the Objective Function</a></li>
<li class="chapter" data-level="1.11" data-path="index.html"><a href="index.html#positive-definite-and-positive-semi-definite-matrices"><i class="fa fa-check"></i><b>1.11</b> Positive Definite and Positive Semi-definite Matrices</a></li>
<li class="chapter" data-level="1.12" data-path="index.html"><a href="index.html#what-is-convexity"><i class="fa fa-check"></i><b>1.12</b> What is Convexity?</a></li>
<li class="chapter" data-level="1.13" data-path="index.html"><a href="index.html#numerical-optimization-algorithms"><i class="fa fa-check"></i><b>1.13</b> Numerical Optimization Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html"><i class="fa fa-check"></i><b>2</b> Introduction to Unconstrained Optimization</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#the-unconstrained-optimization-problem"><i class="fa fa-check"></i><b>2.1</b> The Unconstrained Optimization Problem</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#smooth-functions"><i class="fa fa-check"></i><b>2.2</b> Smooth Functions</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#taylors-theorem"><i class="fa fa-check"></i><b>2.3</b> Taylor’s Theorem</a></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#necessary-and-sufficient-conditions-for-local-minimizer-in-unconstrained-optimization"><i class="fa fa-check"></i><b>2.4</b> Necessary and Sufficient Conditions for Local Minimizer in Unconstrained Optimization</a><ul>
<li class="chapter" data-level="2.4.1" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#first-order-necessary-condition"><i class="fa fa-check"></i><b>2.4.1</b> First-Order Necessary Condition</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#second-order-necessary-conditions"><i class="fa fa-check"></i><b>2.4.2</b> Second-Order Necessary Conditions</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Mathematical Optimization</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-unconstrained-optimization" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Introduction to Unconstrained Optimization</h1>
<p>This chapter introduces what exactly an unconstrained optimization problem is. A detailed discussion of Taylor’s Theorem is provided and has been use to study the first order and second order necessary and sufficient conditions for local minimizer in an unconstrained optimization tasks. Examples have been supplied too in view of understanding the necessary and sufficient conditions better. The Python package <code>scipy.optimize</code>, which will form an integral part in solving many optimization problems in the later chapters of this book, is introduced too. The chapter ends with an overview of how an algorithm to solve unconstrained minimization problem works, covering briefly two procedures: <strong>line search descent method</strong> and <strong>trust region method</strong>.</p>
<hr />
<div id="the-unconstrained-optimization-problem" class="section level2">
<h2><span class="header-section-number">2.1</span> The Unconstrained Optimization Problem</h2>
<p>As we have discussed in the first chapter, an unconstrained optimization problem deals with finding the local minimizer <span class="math inline">\(\mathbf{x}^*\)</span> of a real valued and smooth objective function <span class="math inline">\(f(\mathbf{x})\)</span> of <span class="math inline">\(n\)</span> variables, given by <span class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>, formulated as,</p>
<p><span class="math display" id="eq:1">\[\begin{equation}
    \underset{\mathbf{x} \in \mathbb{R}^n}{\min f(\mathbf{x})} \tag{2.1}
\end{equation}\]</span>
with no restrictions on the decision variables <span class="math inline">\(\mathbf{x}\)</span>. We work towards computing <span class="math inline">\(\mathbf{x}^*\)</span>, such that <span class="math inline">\(\forall\ \mathbf{x}\)</span> near <span class="math inline">\(\mathbf{x}^*\)</span>, the following inequality is satisfied:
<span class="math display" id="eq:2">\[\begin{equation}
    f(\mathbf{x}^*) \leq f(\mathbf{x}) \tag{2.2}
\end{equation}\]</span></p>
</div>
<div id="smooth-functions" class="section level2">
<h2><span class="header-section-number">2.2</span> Smooth Functions</h2>
<p>In terms of analysis, the measure of the number of continuous derivative a function has, characterizes the <em>smoothness</em> of a function.</p>

<div class="definition">
<span id="def:unnamed-chunk-1" class="definition"><strong>Definition 2.1  </strong></span>A function <span class="math inline">\(f\)</span> is <em>smooth</em> if it can be differentiated everywhere, i.e, the function has continuous derivatives up to some desired order over particular domain [Weisstein, Eric W. “Smooth Function.” <a href="https://mathworld.wolfram.com/SmoothFunction.html" class="uri">https://mathworld.wolfram.com/SmoothFunction.html</a>].
</div>

<p>Some examples of smooth functions are , <span class="math inline">\(f(x) = x\)</span>, <span class="math inline">\(f(x)=e^x\)</span>, <span class="math inline">\(f(x)=\sin(x)\)</span>, etc. To study the local minima <span class="math inline">\(\mathbf{x}^*\)</span> of a smooth objective function <span class="math inline">\(f(\mathbf{x})\)</span>, we emphasize on <em>Taylor’s theorem for a multivariate function</em>, thus focusing on the computations of the gradient vector <span class="math inline">\(\nabla f(\mathbf{x})\)</span> and the Hessian matrix <span class="math inline">\(\mathbf{H} f(\mathbf{x})\)</span>.</p>
</div>
<div id="taylors-theorem" class="section level2">
<h2><span class="header-section-number">2.3</span> Taylor’s Theorem</h2>

<div class="theorem">
<span id="thm:unnamed-chunk-2" class="theorem"><strong>Theorem 2.1  </strong></span>For a smooth function of a single variable given by <span class="math inline">\(f: \mathbb{R} \rightarrow \mathbb{R}\)</span>, <span class="math inline">\(m(\geq 1)\)</span> times differentiable at the point <span class="math inline">\(p \in \mathbb{R}\)</span>, there exists a function <span class="math inline">\(j_m: \mathbb{R} \rightarrow \mathbb{R}\)</span>, such that the following equations are satisfied:
<span class="math display" id="eq:3">\[\begin{align}
    f(x) &amp;= f(p) + (x - p)f^{&#39;}(p) + \frac{(x-p)^2}{2!}f^{&#39;&#39;}(p) + \ldots \\ &amp;+ \frac{(x-p)^m}{m!}f^m(p) + (x-p)^m j_m(x) \tag{2.3}
\end{align}\]</span>
and
<span class="math display" id="eq:4">\[\begin{equation}
    \lim_{x \to p}j_m(x)=0 \tag{2.4}
\end{equation}\]</span>
</div>

<p>The <span class="math inline">\(m-\)</span>th order Taylor polynomial of the function <span class="math inline">\(f\)</span> around the point <span class="math inline">\(p\)</span> is given by:
<span class="math display" id="eq:5">\[\begin{align}
    P_m(x)&amp;=f(p) + (x - p)f^{&#39;}(p) + \frac{(x-p)^2}{2!}f^{&#39;&#39;}(p) + \ldots \\ &amp;+ \frac{(x-p)^m}{m!}f^m(p) \tag{2.5}
\end{align}\]</span></p>
<p>Now let <span class="math inline">\(f\)</span> be a smooth, continuously differentiable function that takes in multiple variables, i.e, <span class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span> and <span class="math inline">\(\mathbf{x}, \mathbf{p}, \mathbf{\delta} \in \mathbb{R}^n\)</span>, where <span class="math inline">\(\mathbf{\delta}\)</span> is the direction in which the line <span class="math inline">\(\mathbf{x} = \mathbf{p}+\alpha \mathbf{\delta}\)</span> passes through the point <span class="math inline">\(\mathbf{p}\)</span> [<em>Snyman, Jan A. Practical mathematical optimization. Springer Science+ Business Media, Incorporated, 2005.</em>]. Here, <span class="math inline">\(\alpha \in [0,1]\)</span>. We have,
<span class="math display" id="eq:6">\[\begin{equation}
    f(\mathbf{x}) = f(\mathbf{p} + \alpha \mathbf{\delta}) \tag{2.6}
\end{equation}\]</span></p>
<p>From the definition of the <em>directional derivative</em>, we get,
<span class="math display" id="eq:7">\[\begin{equation}
    \frac{df(\mathbf{x})}{d\alpha}|\mathbf{\delta} = \nabla^T f(\mathbf{x})\mathbf{\delta}=\hat{f}(\mathbf{x}) \tag{2.7}
\end{equation}\]</span>
Again, differentiating <span class="math inline">\(\hat{f}(\mathbf{x})\)</span> with respect to <span class="math inline">\(\alpha\)</span>.</p>
<p><span class="math display" id="eq:8">\[\begin{equation}
    \frac{d \hat{f}(\mathbf{x})}{d \alpha}|\mathbf{\delta} = \frac{d^2 f(\mathbf{x})}{d \alpha^2}=\nabla^T\hat{f}(\mathbf{x})\mathbf{\delta}=\mathbf{\delta}^T\mathbf{H}f(\mathbf{x})\mathbf{\delta} \tag{2.8}
\end{equation}\]</span></p>
<p>So, using equations <a href="introduction-to-unconstrained-optimization.html#eq:5">(2.5)</a> and <a href="introduction-to-unconstrained-optimization.html#eq:6">(2.6)</a> we can generate the Taylor expansion for a multivariable function at a point <span class="math inline">\(\mathbf{p}\)</span>. So, around <span class="math inline">\(\alpha = 0\)</span>, we get,
<span class="math display" id="eq:9">\[\begin{equation}
    f(\mathbf{x}) = f(\mathbf{p}+\alpha \mathbf{\delta}) = f(\mathbf{p}) + \nabla^Tf(\mathbf{p})\alpha \mathbf{\delta} + \frac{1}{2}\alpha \mathbf{\delta}^T\mathbf{H} f(\mathbf{p})\alpha \mathbf{\delta} + \ldots \tag{2.9}
\end{equation}\]</span></p>
<p>The truncated Taylor expansion of the multivariable function, where the higher order terms are ignored, can be written as,
<span class="math display" id="eq:10">\[\begin{equation}
    f(\mathbf{x}) = f(\mathbf{p}+\alpha \mathbf{\delta}) = f(\mathbf{p}) + \nabla^Tf(\mathbf{p})\alpha \mathbf{\delta} + \frac{1}{2}\alpha \mathbf{\delta}^T\mathbf{H} f(\mathbf{p}+\beta\mathbf{\delta})\alpha \mathbf{\delta} \tag{2.10} 
\end{equation}\]</span>
where, <span class="math inline">\(\beta \in [0,1]\)</span>.</p>
</div>
<div id="necessary-and-sufficient-conditions-for-local-minimizer-in-unconstrained-optimization" class="section level2">
<h2><span class="header-section-number">2.4</span> Necessary and Sufficient Conditions for Local Minimizer in Unconstrained Optimization</h2>
<div id="first-order-necessary-condition" class="section level3">
<h3><span class="header-section-number">2.4.1</span> First-Order Necessary Condition</h3>
<p>If there exists a local minimizer <span class="math inline">\(\mathbf{x}^*\)</span> for a real-valued smooth function <span class="math inline">\(f(\mathbf{x}): \mathbb{R}^n \rightarrow \mathbb{R}\)</span>, in an open neighborhood <span class="math inline">\(\subset \mathbb{R}^n\)</span> of <span class="math inline">\(\mathbf{x}^*\)</span> along the direction <span class="math inline">\(\mathbf{\delta}\)</span>, then the <em>first order necessary condition</em> for the minimizer is given by:
<span class="math display" id="eq:11">\[\begin{equation}
    \nabla^Tf(\mathbf{x}^*)\mathbf{\delta}=0\ \forall\ \mathbf{\delta} \neq 0 \tag{2.11}
\end{equation}\]</span>
i.e, the <em>directional derivative</em> is <span class="math inline">\(0\)</span>, which ultimately reduces to the equation:
<span class="math display" id="eq:12">\[\begin{equation}
    \nabla f(\mathbf{x}^*)=0 \tag{2.12}
\end{equation}\]</span></p>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Let a real-valued smooth function <span class="math inline">\(f(\mathbf{x})\)</span> be differentiable at the point <span class="math inline">\(\mathbf{x}^* \in \mathbb{R}^n\)</span>. Using the <em>Taylor expansion</em>, we can write:</p>
<p><span class="math display" id="eq:13">\[\begin{equation}
    f(\mathbf{x})=f(\mathbf{x}^*) + \nabla^T f(\mathbf{x}^*) (\mathbf{x} - \mathbf{x}^*)+\sum_{|\gamma|\leq m}\frac{\mathfrak{D}^{\gamma}f(\mathbf{x}^*)}{\gamma!}(\mathbf{x}-\mathbf{x}^*)^{\gamma} + \sum_{|\gamma|=m}j_{\gamma}(\mathbf{x})(\mathbf{x} - \mathbf{x}^*)^{\gamma} \tag{2.13}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathfrak{D}\)</span> represents the differential and <span class="math inline">\(m\)</span> is the smoothness of the objective function <span class="math inline">\(f\)</span>. Also <span class="math inline">\(\lim_{\mathbf{x} \to \mathbf{x}^*}j_{\gamma(\mathbf{X})}=0\)</span>. Clubbing together the higher order terms, we can write Eq.<a href="introduction-to-unconstrained-optimization.html#eq:13">(2.13)</a> as,
<span class="math display" id="eq:14">\[\begin{equation}
    \label{eq:2.14}
    f(\mathbf{x})=f(\mathbf{x}^*) + \nabla^T f(\mathbf{x}^*) (\mathbf{x} - \mathbf{x}^*)+ \mathcal{O}(\|\mathbf{x} - \mathbf{x}^*\|) \tag{2.14}
\end{equation}\]</span></p>
<p>In this case,
<span class="math display" id="eq:15">\[\begin{equation}
\lim_{\mathbf{x} \to \mathbf{x}^*}\frac{\mathcal{O}(\|\mathbf{x} - \mathbf{x}^*\|)}{\|\mathbf{x} - \mathbf{x}^*\|}=0 \tag{2.15}
\end{equation}\]</span>
Let us consider, <span class="math inline">\(\mathbf{x} = \mathbf{x}^*-\beta \nabla f(\mathbf{x}^*)\)</span>, where <span class="math inline">\(\beta \in [0,1]\)</span>. From Eq.<a href="introduction-to-unconstrained-optimization.html#eq:14">(2.14)</a> we can write,
<span class="math display" id="eq:16">\[\begin{equation}
    f(\mathbf{x}^*-\beta \nabla f(\mathbf{x}^*))=f(\mathbf{x}^*)-\beta\|\nabla f(\mathbf{x}^*)\|^2+\mathcal{O}(\beta\|\nabla f(\mathbf{x}^*)\|) \tag{2.16}
\end{equation}\]</span></p>
<p>Now, dividing Eq.<a href="introduction-to-unconstrained-optimization.html#eq:16">(2.16)</a>-<span class="math inline">\(f(\mathbf{x}^*)\)</span> by <span class="math inline">\(\beta\)</span>, we get,</p>
<p><span class="math display" id="eq:17">\[\begin{equation}
    \frac{f(\mathbf{x}^*-\beta \nabla f(\mathbf{x}^*))-f(\mathbf{x}^*)}{\beta} = -\|\nabla f(\mathbf{x}^*)\|^2 + \frac{\mathcal{O}(\beta \|\nabla f(\mathbf{x}^*)\|)}{\beta} \geq 0 \tag{2.17}
\end{equation}\]</span></p>
<p>Now, considering the limit <span class="math inline">\(\beta \to 0^{+}\)</span>, we get,
<span class="math display" id="eq:18">\[\begin{equation}
    -\|\nabla f(\mathbf{x}^*)\|^2 \leq 0 \tag{2.18}
\end{equation}\]</span></p>
Combining which along with Eq.<a href="introduction-to-unconstrained-optimization.html#eq:18">(2.18)</a>, we get,
<span class="math display" id="eq:19">\[\begin{equation}
    0 \leq -\|\nabla f(\mathbf{x}^*)\|^2 \leq 0\tag{2.19}
\end{equation}\]</span>
This ultimately gives <span class="math inline">\(\nabla f(\mathbf{x}^*)=0\)</span>, proving the first-order necessary condition.
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-4" class="example"><strong>Example 2.1  </strong></span>The <strong>Rosenbrock function</strong> of <span class="math inline">\(n\)</span>-variables is given by:
<span class="math display" id="eq:20">\[\begin{equation}
    f(\mathbf{x}) = \sum_{i=1}^{n-1}(100(x_{i+1}-x_i^2)^2 + (1-x_i)^2) \tag{2.20}
\end{equation}\]</span></p>
<p>where, <span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span>. For this example let us consider the <em>Rosenbrock function</em> for two variables, given by:
<span class="math display" id="eq:21">\[\begin{equation}
    f(\mathbf{x}) = 100(x_2-x_1^2)^2+(1-x_1)^2 \tag{2.21}
\end{equation}\]</span></p>
<p>We will show that the first order necessary condition is satisfied for the local minimizer <span class="math inline">\(\mathbf{x^*}=\begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span>. We first check whether <span class="math inline">\(\mathbf{x}^*\)</span> is a minimizer or not. Putting <span class="math inline">\(x_1=x_2=1\)</span> in <span class="math inline">\(f(\mathbf{x})\)</span>, we get <span class="math inline">\(f(\mathbf{x})=0\)</span>. Now, we check whether the <span class="math inline">\(\mathbf{x^*}\)</span> satisfies the first order necessary condition. For that we calculate <span class="math inline">\(\nabla f(\mathbf{x}^*)\)</span>.
<span class="math display" id="eq:22">\[\begin{equation}
    \nabla f(\mathbf{x}^*) = \begin{bmatrix} -400x_1(x_2-x_1)^2-2(1-x_1) \\ 200(x_2-x_1^2)\end{bmatrix}_{\mathbf{x}^*} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} \tag{2.22}
\end{equation}\]</span></p>
<p>So, we see that the first order necessary condition is satisfied. We can do similar analysis using the <code>scipy.optimize</code> package in Python. The Scipy official reference states that the <code>scipy.optimize</code> package provides the user with many commonly used optimization algorithms and test functions. It packages the following functionalities and aspects:</p>
<ul>
<li>Minimization of multivariate scalar objective functions covering both the unconstrained and constrained domains, using a range of optimization algorithms,</li>
<li>Algorithms for minimization of scalar univariate functions,</li>
<li>A variety of brute-force optimization algorithms, also called global optimization algorithms,</li>
<li>Algorithms like minimization of least-squares and curve-fitting,</li>
<li>Root finding algorithms, and</li>
<li>Algorithms for solving multivariate equation systems.</li>
</ul>
We will build upon the basic concepts of optimization using this package and cover most of the concepts one by one as we advance in the book. Note that, this is not the only package that we are going to use throughout the book. As we advance, we will be required to look into other Python resources according to our needs. But to keep in mind, <code>scipy.optimize</code> is the most important package that we will be using. Now, going back to the example, <code>scipy.optimize</code> already provides with the  test function, its gradient and its Hessian. We will import them first and check whether the given point <span class="math inline">\(\mathbf{x}^*\)</span> is a minimizer or not:
</div>

<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="introduction-to-unconstrained-optimization.html#cb1-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="introduction-to-unconstrained-optimization.html#cb1-2"></a><span class="im">import</span> scipy</span>
<span id="cb1-3"><a href="introduction-to-unconstrained-optimization.html#cb1-3"></a><span class="co"># Import the Rosenbrock function, its gradient and Hessian respectively</span></span>
<span id="cb1-4"><a href="introduction-to-unconstrained-optimization.html#cb1-4"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> rosen, rosen_der, rosen_hess</span>
<span id="cb1-5"><a href="introduction-to-unconstrained-optimization.html#cb1-5"></a>x_m <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>]) <span class="co">#given local minimizer</span></span>
<span id="cb1-6"><a href="introduction-to-unconstrained-optimization.html#cb1-6"></a>rosen(x_m) <span class="co"># check whether x_m is a minimizer</span></span></code></pre></div>
<pre><code>## 0.0</code></pre>
<p>the result is <span class="math inline">\(0.0\)</span>. So <span class="math inline">\(\mathbf{x}^*\)</span> is a minimizer. We then check for the first order necessary condition, using the gradient:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="introduction-to-unconstrained-optimization.html#cb3-1"></a>rosen_der(x_m) <span class="co"># calculates the gradient at the point x_m</span></span></code></pre></div>
<pre><code>## array([0, 0])</code></pre>
<p>This matches with our calculations and also satisfies the first-order necessary condition.</p>
</div>
<div id="second-order-necessary-conditions" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Second-Order Necessary Conditions</h3>
<p>If there exists a local minimizer <span class="math inline">\(\mathbf{x}^*\)</span> for a real-valued smooth function <span class="math inline">\(f(\mathbf{x}): \mathbb{R}^n \rightarrow \mathbb{R}\)</span>, in an open neighborhood <span class="math inline">\(\subset \mathbb{R}^n\)</span> of <span class="math inline">\(\mathbf{x}^*\)</span> along the feasible direction <span class="math inline">\(\mathbf{\delta}\)</span>, and <span class="math inline">\(\mathbf{H} f(\mathbf{x})\)</span> exists and is continuous in the open neighborhood, then the second order necessary conditions for the minimizer are given by:
<span class="math display" id="eq:23">\[\begin{equation}
    \nabla^T f(\mathbf{x}^*)\mathbf{\delta} = 0, \forall\ \mathbf{\delta} \neq 0 \tag{2.23}
\end{equation}\]</span></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/indrag49/Numerical-Optimization/edit/master/02-Unconstrained_Optimization.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/indrag49/Numerical-Optimization/blob/master/02-Unconstrained_Optimization.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
