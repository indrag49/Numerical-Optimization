<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Line Search Descent Methods | Introduction to Mathematical Optimization</title>
  <meta name="description" content="A book for teaching introductory numerical optimization algorithms with Python" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Line Search Descent Methods | Introduction to Mathematical Optimization" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://indrag49.github.io/Numerical-Optimization/" />
  
  <meta property="og:description" content="A book for teaching introductory numerical optimization algorithms with Python" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Line Search Descent Methods | Introduction to Mathematical Optimization" />
  
  <meta name="twitter:description" content="A book for teaching introductory numerical optimization algorithms with Python" />
  

<meta name="author" content="Indranil Ghosh" />


<meta name="date" content="2021-07-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="solving-one-dimensional-optimization-problems.html"/>
<link rel="next" href="conjugate-gradient-methods-1.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Mathematical Optimization</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> What is Numerical Optimization?</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction-to-optimization"><i class="fa fa-check"></i><b>1.1</b> Introduction to Optimization</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#a-solution"><i class="fa fa-check"></i><b>1.2</b> A Solution</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#maximization"><i class="fa fa-check"></i><b>1.3</b> Maximization</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#feasible-region"><i class="fa fa-check"></i><b>1.4</b> Feasible Region</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#discrete-optimization-problems"><i class="fa fa-check"></i><b>1.5</b> Discrete Optimization Problems</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#linear-programming-problems"><i class="fa fa-check"></i><b>1.6</b> Linear Programming Problems</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#stochastic-optimization-problems"><i class="fa fa-check"></i><b>1.7</b> Stochastic Optimization Problems</a></li>
<li class="chapter" data-level="1.8" data-path="index.html"><a href="index.html#scaling-of-decision-variables"><i class="fa fa-check"></i><b>1.8</b> Scaling of Decision Variables</a></li>
<li class="chapter" data-level="1.9" data-path="index.html"><a href="index.html#gradient-vector-and-hessian-matrix-of-the-objective-function"><i class="fa fa-check"></i><b>1.9</b> Gradient Vector and Hessian Matrix of the Objective Function</a></li>
<li class="chapter" data-level="1.10" data-path="index.html"><a href="index.html#directional-derivative-of-the-objective-function"><i class="fa fa-check"></i><b>1.10</b> Directional Derivative of the Objective Function</a></li>
<li class="chapter" data-level="1.11" data-path="index.html"><a href="index.html#positive-definite-and-positive-semi-definite-matrices"><i class="fa fa-check"></i><b>1.11</b> Positive Definite and Positive Semi-definite Matrices</a></li>
<li class="chapter" data-level="1.12" data-path="index.html"><a href="index.html#what-is-convexity"><i class="fa fa-check"></i><b>1.12</b> What is Convexity?</a></li>
<li class="chapter" data-level="1.13" data-path="index.html"><a href="index.html#numerical-optimization-algorithms"><i class="fa fa-check"></i><b>1.13</b> Numerical Optimization Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html"><i class="fa fa-check"></i><b>2</b> Introduction to Unconstrained Optimization</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#the-unconstrained-optimization-problem"><i class="fa fa-check"></i><b>2.1</b> The Unconstrained Optimization Problem</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#smooth-functions"><i class="fa fa-check"></i><b>2.2</b> Smooth Functions</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#taylors-theorem"><i class="fa fa-check"></i><b>2.3</b> Taylorâ€™s Theorem</a></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#necessary-and-sufficient-conditions-for-local-minimizer-in-unconstrained-optimization"><i class="fa fa-check"></i><b>2.4</b> Necessary and Sufficient Conditions for Local Minimizer in Unconstrained Optimization</a><ul>
<li class="chapter" data-level="2.4.1" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#first-order-necessary-condition"><i class="fa fa-check"></i><b>2.4.1</b> First-Order Necessary Condition</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#second-order-necessary-conditions"><i class="fa fa-check"></i><b>2.4.2</b> Second-Order Necessary Conditions</a></li>
<li class="chapter" data-level="2.4.3" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#second-order-sufficient-conditions"><i class="fa fa-check"></i><b>2.4.3</b> Second-Order Sufficient Conditions</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-unconstrained-optimization.html"><a href="introduction-to-unconstrained-optimization.html#algorithms-for-solving-unconstrained-minimization-tasks"><i class="fa fa-check"></i><b>2.5</b> Algorithms for Solving Unconstrained Minimization Tasks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html"><i class="fa fa-check"></i><b>3</b> Solving One Dimensional Optimization Problems</a><ul>
<li class="chapter" data-level="3.1" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#one-dimensional-optimization-problems"><i class="fa fa-check"></i><b>3.1</b> One Dimensional Optimization Problems</a></li>
<li class="chapter" data-level="3.2" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#what-is-a-unimodal-function"><i class="fa fa-check"></i><b>3.2</b> What is a Unimodal Function?</a></li>
<li class="chapter" data-level="3.3" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#fibonacci-search-method"><i class="fa fa-check"></i><b>3.3</b> Fibonacci Search Method</a></li>
<li class="chapter" data-level="3.4" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#golden-section-search-method"><i class="fa fa-check"></i><b>3.4</b> Golden Section Search Method</a></li>
<li class="chapter" data-level="3.5" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#powells-quadratic-interpolation-method"><i class="fa fa-check"></i><b>3.5</b> Powellâ€™s Quadratic Interpolation Method</a></li>
<li class="chapter" data-level="3.6" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#inverse-quadratic-interpolation-method"><i class="fa fa-check"></i><b>3.6</b> Inverse Quadratic Interpolation Method</a></li>
<li class="chapter" data-level="3.7" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#newtons-method"><i class="fa fa-check"></i><b>3.7</b> Newtonâ€™s Method</a></li>
<li class="chapter" data-level="3.8" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#halleys-method"><i class="fa fa-check"></i><b>3.8</b> Halleyâ€™s Method</a></li>
<li class="chapter" data-level="3.9" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#secant-method"><i class="fa fa-check"></i><b>3.9</b> Secant Method</a></li>
<li class="chapter" data-level="3.10" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#bisection-method"><i class="fa fa-check"></i><b>3.10</b> Bisection Method</a></li>
<li class="chapter" data-level="3.11" data-path="solving-one-dimensional-optimization-problems.html"><a href="solving-one-dimensional-optimization-problems.html#brents-method"><i class="fa fa-check"></i><b>3.11</b> Brentâ€™s Method</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html"><i class="fa fa-check"></i><b>4</b> Line Search Descent Methods</a><ul>
<li class="chapter" data-level="4.1" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#introduction-to-line-search-descent-methods-for-unconstrained-minimization"><i class="fa fa-check"></i><b>4.1</b> Introduction to Line Search Descent Methods for Unconstrained Minimization</a></li>
<li class="chapter" data-level="4.2" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#selection-of-step-length"><i class="fa fa-check"></i><b>4.2</b> Selection of Step Length</a><ul>
<li class="chapter" data-level="4.2.1" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#the-wolfe-conditions"><i class="fa fa-check"></i><b>4.2.1</b> The Wolfe Conditions</a></li>
<li class="chapter" data-level="4.2.2" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#an-algorithm-for-the-strong-wolfe-conditions"><i class="fa fa-check"></i><b>4.2.2</b> An Algorithm for the Strong Wolfe Conditions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#first-order-line-search-gradient-descent-method-the-steepest-descent-algorithm"><i class="fa fa-check"></i><b>4.3</b> First Order Line Search Gradient Descent Method: The Steepest Descent Algorithm</a></li>
<li class="chapter" data-level="4.4" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#conjugate-gradient-methods"><i class="fa fa-check"></i><b>4.4</b> Conjugate Gradient Methods</a></li>
<li class="chapter" data-level="4.5" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#second-order-line-search-gradient-descent-method"><i class="fa fa-check"></i><b>4.5</b> Second Order Line Search Gradient Descent Method</a></li>
<li class="chapter" data-level="4.6" data-path="line-search-descent-methods.html"><a href="line-search-descent-methods.html#marquardt-method"><i class="fa fa-check"></i><b>4.6</b> Marquardt Method</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html"><i class="fa fa-check"></i><b>5</b> Conjugate Gradient Methods</a><ul>
<li class="chapter" data-level="5.1" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#introduction-to-conjugate-gradient-methods"><i class="fa fa-check"></i><b>5.1</b> Introduction to Conjugate Gradient Methods</a></li>
<li class="chapter" data-level="5.2" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#linear-conjugate-gradient-algorithm"><i class="fa fa-check"></i><b>5.2</b> Linear Conjugate Gradient Algorithm</a><ul>
<li class="chapter" data-level="5.2.1" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#mutual-conjugacy"><i class="fa fa-check"></i><b>5.2.1</b> Mutual Conjugacy</a></li>
<li class="chapter" data-level="5.2.2" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#conjugate-direction-algorithm"><i class="fa fa-check"></i><b>5.2.2</b> Conjugate Direction Algorithm</a></li>
<li class="chapter" data-level="5.2.3" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#preliminary-algorithm"><i class="fa fa-check"></i><b>5.2.3</b> Preliminary Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#nonlinear-conjugate-gradient-algorithm"><i class="fa fa-check"></i><b>5.3</b> Nonlinear Conjugate Gradient Algorithm</a><ul>
<li class="chapter" data-level="5.3.1" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#feltcher-reeves-algorithm"><i class="fa fa-check"></i><b>5.3.1</b> Feltcher-Reeves Algorithm</a></li>
<li class="chapter" data-level="5.3.2" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#polak-ribiere-algorithm"><i class="fa fa-check"></i><b>5.3.2</b> Polak-Ribiere Algorithm</a></li>
<li class="chapter" data-level="5.3.3" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#hestenes-stiefel-algorithm"><i class="fa fa-check"></i><b>5.3.3</b> Hestenes-Stiefel Algorithm</a></li>
<li class="chapter" data-level="5.3.4" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#dai-yuan-algorithm"><i class="fa fa-check"></i><b>5.3.4</b> Dai-Yuan Algorithm</a></li>
<li class="chapter" data-level="5.3.5" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#hager-zhang-algorithm"><i class="fa fa-check"></i><b>5.3.5</b> Hager-Zhang Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="conjugate-gradient-methods-1.html"><a href="conjugate-gradient-methods-1.html#the-scipy.optimize.minimize-function"><i class="fa fa-check"></i><b>5.4</b> The <code>scipy.optimize.minimize()</code> Function</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="quasi-newton-methods.html"><a href="quasi-newton-methods.html"><i class="fa fa-check"></i><b>6</b> Quasi-Newton Methods</a><ul>
<li class="chapter" data-level="6.1" data-path="quasi-newton-methods.html"><a href="quasi-newton-methods.html#introduction-to-quasi-newton-methods"><i class="fa fa-check"></i><b>6.1</b> Introduction to Quasi-Newton Methods</a></li>
<li class="chapter" data-level="6.2" data-path="quasi-newton-methods.html"><a href="quasi-newton-methods.html#the-approximate-inverse-matrix"><i class="fa fa-check"></i><b>6.2</b> The Approximate Inverse Matrix</a></li>
<li class="chapter" data-level="6.3" data-path="quasi-newton-methods.html"><a href="quasi-newton-methods.html#rank-1-update-algorithm"><i class="fa fa-check"></i><b>6.3</b> Rank 1 Update Algorithm</a></li>
<li class="chapter" data-level="6.4" data-path="quasi-newton-methods.html"><a href="quasi-newton-methods.html#rank-2-update-algorithms"><i class="fa fa-check"></i><b>6.4</b> Rank 2 Update Algorithms</a><ul>
<li class="chapter" data-level="6.4.1" data-path="quasi-newton-methods.html"><a href="quasi-newton-methods.html#davidon-fletcher-powell-algorithm"><i class="fa fa-check"></i><b>6.4.1</b> Davidon-Fletcher-Powell Algorithm</a></li>
<li class="chapter" data-level="6.4.2" data-path="quasi-newton-methods.html"><a href="quasi-newton-methods.html#broyden-fletcher-goldfarb-shanno-bfgs-algorithm"><i class="fa fa-check"></i><b>6.4.2</b> Broyden-Fletcher-Goldfarb-Shanno (BFGS) Algorithm</a></li>
<li class="chapter" data-level="6.4.3" data-path="quasi-newton-methods.html"><a href="quasi-newton-methods.html#huangs-family-of-rank-2-update-formulae"><i class="fa fa-check"></i><b>6.4.3</b> Huangâ€™s Family of Rank 2 Update Formulae</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Mathematical Optimization</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="line-search-descent-methods" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Line Search Descent Methods</h1>
<p>This chapter starts with an outline of a simple <strong>line-search descent algorithm</strong>, before introducing the <strong>Wolfe conditions</strong> and how to use them to design an algorithm for selecting a step length at a chosen descent direction at each step of the line search algorithms. Then a first order line search descent algorithm called the <strong>Steepest Descent Algorithm</strong> and a second order line search descent algorithm, called the <strong>Modified Newton Method</strong> have been discussed. Examples, Python programs and proofs accompanying each section of the chapter have been provided, wherever required. Finally, before ending the chapter with <strong>Marquardt method</strong>, motivations to study <strong>Conjugate Gradient Methods</strong> and <strong>Quasi-Newton Methods</strong> have been explored, which will be introduced in detailed manner in the next and later chapters.</p>
<hr />
<div id="introduction-to-line-search-descent-methods-for-unconstrained-minimization" class="section level2">
<h2><span class="header-section-number">4.1</span> Introduction to Line Search Descent Methods for Unconstrained Minimization</h2>
<p>In the <em>line search descent methods</em>, the optimization technique picks a direction <span class="math inline">\(\mathbb{\delta_j}\)</span> to begin with, for the <span class="math inline">\(j^{th}\)</span> step and carries out a search along this direction from the previous experimental point, to generate a new iterate. The iterative process looks like:
<span class="math display" id="eq:1">\[\begin{equation}
    \mathbb{x}_j = \mathbb{x}_{j-1}+\beta_{j}\mathbb{\delta}_j, \mathbb{x} \in \mathbb{R}^n \tag{4.1}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(\beta_j\)</span> is a positive scalar number at the <span class="math inline">\(j^{th}\)</span> step , called the <em>step length</em>. The performance of a line search descent algorithm depends on the selection of both the <em>step length</em> <span class="math inline">\(\beta_j\)</span> and the descent direction <span class="math inline">\(\mathbb{\delta}_j\)</span>. The condition for selecting the direction <span class="math inline">\(\mathbb{\delta}_j\)</span>for the next iterate :</p>
<p><span class="math display" id="eq:2">\[\begin{equation}
\nabla^T f(\mathbb{x}_{j-1})\mathbb{\delta}_j &lt; 0 \tag{4.2}
\end{equation}\]</span>
i.e, the directional derivative in the direction <span class="math inline">\(\mathbb{\delta}_j\)</span> should be negative. The step length <span class="math inline">\(\beta_j\)</span> is computed by solving the one dimensional optimization problem formulated as:
<span class="math display" id="eq:3">\[\begin{equation}
    \underset{\beta_j &gt; 0}{\min} \tilde{f}(\beta_j) = \underset{\beta_j &gt; 0}{\min} f(\mathbb{x}_{j-1} + \beta_j \mathbb{\delta}_j) \tag{4.3}
\end{equation}\]</span></p>
<p>The general algorithm for a line search descent method is given below:</p>
<p><img src="img%2012.png" /></p>
</div>
<div id="selection-of-step-length" class="section level2">
<h2><span class="header-section-number">4.2</span> Selection of Step Length</h2>
<p>While finding a suitable step length <span class="math inline">\(\beta_j\)</span> at the <span class="math inline">\(j^{th}\)</span> iteration, we should keep in mind that the choice should be such that there is an acceptable reduction in the objective function value. We work towards solving a minimization task formulated as:</p>
<p><span class="math display" id="eq:4">\[\begin{equation}
    \tilde{f}(\beta) = f(\mathbb{x}_{j-1} + \beta\mathbb{\delta}_j),\ \  \beta &gt; 0 \tag{4.4}
\end{equation}\]</span></p>
<p>The algorithm should be designed in such a way that too many computations of the objective function and its gradient should be avoided. This can be achieved by performing <em>inexact</em> line searches, to compute the local minimizer of <span class="math inline">\(\tilde{f}\)</span>. As we discussed earlier, there should be a condition for choosing <span class="math inline">\(\beta_j\)</span> at each iterate. The condition:</p>
<p><span class="math display" id="eq:5">\[\begin{equation}
    f(\mathbb{x}_j) &gt; f(\mathbb{x}_{j-1} + \beta_j\mathbb{\delta}_j) \tag{4.5}
\end{equation}\]</span></p>
<p>does not suffice alone. We need to have a sufficient decrease condition known as the <em>wolfe conditions</em>.</p>
<div id="the-wolfe-conditions" class="section level3">
<h3><span class="header-section-number">4.2.1</span> The Wolfe Conditions</h3>
<p>The step length <span class="math inline">\(\beta_j\)</span>, chosen at each iteration, must result in a <em>sufficient decrease</em> in the objective function <span class="math inline">\(f(\mathbb{x})\)</span> given by:</p>
<p><span class="math display" id="eq:6">\[\begin{equation}
    f(\mathbb{x}_{j-1} +\beta_j\mathbb{\delta}_j) \leq f(\mathbb{x}_j) + \alpha_1 \beta_j\nabla^Tf(\mathbb{x}_j)\mathbb{\delta}_j,\ \ \ \ \alpha_1 \in (0, 1) \tag{4.6}
\end{equation}\]</span></p>
<p>This is also called the <em>Armijo condition</em>. Practically, the value of <span class="math inline">\(\alpha_1\)</span> should be very small, for example in the order of <span class="math inline">\(10^{-4}\)</span>. But the <em>Armijo condition</em> itself is not enough to guarantee a reasonable progress in the algorithm. To avoid unacceptably short step lengths, there is another condition given by:
<span class="math display" id="eq:7">\[\begin{equation}
    \nabla^Tf(\mathbb{x}_{j-1} + \beta_j\mathbb{\delta}_j)\mathbb{\delta}_j \geq \alpha_2\nabla^Tf(\mathbb{x}_{j-1})\mathbb{\delta}_j,\ \ \ \ \alpha_2 \in (\alpha_1, 1) \tag{4.7}
\end{equation}\]</span></p>
<p>This is also called the <em>curvature condition</em>. Practically <span class="math inline">\(\alpha_2\)</span> is chosen between <span class="math inline">\(0.1 - 0.9\)</span> depending on the algorithms we consider. Eq. <a href="line-search-descent-methods.html#eq:6">(4.6)</a> and Eq. <a href="line-search-descent-methods.html#eq:7">(4.7)</a> together form the <em>Wolfe conditions</em>. Further more, the <em>curvature condition</em> can be modified to steer away from cases where a step length might satisfy the <em>Wolfe conditions</em> without being close to the minimizer of <span class="math inline">\(\tilde{f}(\beta)\)</span>. The modified version of Eq. <a href="line-search-descent-methods.html#eq:7">(4.7)</a> can be written as:
<span class="math display" id="eq:8">\[\begin{equation}
    |\nabla^Tf(\mathbb{x}_{j-1} + \beta_j\mathbb{\delta}_j)\mathbb{\delta}_j| \leq \alpha_2|\nabla^Tf(\mathbb{x}_{j-1})\mathbb{\delta}_j| \tag{4.8}
\end{equation}\]</span>
So, Eq. <a href="line-search-descent-methods.html#eq:6">(4.6)</a> and Eq. <a href="line-search-descent-methods.html#eq:7">(4.7)</a> together form the <em>strong Wolfe conditions</em>. For the <em>strong Wolfe conditions</em> the term <span class="math inline">\(\nabla^Tf(\mathbb{x}_{j-1} + \beta_j\mathbb{\delta}_j)\mathbb{\delta}_j\)</span> is no longer ``too positiveâ€™â€™ unlike the case for the <em>Wolfe conditions</em>.</p>
</div>
<div id="an-algorithm-for-the-strong-wolfe-conditions" class="section level3">
<h3><span class="header-section-number">4.2.2</span> An Algorithm for the Strong Wolfe Conditions</h3>
<p>Before moving on to the line search algorithm for the <em>strong wolfe conditions</em>, we discuss a straightforward algorithm called <em>zoom</em> which takes in two values <span class="math inline">\(\beta_l\)</span> and <span class="math inline">\(\beta_r\)</span> that bounds the interval <span class="math inline">\([\beta_l, \beta_r]\)</span> containing the step lengths that satisfy the <em>strong Wolfe conditions</em>. This algorithm has been originally introduced in the classic book by Nocedal and Wright. The purpose of this algorithm is to generate an iterate <span class="math inline">\(\beta_j \in [\beta_l, \beta_r]\)</span> at each step and replaces either <span class="math inline">\(\beta_l\)</span> or <span class="math inline">\(\beta_r\)</span> with <span class="math inline">\(\beta_j\)</span> in such a way that the following properties are maintained:</p>
<ul>
<li>The step length <span class="math inline">\(\beta_j\)</span> satisfying the <em>strong Wolfe conditions</em> lies in the interval <span class="math inline">\([\beta_l, \beta_r]\)</span>,</li>
<li><span class="math inline">\(\beta_l\)</span> is the step length which after a particular iteration gives the lowest function value besides satisfying the <em>Armijo conditions</em>,</li>
<li><span class="math inline">\(\beta_r\)</span> is chosen such that the following condition is satisfied:
<span class="math display" id="eq:9">\[\begin{equation}
      (\beta_r -\beta_l)[\nabla^Tf(\mathbb{x}_{j-1} + \beta_l\mathbb{\delta}_j)\mathbb{\delta}_j] &lt; 0 \tag{4.9}
\end{equation}\]</span></li>
</ul>
<p>The algorithm for <em>zoom</em> is given below:</p>
<p><img src="img%2013.png" /></p>
<p>Now, we describe the algorithm for finding an optimized step length <span class="math inline">\(\beta_j\)</span> at the <span class="math inline">\(j^{th}\)</span> iterate in a line search algorithm, solving the minimization task formulated by Eq. <a href="line-search-descent-methods.html#eq:3">(4.3)</a>. The <em>step-length selection algorithm</em> satisfying the <em>strong Wolfe conditions</em> is given below:</p>
<p><img src="img%2014.png" /></p>
<p>The first part of the above algorithm, starts with a trial estimate of the step length and keeps on increasing it at each step until it finds either an acceptable length or an interval bracketing the optimized step lengths. The <em>zoom()</em> function , given by <strong>Algorithm 11</strong>, is called in the second part which reduces the size of this interval until the optimized step length is reached.</p>

<div class="example">
<span id="exm:unnamed-chunk-1" class="example"><strong>Example 4.1  </strong></span>Let us consider the <em>Himmelblauâ€™s function</em> as the objective function, given by,
<span class="math display" id="eq:10">\[\begin{equation}
    f(x_1, x_2) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2 \tag{4.10}
\end{equation}\]</span>
</div>

<p>Let the starting point be <span class="math inline">\(\mathbb{x}=\begin{bmatrix}-2.5 \\ 2.8 \end{bmatrix}\)</span>, the descent direction be <span class="math inline">\(\mathbb{\delta}=\begin{bmatrix}-2.5 \\ -1 \end{bmatrix}\)</span>, <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\alpha_2\)</span> be <span class="math inline">\(10^{-4}\)</span> and <span class="math inline">\(0.325\)</span> respectively, and the upper bound be <span class="math inline">\(\beta_{max}=0.6\)</span>. For this descent direction, we will compute the step length for generating the next iterate from the starting point using <strong>Algorithm 11</strong>. The <em>step-length selection algorithm</em> solves the optimization problem given by Eq. <a href="line-search-descent-methods.html#eq:4">(4.4)</a> for the parameters provided in this example. Let us first define the objective function and its gradient using Python.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="line-search-descent-methods.html#cb1-1"></a><span class="co"># First let us import all the necessary packages</span></span>
<span id="cb1-2"><a href="line-search-descent-methods.html#cb1-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="line-search-descent-methods.html#cb1-3"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="line-search-descent-methods.html#cb1-4"></a><span class="im">import</span> autograd.numpy <span class="im">as</span> au</span>
<span id="cb1-5"><a href="line-search-descent-methods.html#cb1-5"></a><span class="im">from</span> autograd <span class="im">import</span> grad, jacobian</span>
<span id="cb1-6"><a href="line-search-descent-methods.html#cb1-6"></a><span class="im">import</span> scipy</span>
<span id="cb1-7"><a href="line-search-descent-methods.html#cb1-7"></a></span>
<span id="cb1-8"><a href="line-search-descent-methods.html#cb1-8"></a><span class="kw">def</span> himm(x): <span class="co"># Objective function</span></span>
<span id="cb1-9"><a href="line-search-descent-methods.html#cb1-9"></a>    <span class="cf">return</span> (x[<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> x[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">11</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> (x[<span class="dv">0</span>] <span class="op">+</span> x[<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">7</span>)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb1-10"><a href="line-search-descent-methods.html#cb1-10"></a></span>
<span id="cb1-11"><a href="line-search-descent-methods.html#cb1-11"></a>grad_himm <span class="op">=</span> grad(himm) <span class="co"># Gradient of the objective function</span></span></code></pre></div>
<p>We will now plot the objective function.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="line-search-descent-methods.html#cb2-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">100</span>)</span>
<span id="cb2-2"><a href="line-search-descent-methods.html#cb2-2"></a>y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">100</span>)</span>
<span id="cb2-3"><a href="line-search-descent-methods.html#cb2-3"></a>z <span class="op">=</span> np.zeros(([<span class="bu">len</span>(x), <span class="bu">len</span>(y)]))</span>
<span id="cb2-4"><a href="line-search-descent-methods.html#cb2-4"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(x)):</span>
<span id="cb2-5"><a href="line-search-descent-methods.html#cb2-5"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(y)):</span>
<span id="cb2-6"><a href="line-search-descent-methods.html#cb2-6"></a>        z[j, i] <span class="op">=</span> himm([x[i], y[j]])</span>
<span id="cb2-7"><a href="line-search-descent-methods.html#cb2-7"></a></span>
<span id="cb2-8"><a href="line-search-descent-methods.html#cb2-8"></a>contours<span class="op">=</span>plt.contour(x, y, z, <span class="dv">100</span>, cmap<span class="op">=</span>plt.cm.gnuplot)</span>
<span id="cb2-9"><a href="line-search-descent-methods.html#cb2-9"></a>plt.clabel(contours, inline<span class="op">=</span><span class="dv">1</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="line-search-descent-methods.html#cb3-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>We will use the <code>line_search()</code> function from the <code>scipy.optimize</code> module which is a Python implementation of the <em>step-length selection algorithm</em>. The attributes for the <code>line_search()</code> function are:</p>
<ul>
<li><code>f</code>: This is the objective function, which is a callable datatype,</li>
<li><code>myfprime</code>: This is the gradient of the objective function and is callable,</li>
<li><code>xk</code>: This is the starting iterate, given by an <code>ndarray</code> datatype,</li>
<li><code>pk</code>: This is the descent direction given by <span class="math inline">\(\mathbb{\delta}\)</span>, used for generating the next iterate point from a given starting point. This is also given by an <code>ndarray</code> datatype,</li>
<li><code>c1</code>: This is the <span class="math inline">\(\alpha_1\)</span> value from the <em>Armijo condition</em> given by Eq. <a href="line-search-descent-methods.html#eq:6">(4.6)</a>. This is an optional datatype and is a <code>float</code>,</li>
<li><code>c2</code>: This is the <span class="math inline">\(\alpha_2\)</span> value from the <em>curvature condition</em> given by Eq. <a href="line-search-descent-methods.html#eq:7">(4.7)</a>. This is an optional datatype and is a <code>float</code>,</li>
<li><code>amax</code>: This is the upper bound set for the step lengths, given by <span class="math inline">\(\beta_{max}\)</span> in <em>step-length selection algorithm</em>. This is an optional datatype and is a <code>float</code>.</li>
</ul>
<p>There are other attributes that one can pass to the function, but they are less important but can provide with more flexibilities if provided. These are:</p>
<ul>
<li><code>gfk</code>: Gives the gradient value at the current iterate point, <span class="math inline">\(\mathbb{x}_j\)</span>. This is an optional parameter and is a <code>float</code>,</li>
<li><code>old_fval</code>: This gives the function value at the current iterate point, <span class="math inline">\(\mathbb{x}_j\)</span>. The parameter is optional and is a <code>float</code>,</li>
<li><code>old_old_fval</code>: This gives the function value at the point preceding the current iterate point,i.e, <span class="math inline">\(\mathbb{x}_{j-1}\)</span>. This is optional and is a <code>float</code>,</li>
<li><code>args</code>: These are the additional arguments that might be passed to the objective function. This is optional and is a Python <code>tuple</code>,</li>
<li><code>maxiter</code>: This is the maximum number of iterations that are needed to be performed by the optimization algorithm. This is optional too and is an <code>int</code> datatype, and</li>
<li><code>extra_condition</code>: This is a callable function having the following form: <code>extra_condition(beta, current_iterate, function, gradient)</code>. The step length <code>beta</code> is accepted if only this function returns <code>True</code>, otherwise the algorithm continues with the new iterates. This callable function is only invoked for those iterates which satisfy the <em>strong Wolfe conditions</em>.</li>
</ul>
<p>The <code>scipy.optimize.line_search()</code> method returns the following:</p>
<ul>
<li>The optimized step length <span class="math inline">\(\beta\)</span> solving Eq. <a href="line-search-descent-methods.html#eq:3">(4.3)</a> for the next iterate from the current iterate. This will either be a <code>float</code> or a <code>None</code>,</li>
<li>The number of function evaluations made. This is an <code>int</code>,</li>
<li>The number of gradient evaluations made. This is an <code>int</code>,</li>
<li>The function value given by Eq. <a href="line-search-descent-methods.html#eq:4">(4.4)</a> with the computed step length. This will be a <code>float</code> if the algorithm converges, otherwise this will be a <code>None</code>,</li>
<li>The function value at the starting point, the algorithm starts with. This is a <code>float</code> too, and</li>
<li>The local slope along the descent direction at the new value. This will be a <code>float</code> if the algorithm converges, otherwise, this will be a <code>None</code>.</li>
</ul>
<p>Now, for our example, we enter the values of the starting point, the descent direction, the constants <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\alpha_2\)</span> and the upper bound on the step lengths for the <code>scipy.optimize.line_search()</code> and print the results.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="line-search-descent-methods.html#cb4-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> line_search</span>
<span id="cb4-2"><a href="line-search-descent-methods.html#cb4-2"></a></span>
<span id="cb4-3"><a href="line-search-descent-methods.html#cb4-3"></a>start_point <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">2.5</span>, <span class="fl">2.8</span>])</span>
<span id="cb4-4"><a href="line-search-descent-methods.html#cb4-4"></a>delta <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">2.5</span>, <span class="dv">-1</span>])</span>
<span id="cb4-5"><a href="line-search-descent-methods.html#cb4-5"></a>alpha_1 <span class="op">=</span> <span class="dv">10</span><span class="op">**-</span><span class="dv">4</span></span>
<span id="cb4-6"><a href="line-search-descent-methods.html#cb4-6"></a>alpha_2 <span class="op">=</span> <span class="fl">0.325</span></span>
<span id="cb4-7"><a href="line-search-descent-methods.html#cb4-7"></a>beta_max <span class="op">=</span> <span class="fl">0.6</span></span>
<span id="cb4-8"><a href="line-search-descent-methods.html#cb4-8"></a></span>
<span id="cb4-9"><a href="line-search-descent-methods.html#cb4-9"></a>res<span class="op">=</span>line_search(f <span class="op">=</span> himm, myfprime <span class="op">=</span> grad_himm, xk <span class="op">=</span> start_point, pk <span class="op">=</span> delta, c1 <span class="op">=</span> alpha_1, c2 <span class="op">=</span> alpha_2, amax <span class="op">=</span> beta_max)</span>
<span id="cb4-10"><a href="line-search-descent-methods.html#cb4-10"></a>res</span></code></pre></div>
<pre><code>## (0.04234665754870197, 4, 1, 6.112599989468139, 6.5581000000000005, array([ 11.13041873, -24.97823686]))</code></pre>
<p>We see that the optimized step length is <span class="math inline">\(\sim 0.04\)</span>, the number of function evaluations made is <span class="math inline">\(4\)</span>, the number of gradient evaluations made is <span class="math inline">\(1\)</span>, the function value at the new step length is <span class="math inline">\(\tilde{f(\beta)}\sim 6.11\)</span>, the function value at the starting point is <span class="math inline">\(\sim 6.56\)</span> and the local slope along the descent direction is <span class="math inline">\(\sim \begin{bmatrix} 11.13 \\ -25 \end{bmatrix}\)</span>.</p>
</div>
</div>
<div id="first-order-line-search-gradient-descent-method-the-steepest-descent-algorithm" class="section level2">
<h2><span class="header-section-number">4.3</span> First Order Line Search Gradient Descent Method: The Steepest Descent Algorithm</h2>
<p>Optimization methods that use the gradient vector <span class="math inline">\(\nabla^Tf(\mathbb{x})\)</span> to compute the descent direction <span class="math inline">\(\mathbb{\delta}_j\)</span> at each iteration, are referred to as the <em>first order line search gradient descent methods</em>. We will discuss the <em>steepest descent algorithm</em> that falls under this category. It is also called the <em>Cauchy method</em>, as it was first introduced by the french mathematician Cauchy in 1847.</p>
<p>Now, we explore, how to select the direction of the <em>steepest descent algorithm</em>. At the iterate <span class="math inline">\(\mathbb{x}_{j-1}\)</span>, the direction of the steepest descent, given by the unit vector <span class="math inline">\(\mathbb{\delta}_j\)</span> is chosen, such that the directional derivative <span class="math inline">\(\nabla^Tf(\mathbb{x}_{j-1})\mathbb{\delta}_j\)</span> takes a minimum value for all possible values of <span class="math inline">\(\mathbb{\delta}_j\)</span> at <span class="math inline">\(\mathbb{x}_{j-1}\)</span>. Now, using Schwartzâ€™s inequality,
<span class="math display" id="eq:11">\[\begin{equation}
    \nabla^Tf(\mathbb{x}_{j-1})\mathbb{\delta}_j \geq - \|\nabla^Tf(\mathbb{x}_{j-1})\|\|\mathbb{\delta}_j\| \geq - \|\nabla^Tf(\mathbb{x}_{j-1})\| \tag{4.11}
\end{equation}\]</span></p>
<p>The value <span class="math inline">\(- \|\nabla^Tf(\mathbb{x}_{j-1})\|\)</span> is the minimum value. Now, from the first and the third terms in the Eq. <a href="line-search-descent-methods.html#eq:11">(4.11)</a>, we can write,
<span class="math display" id="eq:12">\[\begin{equation}
    \mathbb{\delta}_j = -\frac{\nabla f(\mathbb{x}_{j-1})}{\|\nabla f(\mathbb{x}_{j-1})\|} \tag{4.12}
\end{equation}\]</span></p>
<p>The expression in Eq. <a href="line-search-descent-methods.html#eq:12">(4.12)</a> is the <em>normalised direction of the steepest descent algorithm</em>. The <em>Steepest Direction Algorithm</em> is given below:</p>
<p><img src="img%2015.png" /></p>

<div class="example">
<span id="exm:unnamed-chunk-5" class="example"><strong>Example 4.2  </strong></span>Let us again consider Himmelblauâ€™s function as the objective function, given by,
<span class="math display">\[\begin{equation}
    f(x_1, x_2) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2 \nonumber
\end{equation}\]</span>
</div>

<p>The function has four local minima:</p>
<ul>
<li><span class="math inline">\(f(3., 2.) = 0\)</span>,</li>
<li><span class="math inline">\(f(-2.8051, 3.1313) = 0\)</span>,</li>
<li><span class="math inline">\(f(-3.7793, -3.2831) = 0\)</span>, and</li>
<li><span class="math inline">\(f(3.5844, -1.8481) = 0\)</span>.</li>
</ul>
<p>We will find one of these local minima of Himmelblauâ€™s function, using the <em>steepest descent algorithm</em> in Python. Let the starting iterate be <span class="math inline">\(\mathbb{x}_j = \begin{bmatrix}1.1 \\ 2.2\end{bmatrix}\)</span>, the tolerances be <span class="math inline">\(\epsilon_1 = \epsilon_2 = \epsilon_3 = 10^{-5}\)</span>, and the constants to be used in determining the step length using the <em>strong Wolfe conditions</em> be <span class="math inline">\(\alpha_1 = 10^{-4}\)</span> and <span class="math inline">\(\alpha_2=0.212\)</span>. Let us rewrite Himmelblauâ€™s function and its gradient in Python:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="line-search-descent-methods.html#cb6-1"></a><span class="kw">def</span> func(x): <span class="co"># Objective function (Himmelblau&#39;s function)</span></span>
<span id="cb6-2"><a href="line-search-descent-methods.html#cb6-2"></a>    <span class="cf">return</span> (x[<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> x[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">11</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> (x[<span class="dv">0</span>] <span class="op">+</span> x[<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">7</span>)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb6-3"><a href="line-search-descent-methods.html#cb6-3"></a></span>
<span id="cb6-4"><a href="line-search-descent-methods.html#cb6-4"></a>Df <span class="op">=</span> grad(func) <span class="co"># Gradient of the objective function</span></span></code></pre></div>
<p>We will use <code>numpy</code>â€™s <code>linalg.norm()</code> function for calculating the norm of a vector or a matrix.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="line-search-descent-methods.html#cb7-1"></a>NORM <span class="op">=</span> np.linalg.norm</span></code></pre></div>
<p>Suppose, we want to calculate the norm of the vector, <span class="math inline">\(v = \begin{bmatrix}-1.1 \\ 2.3 \\ 0.1 \end{bmatrix}\)</span>. We use the <code>NORM()</code> function we just defined:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="line-search-descent-methods.html#cb8-1"></a>v <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">1.1</span>, <span class="fl">2.3</span>, <span class="fl">0.1</span>])</span></code></pre></div>
<p>Next, we visualize the objective function on a two dimensional space identical to the one that we already generated above, for setting up an environment to visualize the trajectory of the optimization. And finally, we write the Python function <code>steepest_descent()</code> implementing the <em>steepest descent algorithm</em>:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="line-search-descent-methods.html#cb9-1"></a></span>
<span id="cb9-2"><a href="line-search-descent-methods.html#cb9-2"></a>x1 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">100</span>)</span>
<span id="cb9-3"><a href="line-search-descent-methods.html#cb9-3"></a>x2 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">100</span>)</span>
<span id="cb9-4"><a href="line-search-descent-methods.html#cb9-4"></a>z <span class="op">=</span> np.zeros(([<span class="bu">len</span>(x1), <span class="bu">len</span>(x2)]))</span>
<span id="cb9-5"><a href="line-search-descent-methods.html#cb9-5"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(x1)):</span>
<span id="cb9-6"><a href="line-search-descent-methods.html#cb9-6"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(x2)):</span>
<span id="cb9-7"><a href="line-search-descent-methods.html#cb9-7"></a>        z[j, i] <span class="op">=</span> func([x1[i], x2[j]])</span>
<span id="cb9-8"><a href="line-search-descent-methods.html#cb9-8"></a></span>
<span id="cb9-9"><a href="line-search-descent-methods.html#cb9-9"></a>contours<span class="op">=</span>plt.contour(x1, x2, z, <span class="dv">100</span>, cmap<span class="op">=</span>plt.cm.gnuplot)</span>
<span id="cb9-10"><a href="line-search-descent-methods.html#cb9-10"></a>plt.clabel(contours, inline<span class="op">=</span><span class="dv">1</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="line-search-descent-methods.html#cb10-1"></a>plt.xlabel(<span class="st">&quot;$x_1$ -&gt;&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="line-search-descent-methods.html#cb11-1"></a>plt.ylabel(<span class="st">&quot;$x_2$ -&gt;&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="line-search-descent-methods.html#cb12-1"></a><span class="kw">def</span> steepest_descent(Xj, tol_1, tol_2, tol_3, alpha_1, alpha_2):</span>
<span id="cb12-2"><a href="line-search-descent-methods.html#cb12-2"></a>    x1 <span class="op">=</span> [Xj[<span class="dv">0</span>]]</span>
<span id="cb12-3"><a href="line-search-descent-methods.html#cb12-3"></a>    x2 <span class="op">=</span> [Xj[<span class="dv">1</span>]]</span>
<span id="cb12-4"><a href="line-search-descent-methods.html#cb12-4"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb12-5"><a href="line-search-descent-methods.html#cb12-5"></a>        D <span class="op">=</span> Df(Xj)</span>
<span id="cb12-6"><a href="line-search-descent-methods.html#cb12-6"></a>        delta <span class="op">=</span> <span class="op">-</span> D <span class="op">/</span> NORM(D) <span class="co"># Selection of the direction of the steepest descent</span></span>
<span id="cb12-7"><a href="line-search-descent-methods.html#cb12-7"></a>        </span>
<span id="cb12-8"><a href="line-search-descent-methods.html#cb12-8"></a>        start_point <span class="op">=</span> Xj <span class="co"># Start point for step length selection </span></span>
<span id="cb12-9"><a href="line-search-descent-methods.html#cb12-9"></a>        beta <span class="op">=</span> line_search(f<span class="op">=</span>func, myfprime<span class="op">=</span>Df, xk<span class="op">=</span>start_point, pk<span class="op">=</span>delta, c1<span class="op">=</span>alpha_1, c2<span class="op">=</span>alpha_2)[<span class="dv">0</span>] <span class="co"># Selecting the step length</span></span>
<span id="cb12-10"><a href="line-search-descent-methods.html#cb12-10"></a>        <span class="cf">if</span> beta<span class="op">!=</span><span class="va">None</span>:</span>
<span id="cb12-11"><a href="line-search-descent-methods.html#cb12-11"></a>            X <span class="op">=</span> Xj<span class="op">+</span> beta<span class="op">*</span>delta</span>
<span id="cb12-12"><a href="line-search-descent-methods.html#cb12-12"></a>        <span class="cf">if</span> NORM(X <span class="op">-</span> Xj) <span class="op">&lt;</span> tol_1 <span class="kw">and</span> NORM(Df(X)) <span class="op">&lt;</span> tol_2 <span class="kw">or</span> <span class="bu">abs</span>(func(X) <span class="op">-</span> func(Xj)) <span class="op">&lt;</span> tol_3:</span>
<span id="cb12-13"><a href="line-search-descent-methods.html#cb12-13"></a>            x1 <span class="op">+=</span> [X[<span class="dv">0</span>], ]</span>
<span id="cb12-14"><a href="line-search-descent-methods.html#cb12-14"></a>            x2 <span class="op">+=</span> [X[<span class="dv">1</span>], ]</span>
<span id="cb12-15"><a href="line-search-descent-methods.html#cb12-15"></a>            plt.plot(x1, x2, <span class="st">&quot;rx-&quot;</span>, ms<span class="op">=</span><span class="fl">5.5</span>) <span class="co"># Plot the final collected data showing the trajectory of optimization</span></span>
<span id="cb12-16"><a href="line-search-descent-methods.html#cb12-16"></a>            plt.show()</span>
<span id="cb12-17"><a href="line-search-descent-methods.html#cb12-17"></a>            <span class="cf">return</span> X, func(X)</span>
<span id="cb12-18"><a href="line-search-descent-methods.html#cb12-18"></a>        <span class="cf">else</span>:</span>
<span id="cb12-19"><a href="line-search-descent-methods.html#cb12-19"></a>            Xj <span class="op">=</span> X</span>
<span id="cb12-20"><a href="line-search-descent-methods.html#cb12-20"></a>            x1 <span class="op">+=</span> [Xj[<span class="dv">0</span>], ]</span>
<span id="cb12-21"><a href="line-search-descent-methods.html#cb12-21"></a>            x2 <span class="op">+=</span> [Xj[<span class="dv">1</span>], ]</span></code></pre></div>
<p>According to our example, we set our parameter values and pass them to the <code>steepest_descent()</code> function:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="line-search-descent-methods.html#cb13-1"></a>steepest_descent(np.array([<span class="fl">1.1</span>, <span class="fl">2.2</span>]), <span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">4</span>, <span class="fl">0.212</span>)</span></code></pre></div>
<pre><code>## (array([2.99995879, 2.00033681]), 1.7139960678005175e-06)</code></pre>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-3.png" width="672" /></p>
<p>We see that for our choice of parameters, the algorithm has converged to the minimizer <span class="math inline">\(\mathbb{x}^* \sim \begin{bmatrix} 3 \\ 2 \end{bmatrix}\)</span> for our objective function, where the function value is <span class="math inline">\(0\)</span>. The optimization data has been collected and shown below:</p>
<pre><code>## +----+---------+---------+--------------+------------+
## |    |     x_1 |     x_2 |         f(X) |   ||grad|| |
## |----+---------+---------+--------------+------------|
## |  0 | 1.1     | 2.2     | 58.7317      | 43.1512    |
## |  1 | 2.13886 | 2.91687 | 25.6095      | 42.178     |
## |  2 | 2.67758 | 2.07438 |  3.08461     | 19.2027    |
## |  3 | 2.90745 | 2.11918 |  0.341516    |  4.87819   |
## |  4 | 3.00201 | 2.06207 |  0.0700952   |  2.64486   |
## |  5 | 2.97935 | 2.0257  |  0.0164196   |  1.107     |
## |  6 | 2.99852 | 2.01658 |  0.00430094  |  0.584801  |
## |  7 | 2.99437 | 2.0065  |  0.00115975  |  0.306008  |
## |  8 | 2.99948 | 2.00454 |  0.000314508 |  0.153945  |
## |  9 | 2.99846 | 2.00174 |  8.54359e-05 |  0.0840138 |
## | 10 | 2.99985 | 2.00124 |  2.32175e-05 |  0.0415497 |
## | 11 | 2.99958 | 2.00047 |  6.31032e-06 |  0.022927  |
## | 12 | 2.99996 | 2.00034 |  1.714e-06   |  0.0112511 |
## +----+---------+---------+--------------+------------+</code></pre>
<p>We can further experiment by changing the parameters, for example, the initial iterate and/or the constants for the <em>strong Wolfe conditions</em> and try to visualize the optimization steps and check for any errors that might arise.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-12" class="theorem"><strong>Theorem 4.1  </strong></span>For a sequence of line search descent directions <span class="math inline">\(\mathbb{\delta}_i\)</span>, where, <span class="math inline">\(i = 1, \ldots, n\)</span>, the successive directions are orthogonal to each other.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Suppose, the line search at iterate <span class="math inline">\(\mathbb{x}_{j-1}\)</span> gives a descent direction <span class="math inline">\(\mathbb{\delta}_j\)</span>, which yields us the next iterate <span class="math inline">\(\mathbb{x}_j\)</span>. The descent direction <span class="math inline">\(\mathbb{\delta}_j\)</span> is an optimum value, if the following condition is satisfied:
<span class="math display" id="eq:13">\[\begin{equation}

    \nabla^Tf(\mathbb{x}_j)\mathbb{\delta}_j = 0 \tag{4.13}
\end{equation}\]</span></p>
<p>Also, from Eq. <a href="line-search-descent-methods.html#eq:12">(4.12)</a>, we have,
<span class="math display" id="eq:14">\[\begin{equation}
    \mathbb{\delta}_{j+1} = - \frac{\nabla f(\mathbb{x}_j)}{\|\nabla f(\mathbb{x}_j)\|} \tag{4.14}
\end{equation}\]</span></p>
<p>Now, multiplying Eq. <a href="line-search-descent-methods.html#eq:13">(4.13)</a> and Eq. <a href="line-search-descent-methods.html#eq:14">(4.14)</a>, we get,
<span class="math display" id="eq:15">\[\begin{equation}
    \mathbb{\delta_{j+1}}^T\mathbb{\delta_j} = 0 \tag{4.15}
\end{equation}\]</span></p>
Eq.<a href="line-search-descent-methods.html#eq:15">(4.15)</a> tells us that the successive search directions are orthogonal to each other. This completes our proof of <strong>Theorem 4.1</strong>.
</div>

<p>We will now look into the rate of convergence of the <em>steepest descent algorithm</em>.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-14" class="theorem"><strong>Theorem 4.2  </strong></span>Given a nonlinear objective function <span class="math inline">\(f(\mathbb{x}), \mathbb{x} \in \mathbb{R}^n\)</span>, the steepest descent algorithm that computes its minimizer, exhibits a linear convergence in the function values for a constant <span class="math inline">\(\Delta &lt;1\)</span> such that the following condition for the experiments, is satisfied:
<span class="math display">\[\begin{equation}
    \frac{f(\mathbb{x}_j) - f(\mathbb{x}^*)}{f(\mathbb{x}_{j-1})-f(\mathbb{x}^*)} \leq \Delta \nonumber 
\end{equation}\]</span>
where, <span class="math inline">\(j\)</span> is sufficiently large and <span class="math inline">\(\mathbb{x}^*\)</span> denotes the minimizer
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> For the proof, let us consider the ideal case where the objective function is quadratic, i.e,
<span class="math display" id="eq:16">\[\begin{equation}
    f(\mathbb{x}) = \frac{1}{2}\mathbb{x}^T\mathbb{A}\mathbb{x} + \mathbb{b}^T\mathbb{x} \tag{4.16}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(\mathbb{A}\)</span> is a positive definite symmetric matrix. The optimum solution of <span class="math inline">\(f(\mathbb{x})\)</span> is computed as:
<span class="math display" id="eq:17">\[\begin{equation}
    -\mathbb{A}\mathbb{x}^*= \mathbb{b} \tag{4.17}
\end{equation}\]</span></p>
<p>This yields us,
<span class="math display" id="eq:18">\[\begin{equation}
    f(\mathbb{x}^*) = -\frac{1}{2}\mathbb{b}^T\mathbb{A}^{-1}\mathbb{b} \tag{4.18}
\end{equation}\]</span></p>
<p>Let the direction of steepest descent be,
<span class="math display" id="eq:19">\[\begin{equation}
    \mathbb{\delta} = -\nabla^Tf(\mathbb{x}) = -\mathbb{A}\mathbb{x} - \mathbb{b} \tag{4.19}
\end{equation}\]</span></p>
<p>letting <span class="math inline">\(\mathbb{x}\)</span> be the current iterate and <span class="math inline">\(\beta\)</span> be the step length, we have,
<span class="math display" id="eq:21" id="eq:20">\[\begin{align}
    f(\mathbb{x} + \beta \mathbb{\delta}) &amp;= \frac{1}{2}(\mathbb{x} + \beta \mathbb{\delta})^T\mathbb{A}(\mathbb{x} + \beta \mathbb{\delta}) + \mathbb{b}^T(\mathbb{x} + \beta \mathbb{\delta}) \tag{4.20}\\
    &amp;= \frac{1}{2}\mathbb{x}^T\mathbb{A}\mathbb{x} + \mathbb{\beta}\mathbb{\delta}^T\mathbb{A}\mathbb{x} + \frac{1}{2}\beta^2\mathbb{\delta}^T\mathbb{A}\mathbb{\delta}+\mathbb{b}^T\mathbb{x} + \beta\mathbb{b}^T\mathbb{\delta} \tag{4.21}
\end{align}\]</span></p>
<p>Now from Eq. <a href="line-search-descent-methods.html#eq:16">(4.16)</a>, we can write Eq. <a href="line-search-descent-methods.html#eq:21">(4.21)</a> as,
<span class="math display" id="eq:22">\[\begin{align}
    f(\mathbb{x} + \beta\mathbb{\delta}) &amp;= f(\mathbb{x}) + \frac{1}{2}\beta^2\mathbb{\delta}^T\mathbb{A}\mathbb{\delta}+ \beta\mathbb{\delta}^T\mathbb{A}\mathbb{x} + \beta\mathbb{b}^T\mathbb{\delta} \tag{4.22}
\end{align}\]</span></p>
<p>We see that using Eq. <a href="line-search-descent-methods.html#eq:19">(4.19)</a>, we can write,
<span class="math display" id="eq:23">\[\begin{align}
    \beta\mathbb{\delta}^T\mathbb{A}\mathbb{x} + \beta\mathbb{b}^T\mathbb{\delta} = \beta\mathbb{\delta}^T\mathbb{A}\mathbb{x} + \beta\mathbb{\delta}^T\mathbb{b}= - \beta\mathbb{\delta}^T\mathbb{\delta} \tag{4.23}
\end{align}\]</span></p>
<p>So using Eq. <a href="line-search-descent-methods.html#eq:23">(4.23)</a> in Eq. <a href="line-search-descent-methods.html#eq:22">(4.22)</a>, we will have,
<span class="math display" id="eq:24">\[\begin{equation}
    f(\mathbb{x} + \beta\mathbb{\delta})=f(\mathbb{x})-\beta\mathbb{\delta}^T\mathbb{\delta}+\frac{1}{2}\beta^2\mathbb{\delta}^T\mathbb{A}\mathbb{\delta} \tag{4.24}
\end{equation}\]</span></p>
<p>Now differentiating Eq. <a href="line-search-descent-methods.html#eq:24">(4.24)</a> with respect to <span class="math inline">\(\beta\)</span> and setting it equal to <span class="math inline">\(0\)</span>, we have,
<span class="math display" id="eq:25">\[\begin{align}
\frac{\partial f(\mathbb{x} + \beta\mathbb{\delta})}{\partial \beta} = -\mathbb{\delta}^T\mathbb{\delta} + \beta\mathbb{\delta}^T\mathbb{A}\mathbb{\delta}=0 \tag{4.25}
\end{align}\]</span>
This gives,
<span class="math display" id="eq:26">\[\begin{equation}
    \beta = \frac{\mathbb{\delta}^T\mathbb{\delta}}{\mathbb{\delta}^T\mathbb{A}\mathbb{\delta}} \tag{4.26}
\end{equation}\]</span></p>
<p>Let the next iterate the algorithm generates from <span class="math inline">\(\mathbb{x}\)</span> be <span class="math inline">\(\mathbb{x}&#39;\)</span>, given by,
<span class="math display" id="eq:27">\[\begin{equation}
    \mathbb{x}&#39; = \mathbb{x} + \frac{\mathbb{\delta}^T\mathbb{\delta}}{\mathbb{\delta}^T\mathbb{A}\mathbb{\delta}}\mathbb{\delta} \tag{4.27}
\end{equation}\]</span></p>
<p>So,
<span class="math display" id="eq:28">\[\begin{align}
    f(\mathbb{x}&#39;) = f(\mathbb{x} + \beta\mathbb{\delta}) &amp;=&amp; f(\mathbb{x}) - \beta\mathbb{\delta}^T\mathbb{\delta}+\frac{1}{2}\beta^2\mathbb{\delta}^T\mathbb{A}\mathbb{\delta} \nonumber \\
    &amp;=&amp; f(\mathbb{x}) - \beta^2\mathbb{\delta}^T\mathbb{A}\mathbb{\delta} + \frac{1}{2}\beta^2\mathbb{\delta}^T\mathbb{A}\mathbb{\delta} \tag{4.28}
\end{align}\]</span></p>
<p>We get the above equation, using the fact from Eq. <a href="line-search-descent-methods.html#eq:26">(4.26)</a> that
<span class="math display" id="eq:29">\[\begin{equation}
    \mathbb{\delta}^T\mathbb{\delta} = \beta\mathbb{\delta}^T\mathbb{A}\mathbb{\delta} \tag{4.29}
\end{equation}\]</span>
So, Eq. <a href="line-search-descent-methods.html#eq:28">(4.28)</a> reduces to,
<span class="math display" id="eq:30">\[\begin{align}
    f(\mathbb{x}&#39;) &amp;= f(\mathbb{x}) - \frac{1}{2}\beta^2\mathbb{\delta}^T\mathbb{A}\mathbb{\delta} \nonumber \\
    &amp;= f(\mathbb{x}) - \frac{1}{2}\frac{(\mathbb{\delta}^T\mathbb{\delta})^2}{\mathbb{\delta}^T\mathbb{A}\mathbb{\delta}} \tag{4.30}
\end{align}\]</span></p>
<p>Therefore, we have,
<span class="math display" id="eq:31">\[\begin{align}
    \frac{f(\mathbb{x}&#39;)-f(\mathbb{x}^*)}{f(\mathbb{x}) - f(\mathbb{x}^*)} &amp;= \frac{f(\mathbb{x}) - \frac{1}{2}\frac{(\mathbb{\delta}^T\mathbb{\delta})^2}{\mathbb{\delta}^T\mathbb{A}\mathbb{\delta}}-f(\mathbb{x}^*)}{f(\mathbb{x}) - f(\mathbb{x}^*)} \tag{4.31}
\end{align}\]</span></p>
<p>Now, using Eq. <a href="line-search-descent-methods.html#eq:16">(4.16)</a> and Eq. <a href="line-search-descent-methods.html#eq:18">(4.18)</a>, we have,
<span class="math display" id="eq:32">\[\begin{equation}
    \frac{f(\mathbb{x}&#39;)-f(\mathbb{x}^*)}{f(\mathbb{x}) - f(\mathbb{x}^*)} = 1 - \frac{\frac{1}{2}\frac{(\mathbb{\delta}^T\mathbb{\delta})^2}{\mathbb{\delta}^T\mathbb{A}\mathbb{\delta}}}{\frac{1}{2}\mathbb{x}^T\mathbb{A}\mathbb{x} + \mathbb{b}^T\mathbb{x}+\frac{1}{2}\mathbb{b}^T\mathbb{A}^{-1}\mathbb{b}} \tag{4.32}
\end{equation}\]</span></p>
<p>As <span class="math inline">\(\mathbb{A}\)</span> is symmetric, <span class="math inline">\(\mathbb{A}^T = \mathbb{A}\)</span>. Also, from the property of dot product, we know that <span class="math inline">\(\mathbb{b}^T\mathbb{x} = \mathbb{x}^T\mathbb{b}\)</span>. We now use these properties to simplify the denominator of the second element of Eq. <a href="line-search-descent-methods.html#eq:32">(4.32)</a>.</p>
<p><span class="math display" id="eq:33">\[\begin{align}
    \frac{1}{2}\mathbb{x}^T\mathbb{A}\mathbb{x} + \mathbb{b}^T\mathbb{x}+\frac{1}{2}\mathbb{b}^T\mathbb{A}^{-1}\mathbb{b} &amp;= \frac{1}{2}[\mathbb{x}^T\mathbb{A}\mathbb{x} + \mathbb{x}^T\mathbb{b} + \mathbb{b}^T\mathbb{x} + \mathbb{b}^T\mathbb{A}^{-1}\mathbb{b}] \nonumber \\
    &amp;= \frac{1}{2}[(\mathbb{x}^T\mathbb{A}+\mathbb{b}^T)(\mathbb{x}+\mathbb{A}^{-1}\mathbb{b})] \nonumber \\
    &amp;= \frac{1}{2}[(\mathbb{x}^T\mathbb{A}^T+\mathbb{b}^T)(\mathbb{x}+\mathbb{A}^{-1}\mathbb{b})] \nonumber \\
    &amp;= \frac{1}{2}(\mathbb{A}\mathbb{x} + \mathbb{b})^T(\mathbb{x}+\mathbb{A}^{-1}\mathbb{b}) \nonumber \\
    &amp;= \frac{1}{2}(\mathbb{A}\mathbb{x} + \mathbb{b})^T\mathbb{A}^{-1}(\mathbb{A}\mathbb{x} + \mathbb{b}) \tag{4.33}
\end{align}\]</span></p>
<p>Therefore, we can modify Eq. <a href="line-search-descent-methods.html#eq:32">(4.32)</a> as,
<span class="math display" id="eq:34">\[\begin{equation}
    \frac{f(\mathbb{x}&#39;)-f(\mathbb{x}^*)}{f(\mathbb{x}) - f(\mathbb{x}^*)} = 1 - \frac{\frac{1}{2}\frac{(\mathbb{\delta}^T\mathbb{\delta})^2}{\mathbb{\delta}^T\mathbb{A}\mathbb{\delta}}}{\frac{1}{2}(\mathbb{A}\mathbb{x} + \mathbb{b})^T\mathbb{A}^{-1}(\mathbb{A}\mathbb{x} + \mathbb{b})} \tag{4.34}
\end{equation}\]</span></p>
<p>Now, using Eq.<a href="line-search-descent-methods.html#eq:19">(4.19)</a> we can write,
<span class="math display" id="eq:35">\[\begin{eqnarray}
    \frac{f(\mathbb{x}&#39;)-f(\mathbb{x}^*)}{f(\mathbb{x}) - f(\mathbb{x}^*)} = 1 - \frac{(\mathbb{\delta}^T\mathbb{\delta})^2}{(\mathbb{\delta}^T\mathbb{A}\mathbb{\delta})(\mathbb{\delta}^T\mathbb{A}^{-1}\mathbb{\delta})} = 1 - \frac{1}{\zeta} \tag{4.35}
\end{eqnarray}\]</span></p>
<p>where,
<span class="math display" id="eq:36">\[\begin{equation}
    \zeta = \frac{(\mathbb{\delta}^T\mathbb{A}\mathbb{\delta})(\mathbb{\delta}^T\mathbb{A}^{-1}\mathbb{\delta})}{(\mathbb{\delta}^T\mathbb{\delta})^2} \tag{4.36}
\end{equation}\]</span></p>
<p>We want <span class="math inline">\(\zeta\)</span> to be small such that we get a fast linear convergence for the algorithm. To compute an upper bound on the value of <span class="math inline">\(\zeta\)</span>, we use the <em>Kantorovich Inequality</em>, [ref, lecture note by Robert M. Freund, The steepest descent algorithm for Unconstrained Optimization and a Bisection Line-search Method, MIT] (look into the lecture note for the derivation of the Kantorovich inequality], given by:
<span class="math display" id="eq:37">\[\begin{equation}
    \zeta \leq \frac{(\lambda_{max} + \lambda_{min})^2}{4\lambda_{max}\lambda_{min}} \tag{4.37}
\end{equation}\]</span></p>
<p>where, <span class="math inline">\(\lambda_{max}\)</span> and <span class="math inline">\(\lambda_{min}\)</span> are the largest and the smallest eigenvalues of the matrix <span class="math inline">\(\mathbb{A}\)</span>. Using Eq. <a href="line-search-descent-methods.html#eq:37">(4.37)</a> in Eq. <a href="line-search-descent-methods.html#eq:35">(4.35)</a>, we have,</p>
<p><span class="math display" id="eq:38">\[\begin{align}
    \frac{f(\mathbb{x}&#39;)-f(\mathbb{x}^*)}{f(\mathbb{x}) - f(\mathbb{x}^*)} &amp;= 1 - \frac{1}{\zeta} \nonumber \\
    &amp;\leq 1 - \frac{(\lambda_{max} + \lambda_{min})^2}{4\lambda_{max}\lambda_{min}} \nonumber \\
    &amp;= \frac{(\lambda_{max} - \lambda_{min})^2}{(\lambda_{max} + \lambda_{min})^2}\nonumber \\ 
    &amp;= \frac{(\frac{\lambda_{max}}{\lambda_{min}}-1)^2}{(\frac{\lambda_{max}}{\lambda_{min}}+1)^2} \nonumber \\
    &amp;= \Delta \tag{4.38}
\end{align}\]</span></p>
<p><span class="math inline">\(\Delta\)</span>is called the <em>convergence constant</em>. The eigenvalues of <span class="math inline">\(A\)</span> follows the following chain of inequalities:
<span class="math display" id="eq:39">\[\begin{equation}
    \lambda_{max} = \lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n = \lambda_{min}&gt;0 \tag{4.39}
\end{equation}\]</span></p>
<p>From Eq. <a href="line-search-descent-methods.html#eq:39">(4.39)</a> we can say that <span class="math inline">\(\frac{\lambda_{max}}{\lambda_{min}}\)</span> is at least <span class="math inline">\(1\)</span>. There arises two conditions:</p>
<ul>
<li>If <span class="math inline">\(\frac{\lambda_{max}}{\lambda_{min}}\)</span> is small, then <span class="math inline">\(\Delta\)</span> will be much less than <span class="math inline">\(1\)</span>,</li>
<li>If <span class="math inline">\(\frac{\lambda_{max}}{\lambda_{min}}\)</span> is large, then <span class="math inline">\(\Delta\)</span> will be only slightly less than 1.
</div></li>
</ul>
</div>
<div id="conjugate-gradient-methods" class="section level2">
<h2><span class="header-section-number">4.4</span> Conjugate Gradient Methods</h2>
<p>The performance of the <em>steepest descent algorithm</em> is often not up the mark, generating a zigzag path of steps decreasing in size. This leads to a slow convergence of the problem at hand when the decision variables are poorly scaled. The algorithm might not always efficiently converge within a finite number of steps. This inefficient convergence also happens when the algorithm works on a positive-definite quadratic objective functions for some starting points <span class="math inline">\(\mathbb{x}_0\)</span>. To overcome the mentioned inefficiencies, we will introduce another class of first order line search gradient descent algorithms called the <em>conjugate gradient algorithms</em>, which will always converge to the minimizer of the given objective function within finite number of iterations, irrespective of the scaling of the decision variables. The <em>conjugate gradient methods</em> form an important class of algorithms and we will dedicate our next chapter discussing this class of methods.</p>
</div>
<div id="second-order-line-search-gradient-descent-method" class="section level2">
<h2><span class="header-section-number">4.5</span> Second Order Line Search Gradient Descent Method</h2>
<p>In <strong>Section 4.3</strong> we have introduced the <em>first order line search gradient descent method</em>. We will now study methods which uses the Hessian of the objective function, <span class="math inline">\(\mathbb{H}f(\mathbb{x})\)</span>, to compute the line search. At each step, the search is given by,</p>
<p><span class="math display" id="eq:40">\[\begin{equation}
    \mathbb{\delta} = -[\mathbb{H}f]^{-1}(\mathbb{x})\nabla f(\mathbb{x}) \tag{4.40}
\end{equation}\]</span></p>
<p>The <em>Newtonâ€™s method</em> for a nonlinear objective function of a single variable has already been introduced in <strong>Section 3.7</strong>, as given by Eq. <a href="line-search-descent-methods.html#eq:40">(4.40)</a>. So, for a multivariate objective function, it modifies to
<span class="math display" id="eq:41">\[\begin{equation}
    \mathbb{x}_j = \mathbb{x}_{j-1} - [\mathbb{H}f]^{-1}(\mathbb{x}_{j-1})\nabla f(\mathbb{x}_{j-1}),\ j=1, 2, \ldots \tag{4.41}
\end{equation}\]</span></p>
<p>This is used for solving the problem <span class="math inline">\(\nabla f(\mathbb{x})=0\)</span> iteratively, given the starting iterate <span class="math inline">\(\mathbb{x}_0\)</span> is provided. As we have already discussed in the last chapter, the <em>Newtonâ€™s method</em> follows quadratic convergence. But convergence is not always guaranteed even from an iterate close to the minimizer, since the Hessian given by <span class="math inline">\(\mathbb{H}f(\mathbb{x})\)</span> may not always be positive definite. Now, to overcome this problem, the search given by Eq. <a href="line-search-descent-methods.html#eq:40">(4.40)</a> is instead used as a descent direction in our <em>line search descent direction</em>. At the <span class="math inline">\(j^{th}\)</span> step, the descent direction is thus given by,
<span class="math display" id="eq:42">\[\begin{equation}
    \mathbb{\delta}_j = -[\mathbb{H}f]^{-1}(\mathbb{x}_{j-1})\nabla f(\mathbb{x}_{j-1}) \tag{4.42}
\end{equation}\]</span></p>
<p>Finally, the step length <span class="math inline">\(\beta_j\)</span> is selected after solving the one dimensional optimization task given by Eq. <a href="line-search-descent-methods.html#eq:3">(4.3)</a>. This modified version is known as the <em>modified Newtonâ€™s method</em>. The algorithm describing the <em>modified Newtonâ€™s method</em> is given below:</p>
<p><img src="img%2016.png" /></p>

<div class="example">
<p><span id="exm:unnamed-chunk-16" class="example"><strong>Example 4.3  </strong></span>Let us consider Bealeâ€™s function as the objective function, having the form,
<span class="math display" id="eq:43">\[\begin{equation}
    f(x_1, x_2) = (1.5 - x_1(1-x_2))^2 + (2.25 - x_1(1-x_2^2))^2 + (2.625 - x_1(1-x_2^3))^2 \tag{4.43}
\end{equation}\]</span></p>
The function has one global minimizer, <span class="math inline">\(f(3, 0.5) = 0\)</span>. We will implement the <em>modified Newton algorithm</em> in Python and figure out the minimizer of the objective function for some input parameters. Let the starting iterate be <span class="math inline">\(\mathbb{x}_j = \begin{bmatrix}1.8 \\ 0.8 \end{bmatrix}\)</span> , the tolerances be <span class="math inline">\(\epsilon_1=\epsilon_2=\epsilon_3=10^{-5}\)</span> and the constants to be used in determining the step length using the <em>strong Wolfe conditions</em> be <span class="math inline">\(\alpha_1=10^{-4}\)</span> and <span class="math inline">\(\alpha_2=0.25\)</span>. First we define Bealeâ€™s function, its gradient and its Hessian using Pythonâ€™s <code>autograd</code> package.
</div>

<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="line-search-descent-methods.html#cb16-1"></a><span class="kw">def</span> func(x): <span class="co"># Objective function (Beale&#39;s function)</span></span>
<span id="cb16-2"><a href="line-search-descent-methods.html#cb16-2"></a>    <span class="cf">return</span> (<span class="fl">1.5</span> <span class="op">-</span> x[<span class="dv">0</span>]<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>x[<span class="dv">1</span>]))<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> (<span class="fl">2.25</span><span class="op">-</span>x[<span class="dv">0</span>]<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>x[<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span>))<span class="op">**</span><span class="dv">2</span><span class="op">+</span>(<span class="fl">2.625</span><span class="op">-</span>x[<span class="dv">0</span>]<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>x[<span class="dv">1</span>]<span class="op">**</span><span class="dv">3</span>))<span class="op">**</span><span class="dv">2</span></span>
<span id="cb16-3"><a href="line-search-descent-methods.html#cb16-3"></a>Df <span class="op">=</span> grad(func) <span class="co"># Gradient of the objective function</span></span>
<span id="cb16-4"><a href="line-search-descent-methods.html#cb16-4"></a>Hf <span class="op">=</span> jacobian(Df) <span class="co"># Hessian of the objective function</span></span></code></pre></div>
<p>Now, we define the Python function <code>modified_newton()</code>.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="line-search-descent-methods.html#cb17-1"></a><span class="kw">def</span> modified_newton(Xj, tol_1, tol_2, tol_3, alpha_1, alpha_2):</span>
<span id="cb17-2"><a href="line-search-descent-methods.html#cb17-2"></a>    x1 <span class="op">=</span> [Xj[<span class="dv">0</span>]]</span>
<span id="cb17-3"><a href="line-search-descent-methods.html#cb17-3"></a>    x2 <span class="op">=</span> [Xj[<span class="dv">1</span>]]</span>
<span id="cb17-4"><a href="line-search-descent-methods.html#cb17-4"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb17-5"><a href="line-search-descent-methods.html#cb17-5"></a>        D <span class="op">=</span> Df(Xj)</span>
<span id="cb17-6"><a href="line-search-descent-methods.html#cb17-6"></a>        inv_hess <span class="op">=</span> np.linalg.inv(Hf(Xj)) <span class="co"># Compute the inverse of the Hessian at the given iterate</span></span>
<span id="cb17-7"><a href="line-search-descent-methods.html#cb17-7"></a>        delta <span class="op">=</span> <span class="op">-</span>inv_hess.dot(D) <span class="co"># Selection of the direction of the steepest descent</span></span>
<span id="cb17-8"><a href="line-search-descent-methods.html#cb17-8"></a>        </span>
<span id="cb17-9"><a href="line-search-descent-methods.html#cb17-9"></a>        </span>
<span id="cb17-10"><a href="line-search-descent-methods.html#cb17-10"></a>        start_point <span class="op">=</span> Xj <span class="co"># Start point for step length selection </span></span>
<span id="cb17-11"><a href="line-search-descent-methods.html#cb17-11"></a>        beta <span class="op">=</span> line_search(f<span class="op">=</span>func, myfprime<span class="op">=</span>Df, xk<span class="op">=</span>start_point, pk<span class="op">=</span>delta, c1<span class="op">=</span>alpha_1, c2<span class="op">=</span>alpha_2)[<span class="dv">0</span>] <span class="co"># Selecting the step length</span></span>
<span id="cb17-12"><a href="line-search-descent-methods.html#cb17-12"></a>        <span class="cf">if</span> beta<span class="op">!=</span><span class="va">None</span>:</span>
<span id="cb17-13"><a href="line-search-descent-methods.html#cb17-13"></a>            X <span class="op">=</span> Xj<span class="op">+</span> beta<span class="op">*</span>delta</span>
<span id="cb17-14"><a href="line-search-descent-methods.html#cb17-14"></a>        <span class="cf">if</span> NORM(X <span class="op">-</span> Xj) <span class="op">&lt;</span> tol_1 <span class="kw">and</span> NORM(Df(X)) <span class="op">&lt;</span> tol_2 <span class="kw">or</span> <span class="bu">abs</span>(func(X) <span class="op">-</span> func(Xj)) <span class="op">&lt;</span> tol_3:</span>
<span id="cb17-15"><a href="line-search-descent-methods.html#cb17-15"></a>            x1 <span class="op">+=</span> [X[<span class="dv">0</span>], ]</span>
<span id="cb17-16"><a href="line-search-descent-methods.html#cb17-16"></a>            x2 <span class="op">+=</span> [X[<span class="dv">1</span>], ]</span>
<span id="cb17-17"><a href="line-search-descent-methods.html#cb17-17"></a>            <span class="cf">return</span> X, func(X)</span>
<span id="cb17-18"><a href="line-search-descent-methods.html#cb17-18"></a>        <span class="cf">else</span>:</span>
<span id="cb17-19"><a href="line-search-descent-methods.html#cb17-19"></a>            Xj <span class="op">=</span> X</span>
<span id="cb17-20"><a href="line-search-descent-methods.html#cb17-20"></a>            x1 <span class="op">+=</span> [Xj[<span class="dv">0</span>], ]</span>
<span id="cb17-21"><a href="line-search-descent-methods.html#cb17-21"></a>            x2 <span class="op">+=</span> [Xj[<span class="dv">1</span>], ]</span></code></pre></div>
<p>According to our example, we set our parameter values and pass them to the <code>modified_newton()</code> function:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="line-search-descent-methods.html#cb18-1"></a>modified_newton(np.array([<span class="fl">1.8</span>, <span class="fl">0.8</span>]), <span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">4</span>, <span class="fl">0.25</span>)</span></code></pre></div>
<pre><code>## (array([3.00000125, 0.5000004 ]), 4.274176325112278e-13)</code></pre>
<p>We notice that, for our choice of parameters, the algorithm has converged to the minimizer <span class="math inline">\(\mathbb{x}^* \sim \begin{bmatrix}3 \\ 0.5\end{bmatrix}\)</span> and the function value at the minimizer is <span class="math inline">\(\sim 0\)</span>. The optimization data has been collected and shown below:</p>
<pre><code>## +----+---------+----------+-------------+--------------+
## |    |     x_1 |      x_2 |        f(X) |     ||grad|| |
## |----+---------+----------+-------------+--------------|
## |  0 | 1.8     | 0.8      | 6.91662     | 25.6193      |
## |  1 | 3.54638 | 0.634246 | 0.0583991   |  2.48467     |
## |  2 | 3.23522 | 0.545634 | 0.00855784  |  0.473631    |
## |  3 | 2.9828  | 0.494168 | 0.000101282 |  0.0699236   |
## |  4 | 3.00048 | 0.500196 | 1.6893e-07  |  0.00355465  |
## |  5 | 3       | 0.5      | 4.27418e-13 |  4.09398e-06 |
## +----+---------+----------+-------------+--------------+</code></pre>
<p>We notice that in <em>Newton method</em> and the <em>modified Newton method</em>, there are requirements to compute the Hessian matrix <span class="math inline">\(\mathbb{H}f(\mathbb{x})\)</span> and also solve a <span class="math inline">\(n \times n\)</span> linear system of equations: <span class="math inline">\(\mathbb{H}f(\mathbb{x})(\mathbb{x}_j - \mathbb{x}_{j-1}) = -\nabla f(\mathbb{x})\)</span> at each iterate <span class="math inline">\(j\)</span>. These will lead to computationally expensive evaluations for large <span class="math inline">\(n\)</span>, the reason being that an order <span class="math inline">\(n^3\)</span> multiplication computations are required during numerically solving the system of equations. To avoid these computational difficulties methods called <em>quasi-Newton methods</em> have been developed which takes into account approximations of the inverse of the Hessian rather than the original inverse at each step. We will dedicate a separate chapter too in studying <em>quasi-Newton</em> methods.</p>
</div>
<div id="marquardt-method" class="section level2">
<h2><span class="header-section-number">4.6</span> Marquardt Method</h2>
<p>The <em>Marquardt method</em> is an amalgamation of the <em>steepest descent algorithm</em> and the <em>Newton method</em>. At each step <span class="math inline">\(j\)</span>, the <em>Marquardt method</em> modifies the diagonal elements of the Hessian matrix in the following way:
<span class="math display" id="eq:44">\[\begin{equation}
    [\mathbb{H}f(\mathbb{x}_j)]&#39; = [\mathbb{H}f(\mathbb{x}_j)] + \gamma\mathbb{I} \tag{4.44}
\end{equation}\]</span></p>
<p>where, <span class="math inline">\(\gamma\)</span> is a sufficiently large positive constant that guarantees the positive definiteness of <span class="math inline">\([\mathbb{H}f(\mathbb{x}_j)]&#39;\)</span> when <span class="math inline">\([\mathbb{H}f(\mathbb{x}_j)]\)</span> is not positive definite, and <span class="math inline">\(\mathbb{I}\)</span> is the identity matrix. The descent direction in the algorithm is given by:
<span class="math display" id="eq:45">\[\begin{equation}
    \mathbb{\delta}_j = -[\mathbb{H}f(\mathbb{x}_j)]&#39;^{-1}\nabla f(\mathbb{x}_j) \tag{4.45}
\end{equation}\]</span></p>
<p>The algorithm describing <em>Marquardt method</em> is given below:</p>
<p><img src="img%2017.png" /></p>

<div class="example">
<span id="exm:unnamed-chunk-21" class="example"><strong>Example 4.4  </strong></span>We will work on Himmelblauâ€™s function once again and try to find one of the local minima using <em>Marquardt algorithm</em>. Let the starting iterate be, <span class="math inline">\(\mathbb{x}_j = \begin{bmatrix}-2 \\ -2.1 \end{bmatrix}\)</span>, the positive constant be <span class="math inline">\(\gamma = 10^3\)</span>, the tolerances be <span class="math inline">\(\epsilon_1=\epsilon_2 = \epsilon_3=10^{-5}\)</span>, and the constants to be used in determining the step length using the <em>strong Wolfe conditions</em> be <span class="math inline">\(\alpha_1=10^{-4}\)</span> and <span class="math inline">\(\alpha_2=0.25\)</span>. The whole Python program along with the function <code>marquardt()</code> that implements the <em>Marquardt algorithm</em> is given below:
</div>

<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="line-search-descent-methods.html#cb21-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> line_search</span>
<span id="cb21-2"><a href="line-search-descent-methods.html#cb21-2"></a>NORM <span class="op">=</span> np.linalg.norm <span class="co"># Function that gives norm of a vector</span></span>
<span id="cb21-3"><a href="line-search-descent-methods.html#cb21-3"></a></span>
<span id="cb21-4"><a href="line-search-descent-methods.html#cb21-4"></a><span class="kw">def</span> func(x): <span class="co"># Objective function (Himmelblau&#39;s function)</span></span>
<span id="cb21-5"><a href="line-search-descent-methods.html#cb21-5"></a>    <span class="cf">return</span> (x[<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> x[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">11</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> (x[<span class="dv">0</span>] <span class="op">+</span> x[<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">7</span>)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb21-6"><a href="line-search-descent-methods.html#cb21-6"></a>    </span>
<span id="cb21-7"><a href="line-search-descent-methods.html#cb21-7"></a>Df <span class="op">=</span> grad(func) <span class="co"># Gradient of the objective function</span></span>
<span id="cb21-8"><a href="line-search-descent-methods.html#cb21-8"></a>Hf <span class="op">=</span> jacobian(Df) <span class="co"># Hessian of the objective function</span></span>
<span id="cb21-9"><a href="line-search-descent-methods.html#cb21-9"></a></span>
<span id="cb21-10"><a href="line-search-descent-methods.html#cb21-10"></a>x1 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">100</span>)</span>
<span id="cb21-11"><a href="line-search-descent-methods.html#cb21-11"></a>x2 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">100</span>)</span>
<span id="cb21-12"><a href="line-search-descent-methods.html#cb21-12"></a>z <span class="op">=</span> np.zeros(([<span class="bu">len</span>(x1), <span class="bu">len</span>(x2)]))</span>
<span id="cb21-13"><a href="line-search-descent-methods.html#cb21-13"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(x1)):</span>
<span id="cb21-14"><a href="line-search-descent-methods.html#cb21-14"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(x2)):</span>
<span id="cb21-15"><a href="line-search-descent-methods.html#cb21-15"></a>        z[j, i] <span class="op">=</span> func([x1[i], x2[j]])</span>
<span id="cb21-16"><a href="line-search-descent-methods.html#cb21-16"></a></span>
<span id="cb21-17"><a href="line-search-descent-methods.html#cb21-17"></a>contours<span class="op">=</span>plt.contour(x1, x2, z, <span class="dv">100</span>, cmap<span class="op">=</span>plt.cm.gnuplot)</span>
<span id="cb21-18"><a href="line-search-descent-methods.html#cb21-18"></a>plt.clabel(contours, inline<span class="op">=</span><span class="dv">1</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span></code></pre></div>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="line-search-descent-methods.html#cb22-1"></a>plt.xlabel(<span class="st">&quot;$x_1$ -&gt;&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="line-search-descent-methods.html#cb23-1"></a>plt.ylabel(<span class="st">&quot;$x_2$ -&gt;&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="line-search-descent-methods.html#cb24-1"></a><span class="kw">def</span> marquadt(Xj, gamma, tol_1, tol_2, tol_3, alpha_1, alpha_2):</span>
<span id="cb24-2"><a href="line-search-descent-methods.html#cb24-2"></a>    x1 <span class="op">=</span> [Xj[<span class="dv">0</span>]]</span>
<span id="cb24-3"><a href="line-search-descent-methods.html#cb24-3"></a>    x2 <span class="op">=</span> [Xj[<span class="dv">1</span>]]</span>
<span id="cb24-4"><a href="line-search-descent-methods.html#cb24-4"></a>    I <span class="op">=</span> np.eye(<span class="bu">len</span>(Xj)) <span class="co">#Set the identity matrix with equal dimensions as that of the Hessian matrix</span></span>
<span id="cb24-5"><a href="line-search-descent-methods.html#cb24-5"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb24-6"><a href="line-search-descent-methods.html#cb24-6"></a>        D <span class="op">=</span> Df(Xj)</span>
<span id="cb24-7"><a href="line-search-descent-methods.html#cb24-7"></a>        Hess_tilde <span class="op">=</span> Hf(Xj) <span class="op">+</span> gamma<span class="op">*</span>I <span class="co"># Diagonal elements of the Hessian modified</span></span>
<span id="cb24-8"><a href="line-search-descent-methods.html#cb24-8"></a>        inv_hess <span class="op">=</span> np.linalg.inv(Hess_tilde) <span class="co"># Inverse of the modified Hessian</span></span>
<span id="cb24-9"><a href="line-search-descent-methods.html#cb24-9"></a>        delta <span class="op">=</span> <span class="op">-</span>inv_hess.dot(D) <span class="co"># Selection of the direction of the steepest descent</span></span>
<span id="cb24-10"><a href="line-search-descent-methods.html#cb24-10"></a>        </span>
<span id="cb24-11"><a href="line-search-descent-methods.html#cb24-11"></a>        </span>
<span id="cb24-12"><a href="line-search-descent-methods.html#cb24-12"></a>        start_point <span class="op">=</span> Xj <span class="co"># Start point for step length selection </span></span>
<span id="cb24-13"><a href="line-search-descent-methods.html#cb24-13"></a>        beta <span class="op">=</span> line_search(f<span class="op">=</span>func, myfprime<span class="op">=</span>Df, xk<span class="op">=</span>start_point, pk<span class="op">=</span>delta, c1<span class="op">=</span>alpha_1, c2<span class="op">=</span>alpha_2)[<span class="dv">0</span>] <span class="co"># Selecting the step length</span></span>
<span id="cb24-14"><a href="line-search-descent-methods.html#cb24-14"></a>        <span class="cf">if</span> beta<span class="op">!=</span><span class="va">None</span>:</span>
<span id="cb24-15"><a href="line-search-descent-methods.html#cb24-15"></a>            X <span class="op">=</span> Xj<span class="op">+</span> beta<span class="op">*</span>delta</span>
<span id="cb24-16"><a href="line-search-descent-methods.html#cb24-16"></a>        <span class="cf">if</span> NORM(X <span class="op">-</span> Xj) <span class="op">&lt;</span> tol_1 <span class="kw">and</span> NORM(Df(X)) <span class="op">&lt;</span> tol_2 <span class="kw">or</span> <span class="bu">abs</span>(func(X) <span class="op">-</span> func(Xj)) <span class="op">&lt;</span> tol_3:</span>
<span id="cb24-17"><a href="line-search-descent-methods.html#cb24-17"></a>            x1 <span class="op">+=</span> [X[<span class="dv">0</span>], ]</span>
<span id="cb24-18"><a href="line-search-descent-methods.html#cb24-18"></a>            x2 <span class="op">+=</span> [X[<span class="dv">1</span>], ]</span>
<span id="cb24-19"><a href="line-search-descent-methods.html#cb24-19"></a>            plt.plot(x1, x2, <span class="st">&quot;rx-&quot;</span>, ms<span class="op">=</span><span class="fl">5.5</span>) <span class="co"># Plot the final collected data showing the trajectory of optimization</span></span>
<span id="cb24-20"><a href="line-search-descent-methods.html#cb24-20"></a>            plt.show()</span>
<span id="cb24-21"><a href="line-search-descent-methods.html#cb24-21"></a>            <span class="cf">return</span> X, func(X)</span>
<span id="cb24-22"><a href="line-search-descent-methods.html#cb24-22"></a>        <span class="cf">else</span>:</span>
<span id="cb24-23"><a href="line-search-descent-methods.html#cb24-23"></a>            Xj <span class="op">=</span> X</span>
<span id="cb24-24"><a href="line-search-descent-methods.html#cb24-24"></a>            x1 <span class="op">+=</span> [Xj[<span class="dv">0</span>], ]</span>
<span id="cb24-25"><a href="line-search-descent-methods.html#cb24-25"></a>            x2 <span class="op">+=</span> [Xj[<span class="dv">1</span>], ]</span></code></pre></div>
<p>We set the parameter values according to what we have set in our example and pass them to the <code>marquardt()</code> function:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="line-search-descent-methods.html#cb25-1"></a>marquadt(np.array([<span class="op">-</span><span class="fl">2.</span>, <span class="fl">-2.1</span>]), <span class="dv">10</span><span class="op">**</span><span class="dv">3</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">4</span>, <span class="fl">0.25</span>)</span></code></pre></div>
<pre><code>## (array([-3.7792679 , -3.28317977]), 9.852018841385639e-08)</code></pre>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-5.png" width="672" /></p>
<p>We see that, for our choice of parameters, the algorithm has correctly converged to the minimizer <span class="math inline">\(\mathbb{x}^* \sim \begin{bmatrix}-3.7793 \\ -3.2831 \end{bmatrix}\)</span>, where the function value is <span class="math inline">\(0\)</span>.</p>
<p>The optimization data has been collected and shown below:</p>
<pre><code>## +----+----------+----------+---------------+-------------+
## |    |      x_1 |      x_2 |          f(X) |    ||grad|| |
## |----+----------+----------+---------------+-------------|
## |  0 | -2       | -2.1     | 103.878       | 66.7972     |
## |  1 | -3.69991 | -2.65702 |  13.2518      | 39.5215     |
## |  2 | -3.60177 | -3.24858 |   1.63007     | 18.3846     |
## |  3 | -3.73439 | -3.23778 |   0.148408    |  4.69335    |
## |  4 | -3.79094 | -3.2786  |   0.0103291   |  1.6592     |
## |  5 | -3.78042 | -3.28371 |   6.68136e-05 |  0.114701   |
## |  6 | -3.7796  | -3.28358 |   8.42206e-06 |  0.0346822  |
## |  7 | -3.77927 | -3.28318 |   9.85202e-08 |  0.00479187 |
## +----+----------+----------+---------------+-------------+</code></pre>
<p>We now move on to the next chapter where we study <em>conjugate gradient methods</em> in more detail.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="solving-one-dimensional-optimization-problems.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="conjugate-gradient-methods-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/indrag49/Numerical-Optimization/edit/master/04-Line_Search_Descent_Methods.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/indrag49/Numerical-Optimization/blob/master/04-Line_Search_Descent_Methods.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
