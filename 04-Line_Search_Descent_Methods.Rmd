# Line Search Descent Methods

This chapter starts with an outline of a simple **line-search descent algorithm**, before introducing the **Wolfe conditions** and how to use them to design an algorithm for selecting a step length at a chosen descent direction at each step of the line search algorithms. Then a first order line search descent algorithm called the **Steepest Descent Algorithm** and a second order line search descent algorithm, called the **Modified Newton Method** have been discussed. Examples, Python programs and proofs accompanying each section of the chapter have been provided, wherever required. Finally, before ending the chapter with **Marquardt method**, motivations to study **Conjugate Gradient Methods** and **Quasi-Newton Methods** have been explored, which will be introduced in detailed manner in the next and later chapters.

---

## Introduction to Line Search Descent Methods for Unconstrained Minimization

In the *line search descent methods*, the optimization technique picks a direction $\mathbb{\delta_j}$ to begin with, for the $j^{th}$ step and carries out a search along this direction from the previous experimental point, to generate a new iterate. The iterative process looks like:
\begin{equation}
    \mathbb{x}_j = \mathbb{x}_{j-1}+\beta_{j}\mathbb{\delta}_j, \mathbb{x} \in \mathbb{R}^n (\#eq:1)
\end{equation}

Here, $\beta_j$ is a positive scalar number at the $j^{th}$ step , called the *step length*. The performance of a line search descent algorithm depends on the selection of both the *step length* $\beta_j$ and the descent direction $\mathbb{\delta}_j$. The condition for selecting the direction $\mathbb{\delta}_j$for the next iterate :

\begin{equation}
\nabla^T f(\mathbb{x}_{j-1})\mathbb{\delta}_j < 0 (\#eq:2)
\end{equation}
i.e, the directional derivative in the direction $\mathbb{\delta}_j$ should be negative. The step length $\beta_j$ is computed by solving the one dimensional optimization problem formulated as:
\begin{equation}
    \underset{\beta_j > 0}{\min} \tilde{f}(\beta_j) = \underset{\beta_j > 0}{\min} f(\mathbb{x}_{j-1} + \beta_j \mathbb{\delta}_j) (\#eq:3)
\end{equation}

The general algorithm for a line search descent method is given below:

![](img 12.png)

## Selection of Step Length

While finding a suitable step length $\beta_j$ at the $j^{th}$ iteration, we should keep in mind that the choice should be such that there is an acceptable reduction in the objective function value. We work towards solving a minimization task formulated as:

\begin{equation}
    \tilde{f}(\beta) = f(\mathbb{x}_{j-1} + \beta\mathbb{\delta}_j),\ \  \beta > 0 (\#eq:4)
\end{equation}

The algorithm should be designed in such a way that too many computations of the objective function and its gradient should be avoided. This can be achieved by performing *inexact* line searches, to compute the local minimizer of $\tilde{f}$. As we discussed earlier, there should be a condition for choosing $\beta_j$ at each iterate. The condition:

\begin{equation}
    f(\mathbb{x}_j) > f(\mathbb{x}_{j-1} + \beta_j\mathbb{\delta}_j) (\#eq:5)
\end{equation}

does not suffice alone. We need to have a sufficient decrease condition known as the \textit{wolfe conditions}.

### The Wolfe Conditions

The step length $\beta_j$, chosen at each iteration, must result in a \textit{sufficient decrease} in the objective function $f(\mathbb{x})$ given by:

\begin{equation}
    f(\mathbb{x}_{j-1} +\beta_j\mathbb{\delta}_j) \leq f(\mathbb{x}_j) + \alpha_1 \beta_j\nabla^Tf(\mathbb{x}_j)\mathbb{\delta}_j,\ \ \ \ \alpha_1 \in (0, 1) (\#eq:6)
\end{equation}

This is also called the *Armijo condition*. Practically, the value of $\alpha_1$ should be very small, for example in the order of $10^{-4}$. But the *Armijo condition* itself is not enough to guarantee a reasonable progress in the algorithm. To avoid unacceptably short step lengths, there is another condition given by:
\begin{equation}
    \nabla^Tf(\mathbb{x}_{j-1} + \beta_j\mathbb{\delta}_j)\mathbb{\delta}_j \geq \alpha_2\nabla^Tf(\mathbb{x}_{j-1})\mathbb{\delta}_j,\ \ \ \ \alpha_2 \in (\alpha_1, 1) (\#eq:7)
\end{equation}

This is also called the *curvature condition*. Practically $\alpha_2$ is chosen between $0.1 - 0.9$ depending on the algorithms we consider. Eq. \@ref(eq:6) and Eq. \@ref(eq:7) together form the *Wolfe conditions*. Further more, the *curvature condition* can be modified to steer away from cases where a step length might satisfy the *Wolfe conditions* without being close to the minimizer of $\tilde{f}(\beta)$. The modified version of Eq. \@ref(eq:7) can be written as:
\begin{equation}
    |\nabla^Tf(\mathbb{x}_{j-1} + \beta_j\mathbb{\delta}_j)\mathbb{\delta}_j| \leq \alpha_2|\nabla^Tf(\mathbb{x}_{j-1})\mathbb{\delta}_j| (\#eq:8)
\end{equation}
So, Eq. \@ref(eq:6) and Eq. \@ref(eq:7) together form the \textit{strong Wolfe conditions}. For the \textit{strong Wolfe conditions} the term $\nabla^Tf(\mathbb{x}_{j-1} + \beta_j\mathbb{\delta}_j)\mathbb{\delta}_j$ is no longer ``too positive'' unlike the case for the \textit{Wolfe conditions}.

### An Algorithm for the Strong Wolfe Conditions

Before moving on to the line search algorithm for the *strong wolfe conditions*, we discuss a straightforward algorithm called *zoom* which takes in two values $\beta_l$ and $\beta_r$ that bounds the interval $[\beta_l, \beta_r]$ containing the step lengths that satisfy the *strong Wolfe conditions*. This algorithm has been originally introduced in the classic book by Nocedal and Wright. The purpose of this algorithm is to generate an iterate $\beta_j \in [\beta_l, \beta_r]$ at each step and replaces either $\beta_l$ or $\beta_r$ with $\beta_j$ in such a way that the following properties are maintained:

* The step length $\beta_j$ satisfying the \textit{strong Wolfe conditions} lies in the interval $[\beta_l, \beta_r]$,
* $\beta_l$ is the step length which after a particular iteration gives the lowest function value besides satisfying the *Armijo conditions*,
* $\beta_r$ is chosen such that the following condition is satisfied:
\begin{equation}
        (\beta_r -\beta_l)[\nabla^Tf(\mathbb{x}_{j-1} + \beta_l\mathbb{\delta}_j)\mathbb{\delta}_j] < 0 (\#eq:9)
\end{equation}

The algorithm for *zoom* is given below:

![](img 13.png)

Now, we describe the algorithm for finding an optimized step length $\beta_j$ at the $j^{th}$ iterate in a line search algorithm, solving the minimization task formulated by Eq. \@ref(eq:3). The *step-length selection algorithm* satisfying the *strong Wolfe conditions* is given below:

![](img 14.png)

The first part of the above algorithm, starts with a trial estimate of the step length and keeps on increasing it at each step until it finds either an acceptable length or an interval bracketing the optimized step lengths. The *zoom()* function , given by **Algorithm 11**, is called in the second part which reduces the size of this interval until the optimized step length is reached.

```{example}
Let us consider the *Himmelblau's function* as the objective function, given by,
\begin{equation}
    f(x_1, x_2) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2 (\#eq:10)
\end{equation}
```

Let the starting point be $\mathbb{x}=\begin{bmatrix}-2.5 \\ 2.8 \end{bmatrix}$, the descent direction be $\mathbb{\delta}=\begin{bmatrix}-2.5 \\ -1 \end{bmatrix}$, $\alpha_1$ and $\alpha_2$ be $10^{-4}$ and $0.325$ respectively, and the upper bound be $\beta_{max}=0.6$. For this descent direction, we will compute the step length for generating the next iterate from the starting point using **Algorithm 11**. The *step-length selection algorithm* solves the optimization problem given by Eq. \@ref(eq:4) for the parameters provided in this example. Let us first define the objective function and its gradient using Python.

```{python}
# First let us import all the necessary packages
import matplotlib.pyplot as plt
import numpy as np
import autograd.numpy as au
from autograd import grad, jacobian
import scipy

def himm(x): # Objective function
    return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2

grad_himm = grad(himm) # Gradient of the objective function
```

We will now plot the objective function. 

```{python, results=FALSE}
x = np.linspace(-6, 6, 100)
y = np.linspace(-6, 6, 100)
z = np.zeros(([len(x), len(y)]))
for i in range(0, len(x)):
    for j in range(0, len(y)):
        z[j, i] = himm([x[i], y[j]])

contours=plt.contour(x, y, z, 100, cmap=plt.cm.gnuplot)
plt.clabel(contours, inline=1, fontsize=10)

plt.show()
```

We will use the `line_search()` function from the `scipy.optimize` module which is a Python implementation of the *step-length selection algorithm*. The attributes for the `line_search()` function are:

* `f`: This is the objective function, which is a callable datatype,
* `myfprime`: This is the gradient of the objective function and is callable,
* `xk`: This is the starting iterate, given by an `ndarray` datatype,
* `pk`: This is the descent direction given by $\mathbb{\delta}$, used for generating the next iterate point from a given starting point. This is also given by an `ndarray` datatype,
* `c1`: This is the $\alpha_1$ value from the *Armijo condition* given by Eq. \@ref(eq:6). This is an optional datatype and is a `float`,
* `c2`: This is the $\alpha_2$ value from the *curvature condition* given by Eq. \@ref(eq:7). This is an optional datatype and is a `float`,
* `amax`: This is the upper bound set for the step lengths, given by $\beta_{max}$ in *step-length selection algorithm*. This is an optional datatype and is a `float`.

There are other attributes that one can pass to the function, but they are less important but can provide with more flexibilities if provided. These are:

* `gfk`: Gives the gradient value at the current iterate point, $\mathbb{x}_j$. This is an optional parameter and is a `float`,
* `old_fval`: This gives the function value at the current iterate point, $\mathbb{x}_j$. The parameter is optional and is a `float`,
* `old_old_fval`: This gives the function value at the point preceding the current iterate point,i.e, $\mathbb{x}_{j-1}$. This is optional and is a `float`, 
* `args`: These are the additional arguments that might be passed to the objective function. This is optional and is a Python `tuple`,
* `maxiter`: This is the maximum number of iterations that are needed to be performed by the optimization algorithm. This is optional too and is an `int` datatype, and
* `extra_condition`: This is a callable function having the following form: `extra_condition(beta, current_iterate, function, gradient)`. The step length `beta` is accepted if only this function returns `True`, otherwise the algorithm continues with the new iterates. This callable function is only invoked for those iterates which satisfy the *strong Wolfe conditions*.

The `scipy.optimize.line_search()` method returns the following:

* The optimized step length $\beta$ solving Eq. \@ref(eq:3) for the next iterate from the current iterate. This will either be a `float` or a `None`,
* The number of function evaluations made. This is an `int`,
* The number of gradient evaluations made. This is an `int`,
* The function value given by Eq. \@ref(eq:4) with the computed step length. This will be a `float` if the algorithm converges, otherwise this will be a `None`,
* The function value at the starting point, the algorithm starts with. This is a `float` too, and
* The local slope along the descent direction at the new value. This will be a `float` if the algorithm converges, otherwise, this will be a `None`.

Now, for our example, we enter the values of the starting point, the descent direction, the constants $\alpha_1$ and $\alpha_2$ and the upper bound on the step lengths for the `scipy.optimize.line_search()` and print the results.

```{python}
from scipy.optimize import line_search

start_point = np.array([-2.5, 2.8])
delta = np.array([-2.5, -1])
alpha_1 = 10**-4
alpha_2 = 0.325
beta_max = 0.6

res=line_search(f = himm, myfprime = grad_himm, xk = start_point, pk = delta, c1 = alpha_1, c2 = alpha_2, amax = beta_max)
res
```

We see that the optimized step length is $\sim 0.04$, the number of function evaluations made is $4$, the number of gradient evaluations made is $1$, the function value at the new step length is $\tilde{f(\beta)}\sim 6.11$, the function value at the starting point is $\sim 6.56$ and the local slope along the descent direction is $\sim \begin{bmatrix} 11.13 \\ -25 \end{bmatrix}$.

## First Order Line Search Gradient Descent Method: The Steepest Descent Algorithm

Optimization methods that use the gradient vector $\nabla^Tf(\mathbb{x})$ to compute the descent direction $\mathbb{\delta}_j$ at each iteration, are referred to as the *first order line search gradient descent methods*. We will discuss the *steepest descent algorithm* that falls under this category. It is also called the *Cauchy method*, as it was first introduced by the french mathematician Cauchy in 1847.

Now, we explore, how to select the direction of the *steepest descent algorithm*. At the iterate $\mathbb{x}_{j-1}$, the direction of the steepest descent, given by the unit vector $\mathbb{\delta}_j$ is chosen, such that the directional derivative $\nabla^Tf(\mathbb{x}_{j-1})\mathbb{\delta}_j$ takes a minimum value for all possible values of $\mathbb{\delta}_j$ at $\mathbb{x}_{j-1}$. Now, using Schwartz's inequality,
\begin{equation}
    \nabla^Tf(\mathbb{x}_{j-1})\mathbb{\delta}_j \geq - \|\nabla^Tf(\mathbb{x}_{j-1})\|\|\mathbb{\delta}_j\| \geq - \|\nabla^Tf(\mathbb{x}_{j-1})\| (\#eq:11)
\end{equation}

The value $- \|\nabla^Tf(\mathbb{x}_{j-1})\|$ is the minimum value. Now, from the first and the third terms in the Eq. \@ref(eq:11), we can write,
\begin{equation}
    \mathbb{\delta}_j = -\frac{\nabla f(\mathbb{x}_{j-1})}{\|\nabla f(\mathbb{x}_{j-1})\|} (\#eq:12)
\end{equation}

The expression in Eq. \@ref(eq:12) is the *normalised direction of the steepest descent algorithm*. The *Steepest Direction Algorithm* is given below:

![](img 15.png)

```{example}
Let us again consider Himmelblau's function as the objective function, given by,
\begin{equation}
    f(x_1, x_2) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2 \nonumber
\end{equation}
```

The function has four local minima:

* $f(3., 2.) = 0$,
* $f(-2.8051, 3.1313) = 0$,
* $f(-3.7793, -3.2831) = 0$, and
* $f(3.5844, -1.8481) = 0$.

We will find one of these local minima of Himmelblau's function, using the *steepest descent algorithm* in Python. Let the starting iterate be $\mathbb{x}_j = \begin{bmatrix}1.1 \\ 2.2\end{bmatrix}$, the tolerances be $\epsilon_1 = \epsilon_2 = \epsilon_3 = 10^{-5}$, and the constants to be used in determining the step length using the *strong Wolfe conditions* be $\alpha_1 = 10^{-4}$ and $\alpha_2=0.212$. Let us rewrite Himmelblau's function and its gradient in Python:

```{python}
def func(x): # Objective function (Himmelblau's function)
    return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2

Df = grad(func) # Gradient of the objective function
```

We will use `numpy`'s `linalg.norm()` function for calculating the norm of a vector or a matrix.

```{python}
NORM = np.linalg.norm
```

Suppose, we want to calculate the norm of the vector, $v = \begin{bmatrix}-1.1 \\ 2.3 \\ 0.1 \end{bmatrix}$. We use the `NORM()` function we just defined:

```{python}
v = np.array([-1.1, 2.3, 0.1])
```

Next, we visualize the objective function on a two dimensional space identical to the one that we already generated above, for setting up an environment to visualize the trajectory of the optimization. And finally, we write the Python function `steepest_descent()` implementing the *steepest descent algorithm*:

```{python, results=FALSE}

x1 = np.linspace(-6, 6, 100)
x2 = np.linspace(-6, 6, 100)
z = np.zeros(([len(x1), len(x2)]))
for i in range(0, len(x1)):
    for j in range(0, len(x2)):
        z[j, i] = func([x1[i], x2[j]])

contours=plt.contour(x1, x2, z, 100, cmap=plt.cm.gnuplot)
plt.clabel(contours, inline=1, fontsize=10)
plt.xlabel("$x_1$ ->")
plt.ylabel("$x_2$ ->")


def steepest_descent(Xj, tol_1, tol_2, tol_3, alpha_1, alpha_2):
    x1 = [Xj[0]]
    x2 = [Xj[1]]
    while True:
        D = Df(Xj)
        delta = - D / NORM(D) # Selection of the direction of the steepest descent
        
        start_point = Xj # Start point for step length selection 
        beta = line_search(f=func, myfprime=Df, xk=start_point, pk=delta, c1=alpha_1, c2=alpha_2)[0] # Selecting the step length
        if beta!=None:
            X = Xj+ beta*delta
        if NORM(X - Xj) < tol_1 and NORM(Df(X)) < tol_2 or abs(func(X) - func(Xj)) < tol_3:
            x1 += [X[0], ]
            x2 += [X[1], ]
            plt.plot(x1, x2, "rx-", ms=5.5) # Plot the final collected data showing the trajectory of optimization
            plt.show()
            return X, func(X)
        else:
            Xj = X
            x1 += [Xj[0], ]
            x2 += [Xj[1], ]
```

According to our example, we set our parameter values and pass them to the `steepest_descent()` function:

```{python}
steepest_descent(np.array([1.1, 2.2]), 10**-5, 10**-5, 10**-5, 10**-4, 0.212)
```

We see that for our choice of parameters, the algorithm has converged to the minimizer $\mathbb{x}^* \sim \begin{bmatrix} 3 \\ 2 \end{bmatrix}$ for our objective function, where the function value is $0$.


**This chapter is under development**